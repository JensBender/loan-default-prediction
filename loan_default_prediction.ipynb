{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d843a7-8378-4685-bc72-96cf409b3000",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:36px; font-weight:bold; color:#4A4A4A; background-color:#fff6e4; padding:10px; border:3px solid #f5ecda; border-radius:6px\">\n",
    "    Predicting Loan Defaults\n",
    "    <p style=\"text-align:center; font-size:14px; font-weight:normal; color:#4A4A4A; margin-top:12px;\">\n",
    "        Author: Jens Bender <br> \n",
    "        February 2025\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf3cd6-69e4-4aa0-a083-987254cd77f0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Project Overview</h1>\n",
    "</div> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4275851-2816-4903-91c3-8b4a4c489b12",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "This project aims to develop a machine learning model to predict whether the customers of a financial institution will default on a loan based on data from their loan application. By accurately identifying potential defaulters, financial institutions can make more informed lending decisions, reduce losses, improve profitability, and increase operational efficiency through the automation of risk assessment.\n",
    "\n",
    "**Problem**  \n",
    "Predicting loan defaults is a challenging task due to the multitude of influencing factors such as customers' demographic, financial, location, and behavioral attributes. Traditional default prediction models often oversimplify complex relationships between customer features and default risk. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in loan application data, enabling more accurate predictions of loan default risk.\n",
    "\n",
    "**Objectives**  \n",
    "- Develop a machine learning model to predict loan defaults using customer data from loan applications.\n",
    "- Compare multiple models (e.g., Logistic Regression, Random Forest, XGBoost) using a suitable evaluation metric (such as AUC-PR).\n",
    "- Identify key factors influencing loan default risk through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project enables financial institutions to reduce loan default rates and make better and faster lending decisions by leveraging machine learning for automated and improved risk assessment. \n",
    "\n",
    "**Business Goals**  \n",
    "- Reduce losses by 5M-10M INR within 12 months of model deployment by decreasing the loan default rate by 10%-20%.\n",
    "- Decrease loan processing time by 25%-40% by automating risk assessment, leading to less time spent on manual evaluations.\n",
    "- Ensure 100% compliance with regulatory requirements and fair lending practices.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains information provided by customers of a financial institution during the loan application process. It is sourced from the \"Loan Prediction Based on Customer Behavior\" dataset by Subham Jain, available on [Kaggle](https://www.kaggle.com/datasets/subhamjain/loan-prediction-based-on-customer-behavior). The dataset consists of three `.csv` files:\n",
    "1. `Training Data.csv`: Contains the features, target variable (`Risk Flag`), and `ID` column from the training data. \n",
    "2. `Test Data.csv`: Contains the features and `ID` column from the test data.\n",
    "3. `Sample Prediction Dataset.csv`: Contains the target variable (`Risk Flag`) and `ID` column from the test data. \n",
    "\n",
    "Dataset Statistics:\n",
    "- Training set size: 252,000 records \n",
    "- Test set size: 28,000 records \n",
    "- Target variable: Risk flag (training: 12.3% defaults, test: 12.8% defaults)\n",
    "- Features: 11 \n",
    "  - Demographic: Age, married, profession\n",
    "  - Financial: Income, house ownership, car ownership\n",
    "  - Location: City, state\n",
    "  - Behavioral: Experience, current job years, current house years\n",
    "\n",
    "Data Overview Table:\n",
    "\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Training Data Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Risk Flag | Defaulted on loan (0: No, 1: Yes) | Integer | Categorical (Binary) | [0, 1] | [0, 1] |\n",
    "| Income | Income of the applicant | Integer | Numerical | [0, âˆž] | [10K, 10M] |\n",
    "| Age | Age of the applicant (in years) | Integer | Numerical | [18, âˆž] | [21, 79] |\n",
    "| Experience | Work experience (in years) | Integer | Numerical | [0, âˆž] | [0, 20] |\n",
    "| Profession | Applicant's profession | String | Categorical (Nominal) | Any profession [e.g., \"Architect\", \"Dentist\"] | 51 unique professions |\n",
    "| Married | Marital status | String | Categorical (Binary) | [\"single\", \"married\"] | [\"single\", \"married\"] |\n",
    "| House Ownership | Applicant owns or rents a house | String | Categorical (Nominal) | [\"rented\", \"owned\", \"norent_noown\"] | [\"rented\", \"owned\", \"norent_noown\"] |\n",
    "| Car Ownership | Whether applicant owns a car | String | Categorical (Binary) | [\"yes\", \"no\"] | [\"yes\", \"no\"] |\n",
    "| Current Job Years | Years in the current job | Integer | Numerical | [0, âˆž] | [0, 14] |\n",
    "| Current House Years | Years in the current house | Integer | Numerical | [0, âˆž] | [10, 14] |\n",
    "| City | City of residence | String | Categorical (Nominal) | Any city [e.g., \"Mumbai\", \"Bangalore\"] | 317 unique cities |\n",
    "| State | State of residence | String | Categorical (Nominal) | Any state [e.g., \"Maharashtra\", \"Tamil_Nadu\"] | 29 unique states |\n",
    "\n",
    "Example Training Data:\n",
    "\n",
    "| Risk Flag | Income    | Age | Experience | Profession         | Married | House Ownership | Car Ownership | Current Job Years | Current House Years | City      | State         |\n",
    "| :-------- | :-------- | :-- | :--------- | :----------------- | :------ | :-------------- | :------------ | :---------------- | :------------------ | :-------- | :------------ |\n",
    "| 0         | 1,303,834 | 23  | 3          | Mechanical_engineer | single  | rented          | no            | 3                 | 13                   | Rewa      | Madhya_Pradesh |\n",
    "| 1         | 6,256,451 | 41  | 2          | Software_Developer | single  | rented          | yes           | 2                 | 12                   | Bangalore | Tamil_Nadu    |\n",
    "| 0         | 3,991,815 | 66  | 4          | Technical_writer   | married | rented          | no            | 4                 | 10                   | Alappuzha | Kerala        |\n",
    "\n",
    "**Technical Requirements**  \n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, data types, missing values, and outliers.\n",
    "  - Extract features, scale numerical features, and encode categorical features.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models and perform hyperparameter tuning for binary classification task with `sklearn` and `xgboost`.\n",
    "  - Baseline models: Logistic Regression, Elastic Net Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Decision Tree, Random Forest, Multi-Layer Perceptron, XGBoost.\n",
    "  - Evaluate model performance using Area Under the Precision-Recall Curve (AUC-PR).\n",
    "    - AUC-PR is more suitable to address class imbalance (12.3% defaults) with a focus on the positive class (preventing defaults) than accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "    - Success criterion: Minimum AUC-PR of 0.70 on the test data.\n",
    "  - Potentially use additional techniques to address class imbalance (e.g., SMOTE, class weights).\n",
    "  - Visualize feature importance, show model prediction examples, and save the final model with `pickle`.\n",
    "- Deployment:\n",
    "  - Expose the final model via a REST API for easy integration with existing loan processing systems.\n",
    "  - Implement efficient batch processing capabilities to handle up to 10K predictions in under 30 seconds.\n",
    "  - Deploy using cloud infrastructure to ensure scalability and security.\n",
    "  - Set up model performance monitoring and data drift detection.\n",
    "- Stakeholders:\n",
    "  - Loan officers: Direct users of the model predictions in day-to-day loan approvals.\n",
    "  - Credit risk analysts: Provide subject matter expertise on loan default risk.\n",
    "  - Compliance officers: Ensure the model complies with any legal and regulatory guidelines.\n",
    "  - IT department: Manage the IT infrastructure and ensure data access for the model's development and deployment. \n",
    "\n",
    "By fulfilling these objectives and requirements, the project will provide a valuable tool for predicting loan defaults, thereby enhancing decision-making for financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9b38-149a-4c08-bb6a-c1ad97d33d28",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Imports</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00e3c4-73f9-45bb-a474-58fc6049796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, auc, accuracy_score, precision_recall_fscore_support\n",
    "from scipy.stats import randint, uniform\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7316e-f065-4b92-8f2c-5cd5681b5a69",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Loading and Inspection</h1>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Load data from the three <code>.csv</code> files into three Pandas DataFrames.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e0114-5f1a-44ce-85e7-3136fe031aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"data/training_data.csv\")\n",
    "    X_test = pd.read_csv(\"data/test_data.csv\")\n",
    "    y_test = pd.read_csv(\"data/sample_prediction_dataset.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866b8d1-12e6-4f47-b54e-8132b55b4c1e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Initial data inspection to understand the structure of the dataset and detect obvious issues.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc38bee-eefa-48ce-9ddb-b9464e6caa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.info())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3692-d2c8-46f1-b0ad-36372c1fa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the training data\n",
    "print(\"Training Data:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890b354-634e-46b6-aaf0-0868ad775bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the test data features\n",
    "print(\"Test Data - Features:\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad411a4-8229-41b9-843b-ba280a6b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the test data target variable\n",
    "print(\"Test Data - Target Variable:\")\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9596-7328-43a4-8cf6-a18d1b64b4f7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Preprocessing</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef27083-1da0-42ce-a1b0-ad15b1b7a495",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Standardizing Names and Labels</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Column Names</strong> <br> \n",
    "    ðŸ“Œ Convert all column names to snake_case for consistency, improved readability, and to minimize the risk of errors. This also resolves inconsistencies in column names between the training and test datasets (e.g., \"Id\" vs. \"ID\", \"Risk_Flag\" vs. \"risk_flag\").  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e7b5f-3c5a-40c8-9f56-9d6040da7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake_case\n",
    "df_train.columns = (\n",
    "    df_train.columns\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  # Replace spaces and special characters with \"_\"\n",
    "    .str.replace(\"_single\", \"\")  # Shorten \"married_single\" to \"married\"\n",
    ")\n",
    "\n",
    "X_test.columns = (\n",
    "    X_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True) \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")\n",
    "\n",
    "y_test.columns = (\n",
    "    y_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7733ee-1f2d-48f3-92ae-1504a4b7f6d1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Categorical Labels</strong> <br> \n",
    "    ðŸ“Œ Convert all categorical labels to snake_case for consistency, improved readability, and to minimize the risk of errors. This also resolves inconsistencies in the state names of the \"state\" column between the training and test datasets (e.g., \"Uttar_Pradesh\" vs. \"Uttar Pradesh\", \"Jammu_and_Kashmir\" vs. \"Jammu and Kashmir\"). \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5e3b2-0cff-4253-8e60-ca868377217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_labels(categorical_label):\n",
    "    return (\n",
    "        categorical_label\n",
    "        .strip()  # Remove leading/trailing spaces\n",
    "        .lower()  # Convert to lowercase\n",
    "        .replace(\"-\", \"_\")  # Replace hyphens with \"_\"\n",
    "        .replace(\"/\", \"_\")  # Replace slashes with \"_\"\n",
    "        .replace(\" \", \"_\")  # Replace spaces with \"_\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define categorical columns to standardize labels\n",
    "columns_to_standardize = [\"profession\", \"city\", \"state\"]\n",
    "\n",
    "# Apply standardization of categorical labels\n",
    "for column in columns_to_standardize:\n",
    "    df_train[column] = df_train[column].apply(standardize_categorical_labels)\n",
    "    X_test[column] = X_test[column].apply(standardize_categorical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3912db-7304-42d3-ac5d-bd1b413a5bd7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\"> Merging Datasets</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2864227-c6dc-4d89-a603-060387350f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_test and y_test\n",
    "df_test = pd.merge(X_test, y_test, on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565d7a-e2b0-472d-91dd-f860878e384b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Duplicates</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify and remove duplicates based on all columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d98e50-9696-47fe-b1ad-51ec59375e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated().value_counts())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b48ba-dc42-4591-bc44-e490092a79f2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> âœ… No duplicates were found based on all columns in both the training and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029181-c3e9-4588-8055-7770ab866eff",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify and remove duplicates based on the ID column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e53c-dcfa-42eb-8542-7f215b4d3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.duplicated(subset=[\"id\"]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57954bf9-09ec-49df-8cb7-ed39d546d14f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> âœ… No duplicates were found based on the ID column in both the training and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89142d-7b4a-4b23-98b3-3c9e63ee5eb8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Data Types</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify and convert incorrect storage data types.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be913b0-2788-4e8d-8797-37233709dfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify storage data types\n",
    "print(\"Training Data:\")\n",
    "print(df_train.dtypes)\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da11a2-50d5-4c7a-907e-3b71830f0295",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> âœ… No incorrect storage data types were found at first glance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb80b7-c29b-4dd9-83ce-1b6a94abdb67",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify object columns with two unique categories and convert them to boolean columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0d39d-104b-47b4-8538-ffd2f55a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object columns with two unique categories \n",
    "print(\"Training Data:\")\n",
    "print(df_train[df_train.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test[df_test.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876603a-e2d3-493e-a1a7-cdf2233a8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert married and car_ownership column from object to boolean\n",
    "df_train[\"married\"] = df_train[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df_test[\"married\"] = df_test[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df_train[\"car_ownership\"] = df_train[\"car_ownership\"].map({\"yes\": True, \"no\": False})\n",
    "df_test[\"car_ownership\"] = df_test[\"car_ownership\"].map({\"yes\": True, \"no\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35470664-5860-4f0f-ac00-333004e26263",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Train-Validation-Test Split</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "ðŸ“Œ The dataset is initially divided into a training set (90%) and a test set (10%). Split the training set further to achieve the following train-validation-test split:\n",
    "    \n",
    "<table style=\"margin-left:0; margin-top:20px; margin-bottom:20px\">\n",
    "    <tr>\n",
    "        <th style=\"background-color:#f5ecda;\">Data</th>\n",
    "        <th style=\"background-color:#f5ecda;\">Size (%)</th>\n",
    "        <th style=\"background-color:#f5ecda;\">Size (Total)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#fff6e4;\">Training Set</td>\n",
    "        <td style=\"background-color:#fff6e4;\">80%</td>\n",
    "        <td style=\"background-color:#fff6e4;\">224,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#f5ecda;\">Validation Set</td>\n",
    "        <td style=\"background-color:#f5ecda;\">10%</td>\n",
    "        <td style=\"background-color:#f5ecda;\">28,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#fff6e4;\">Test Set</td>\n",
    "        <td style=\"background-color:#fff6e4;\">10%</td>\n",
    "        <td style=\"background-color:#fff6e4;\">28,000</td>\n",
    "    </tr>\n",
    "</table>\n",
    "This results in 80% of the total data being used for training, 10% for validation, and 10% for testing, while keeping the original test set size unchanged.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2ee21-930d-44fa-8740-41255d97336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X_train = df_train.drop(\"risk_flag\", axis=1)\n",
    "y_train = df_train[\"risk_flag\"]\n",
    "X_test = df_test.drop(\"risk_flag\", axis=1)\n",
    "y_test = df_test[\"risk_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee908d5-43d1-4098-8c78-8a4c61aacebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set further into training and validation sets (validation set size same as test set size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=X_test.shape[0]/df_train.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413ec8b-66cd-4d93-a071-7b076db6fbf4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Engineering New Features</h2>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07a4d2-f28d-4ad1-a618-fde9e0753d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore number of unique categories in categorical columns\n",
    "print(\"Training Data:\")\n",
    "print(X_train[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(X_val[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nTest Data:\")\n",
    "print(X_test[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965f441-5b9b-4c4f-8d6d-1a3f40998771",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Profession-Based Features</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Job Stability</strong> <br>\n",
    "    ðŸ“Œ Derive job stability from profession.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63250d-c76a-42b9-b8c6-a4f0d94b9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_job_stability(profession):\n",
    "    job_stability_map = {\n",
    "        # Government and highly regulated roles with exceptional job security\n",
    "        \"civil_servant\": \"very_stable\",\n",
    "        \"army_officer\": \"very_stable\",\n",
    "        \"police_officer\": \"very_stable\",\n",
    "        \"magistrate\": \"very_stable\",\n",
    "        \"official\": \"very_stable\",\n",
    "        \"air_traffic_controller\": \"very_stable\",\n",
    "        \"firefighter\": \"very_stable\",\n",
    "        \"librarian\": \"very_stable\",\n",
    "        \n",
    "        # Licensed/regulated professionals with strong job security\n",
    "        \"physician\": \"stable\",\n",
    "        \"surgeon\": \"stable\",\n",
    "        \"dentist\": \"stable\",\n",
    "        \"chartered_accountant\": \"stable\",\n",
    "        \"civil_engineer\": \"stable\",\n",
    "        \"mechanical_engineer\": \"stable\",\n",
    "        \"chemical_engineer\": \"stable\",\n",
    "        \"petroleum_engineer\": \"stable\",\n",
    "        \"biomedical_engineer\": \"stable\",\n",
    "        \"engineer\": \"stable\",\n",
    "        \n",
    "        # Corporate roles with steady demand\n",
    "        \"software_developer\": \"moderate\",\n",
    "        \"computer_hardware_engineer\": \"moderate\",\n",
    "        \"financial_analyst\": \"moderate\",\n",
    "        \"industrial_engineer\": \"moderate\",\n",
    "        \"statistician\": \"moderate\",\n",
    "        \"microbiologist\": \"moderate\",\n",
    "        \"scientist\": \"moderate\",\n",
    "        \"geologist\": \"moderate\",\n",
    "        \"economist\": \"moderate\",\n",
    "        \"technology_specialist\": \"moderate\",\n",
    "        \"design_engineer\": \"moderate\",\n",
    "        \"architect\": \"moderate\",\n",
    "        \"surveyor\": \"moderate\",\n",
    "        \"secretary\": \"moderate\",\n",
    "        \"flight_attendant\": \"moderate\",\n",
    "        \"hotel_manager\": \"moderate\",\n",
    "        \"computer_operator\": \"moderate\",\n",
    "        \"technician\": \"moderate\",\n",
    "        \n",
    "        # Project-based or variable demand roles\n",
    "        \"web_designer\": \"variable\",\n",
    "        \"fashion_designer\": \"variable\",\n",
    "        \"graphic_designer\": \"variable\",\n",
    "        \"designer\": \"variable\",\n",
    "        \"consultant\": \"variable\",\n",
    "        \"technical_writer\": \"variable\",\n",
    "        \"artist\": \"variable\",\n",
    "        \"comedian\": \"variable\",\n",
    "        \"chef\": \"variable\",\n",
    "        \"analyst\": \"variable\",\n",
    "        \"psychologist\": \"variable\",\n",
    "        \"drafter\": \"variable\",\n",
    "        \"aviator\": \"variable\",\n",
    "        \"politician\": \"variable\",\n",
    "        \"lawyer\": \"variable\"\n",
    "    }\n",
    "\n",
    "    # Return the job stability score based on the profession (default to \"moderate\" for unknown categories)\n",
    "    return job_stability_map.get(profession, \"moderate\")\n",
    "    \n",
    "# Apply function to create job stability feature in training, validation, and test data\n",
    "X_train[\"job_stability\"] = X_train[\"profession\"].map(derive_job_stability)\n",
    "X_val[\"job_stability\"] = X_val[\"profession\"].map(derive_job_stability)\n",
    "X_test[\"job_stability\"] = X_test[\"profession\"].map(derive_job_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b0bae-6b21-4c17-9b87-f2c63d1ca0cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Location-Based Features</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "    <strong>City Tier</strong> </br>\n",
    "    ðŸ“Œ Derive city tier from city. Specifically, categorize cities into three tiers that reflect differences in employment opportunities, income levels, cost of living, population densitiy, and economic activity.\n",
    "<ul>\n",
    "  <li>Tier 1: Large metropolitan cities with high population density, significant economic activity, and robust infrastructure. India's most developed and urbanized cities.</li>\n",
    "  <li>Tier 2: Medium-sized cities with growing industries, regional importance, and moderate economic activity. Less urbanized than Tier 1.</li>\n",
    "  <li>Tier 3: Smaller cities or towns with limited industrial and economic activity, often rural or semi-urban areas.</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eca54-93ec-43b5-a815-3221da489f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_city_tier(city):\n",
    "    tier_map = {\n",
    "        # Tier 1 cities\n",
    "        \"new_delhi\": \"tier_1\",\n",
    "        \"navi_mumbai\": \"tier_1\",\n",
    "        \"kolkata\": \"tier_1\",\n",
    "        \"bangalore\": \"tier_1\",\n",
    "        \"chennai\": \"tier_1\",\n",
    "        \"hyderabad\": \"tier_1\",\n",
    "        \"mumbai\": \"tier_1\",\n",
    "        \"pune\": \"tier_1\",\n",
    "        \"ahmedabad\": \"tier_1\",\n",
    "        \"jaipur\": \"tier_1\",\n",
    "        \"lucknow\": \"tier_1\",\n",
    "        \"noida\": \"tier_1\",\n",
    "        \"coimbatore\": \"tier_1\",\n",
    "        \"surat\": \"tier_1\",\n",
    "        \"nagpur\": \"tier_1\",\n",
    "        \"kochi\": \"tier_1\",\n",
    "        \"thiruvananthapuram\": \"tier_1\",\n",
    "        \"kanpur\": \"tier_1\",\n",
    "        \"patna\": \"tier_1\",\n",
    "        \n",
    "        # Tier 2 cities\n",
    "        \"bhopal\": \"tier_2\",\n",
    "        \"vijayawada\": \"tier_2\",\n",
    "        \"indore\": \"tier_2\",\n",
    "        \"jodhpur\": \"tier_2\",\n",
    "        \"vadodara\": \"tier_2\",\n",
    "        \"ludhiana\": \"tier_2\",\n",
    "        \"madurai\": \"tier_2\",\n",
    "        \"agra\": \"tier_2\",\n",
    "        \"mysore[7][8][9]\": \"tier_2\",\n",
    "        \"rajkot\": \"tier_2\",\n",
    "        \"nashik\": \"tier_2\",\n",
    "        \"amritsar\": \"tier_2\",\n",
    "        \"ranchi\": \"tier_2\",\n",
    "        \"chandigarh_city\": \"tier_2\",\n",
    "        \"allahabad\": \"tier_2\",\n",
    "        \"bhubaneswar\": \"tier_2\",\n",
    "        \"varanasi\": \"tier_2\",\n",
    "        \"jabalpur\": \"tier_2\",\n",
    "        \"guwahati\": \"tier_2\",\n",
    "        \"tiruppur\": \"tier_2\",\n",
    "        \"raipur\": \"tier_2\",\n",
    "        \"udaipur\": \"tier_2\",\n",
    "        \"gwalior\": \"tier_2\",\n",
    "        \n",
    "        # Tier 3 cities\n",
    "        \"vijayanagaram\": \"tier_3\",\n",
    "        \"bulandshahr\": \"tier_3\",\n",
    "        \"saharsa[29]\": \"tier_3\",\n",
    "        \"hajipur[31]\": \"tier_3\",\n",
    "        \"satara\": \"tier_3\",\n",
    "        \"ongole\": \"tier_3\",\n",
    "        \"bellary\": \"tier_3\",\n",
    "        \"giridih\": \"tier_3\",\n",
    "        \"hospet\": \"tier_3\",\n",
    "        \"khammam\": \"tier_3\",\n",
    "        \"danapur\": \"tier_3\",\n",
    "        \"bareilly\": \"tier_3\",\n",
    "        \"satna\": \"tier_3\",\n",
    "        \"howrah\": \"tier_3\",\n",
    "        \"thanjavur\": \"tier_3\",\n",
    "        \"farrukhabad\": \"tier_3\",\n",
    "        \"buxar[37]\": \"tier_3\",\n",
    "        \"arrah\": \"tier_3\",\n",
    "        \"thrissur\": \"tier_3\",\n",
    "        \"proddatur\": \"tier_3\",\n",
    "        \"bahraich\": \"tier_3\",\n",
    "        \"nandyal\": \"tier_3\",\n",
    "        \"siwan[32]\": \"tier_3\",\n",
    "        \"barasat\": \"tier_3\",\n",
    "        \"dhule\": \"tier_3\",\n",
    "        \"begusarai\": \"tier_3\",\n",
    "        \"khandwa\": \"tier_3\",\n",
    "        \"guntakal\": \"tier_3\",\n",
    "        \"latur\": \"tier_3\",\n",
    "        \"karaikudi\": \"tier_3\"\n",
    "    }\n",
    "    \n",
    "    # Return city tier based on the city (default to \"unknown\" for unknown categories)\n",
    "    return tier_map.get(city, \"unknown\")\n",
    "\n",
    "# Apply function to create city tier feature in training, validation, and test data\n",
    "X_train[\"city_tier\"] = X_train[\"city\"].map(derive_city_tier)\n",
    "X_val[\"city_tier\"] = X_val[\"city\"].map(derive_city_tier)\n",
    "X_test[\"city_tier\"] = X_test[\"city\"].map(derive_city_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d2bf-5854-406a-98d7-77bb5cd7f083",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "    <strong>State Default Rate</strong> </br>\n",
    "    ðŸ“Œ Derive state default rate from state using target encoding.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab2f19-eade-4898-a091-0b58aa213179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Calculate default rate by state based on the training data\n",
    "default_rate_by_state = df_train.groupby(\"state\")[\"risk_flag\"].mean()\n",
    "\n",
    "# Create state default rate feature in training, validation, and test data by replacing the state with its corresponding default rate\n",
    "X_train[\"state_default_rate\"] = X_train[\"state\"].map(default_rate_by_state)\n",
    "X_val[\"state_default_rate\"] = X_val[\"state\"].map(default_rate_by_state)\n",
    "X_test[\"state_default_rate\"] = X_test[\"state\"].map(default_rate_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0420a-3d11-4dfd-ac78-fb7e54a7a5f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Defining Semantic Type</h2>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Define semantic column types (numerical, categorical, boolean) for downstream tasks like additional preprocessing steps, exploratory data analysis, and machine learning.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d07e51-218e-4826-99cc-73de5abf3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic column types manually\n",
    "numerical_columns = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\"]\n",
    "categorical_columns = [\"house_ownership\", \"job_stability\", \"city_tier\", \"profession\", \"city\", \"state\"]\n",
    "boolean_columns = [\"risk_flag\", \"married\", \"car_ownership\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332edd65-5789-48e4-8aa6-d055d94711b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Missing Values</h2>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37db6-2329-44af-90ab-ec03a0e6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Training Data - Features:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(\"\\nTraining Data - Target Variable:\")\n",
    "print(y_train.isnull().sum())\n",
    "\n",
    "print(\"\\nValidation Data - Features:\")\n",
    "print(X_val.isnull().sum())\n",
    "print(\"\\nValidation Data - Target Variable:\")\n",
    "print(y_val.isnull().sum())\n",
    "\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e30f60-8cfa-419d-a915-e37a9ddde4ac",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> âœ… No missing values were found in any of the columns in the training, validation, and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386d0a-d26f-462c-96f4-4b557da2943c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Outliers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7375e-b714-44f3-b184-c28af316e22a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">3SD Method</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b38504-167d-42e9-9bd5-ab44c552929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "            \n",
    "        # Calculate statistics (mean, standard deviation, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"mean\"] = df[self.numerical_columns_].mean()\n",
    "        self.stats_[\"sd\"] = df[self.numerical_columns_].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"sd\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"sd\"]\n",
    "        \n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "     \n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_3sd.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show outliers in training data\n",
    "print(f\"Training data: Identified {outlier_remover_3sd.outliers_} rows ({outlier_remover_3sd.outliers_ / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "print(\"Statistics and outliers by column:\")\n",
    "round(outlier_remover_3sd.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dd47c-ef92-425a-89c0-bca0ae30982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = outlier_remover_3sd.transform(X_train)\n",
    "print(f\"Training Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_val_no_outliers = outlier_remover_3sd.transform(X_val)\n",
    "print(f\"Validation Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_test_no_outliers = outlier_remover_3sd.transform(X_test)\n",
    "print(f\"Test Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858552d1-4764-4364-8440-30409e324a57",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">1.5 IQR Method </h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ðŸ“Œ Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7b0d-c5dd-4820-974e-445571f160da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "        \n",
    "        # Calculate statistics (first quartile, third quartile, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"Q1\"] = df[self.numerical_columns_].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[self.numerical_columns_].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "               \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_iqr.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show outliers by column for training data\n",
    "print(f\"Training data: Identified {outlier_remover_iqr.outliers_} rows ({outlier_remover_iqr.outliers_ / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "print(\"Statistics and outliers by column:\")\n",
    "round(outlier_remover_iqr.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6555a-4991-4861-a177-e21b6c8d356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default rate by state\n",
    "default_rate_by_state.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c757cd-3857-4b44-80bd-9284973c7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = outlier_remover_iqr.transform(X_train)\n",
    "print(f\"Training Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_val_no_outliers = outlier_remover_iqr.transform(X_val)\n",
    "print(f\"Validation Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_test_no_outliers = outlier_remover_iqr.transform(X_test)\n",
    "print(f\"Test Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443aedcf-68b9-457b-8d7e-0d9830e424ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Isolation Forest</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Identify and remove multivariate outliers using the isolation forest algorithm.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8d2e0-bca4-4d41-a331-4de343e9852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize isolation forest\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Create list of numerical and boolean features (without the target variable \"risk_flag\")\n",
    "numerical_boolean_features = numerical_columns + [\"married\", \"car_ownership\"]\n",
    "\n",
    "# Fit isolation forest on training data\n",
    "isolation_forest.fit(X_train[numerical_boolean_features])\n",
    "\n",
    "# Predict outliers on training, validation, and test data\n",
    "X_train[\"outlier\"] = isolation_forest.predict(X_train[numerical_boolean_features])\n",
    "X_train[\"outlier_score\"] = isolation_forest.decision_function(X_train[numerical_boolean_features])\n",
    "X_val[\"outlier\"] = isolation_forest.predict(X_val[numerical_boolean_features])\n",
    "X_val[\"outlier_score\"] = isolation_forest.decision_function(X_val[numerical_boolean_features])\n",
    "X_test[\"outlier\"] = isolation_forest.predict(X_test[numerical_boolean_features])\n",
    "X_test[\"outlier_score\"] = isolation_forest.decision_function(X_test[numerical_boolean_features])\n",
    "\n",
    "# Show number of outliers\n",
    "n_outliers_train = X_train[\"outlier\"].value_counts()[-1]\n",
    "contamination_train = X_train[\"outlier\"].value_counts()[-1] / X_train[\"outlier\"].value_counts().sum()\n",
    "print(f\"Training Data: Identified {n_outliers_train} rows ({100 * contamination_train:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_val = X_val[\"outlier\"].value_counts()[-1]\n",
    "contamination_val = X_val[\"outlier\"].value_counts()[-1] / X_val[\"outlier\"].value_counts().sum()\n",
    "print(f\"Validation Data: Identified {n_outliers_val} rows ({100 * contamination_val:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_test = X_test[\"outlier\"].value_counts()[-1]\n",
    "contamination_test = X_test[\"outlier\"].value_counts()[-1] / X_test[\"outlier\"].value_counts().sum()\n",
    "print(f\"Test Data: Identified {n_outliers_test} rows ({100 * contamination_test:.1f}%) as multivariate outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7c69a-dd5a-4d3f-9686-92c0f1ded562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to visualize outliers for a subsample of the training data\n",
    "X_train_subsample = X_train[numerical_boolean_features + [\"outlier\"]].sample(n=1000, random_state=42)\n",
    "sns.pairplot(X_train_subsample, hue=\"outlier\", palette={1: \"#4F81BD\", -1: \"#D32F2F\"}, plot_kws={\"alpha\":0.6, \"s\":40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2890-0ad1-4ef6-8438-373b5d958c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = X_train[X_train[\"outlier\"] == 1]\n",
    "print(f\"Training Data: Removed {X_train[X_train['outlier'] == -1].shape[0]} rows ({X_train[X_train['outlier'] == -1].shape[0] / X_train.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_val_no_outliers = X_val[X_val[\"outlier\"] == 1]\n",
    "print(f\"Validation Data: Removed {X_val[X_val['outlier'] == -1].shape[0]} rows ({X_val[X_val['outlier'] == -1].shape[0] / X_val.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_test_no_outliers = df_test[X_test[\"outlier\"] == 1]\n",
    "print(f\"Test Data: Removed {X_test[X_test['outlier'] == -1].shape[0]} rows ({X_test[X_test['outlier'] == -1].shape[0] / X_test.shape[0] * 100:.1f}%) with multivariate outliers.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c818cb-9883-4a56-9762-53b1cb61a637",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Feature Scaling and Encoding</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "ðŸ“Œ Use a <code>ColumnTransformer</code> to preprocess columns based on their semantic type. This allows the appropriate transformation to each semantic column type in a single step.\n",
    "<ul>\n",
    "  <li>Scale numerical columns: <code>StandardScaler</code> to transform numerical columns to have mean = 0 and standard deviation = 1.</li>\n",
    "  <li>Encode categorical columns:</li>\n",
    "    <ul>\n",
    "      <li>Nominal columns (unordered categories): <code>OneHotEncoder</code> to convert string categories into binary (one-hot) encoded columns.</li>\n",
    "      <li>Ordinal columns (ordered categories): <code>OrdinalEncoder</code> to convert string categories into integers.</li>\n",
    "    </ul>\n",
    "  <li>Retain boolean columns: Pass through boolean columns unchanged using <code>remainder=\"passthrough\"</code>.</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74802f13-c326-403e-ba65-0786a78246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nominal and ordinal columns\n",
    "nominal_columns = [\"house_ownership\"]\n",
    "ordinal_columns = [\"job_stability\", \"city_tier\"]\n",
    "\n",
    "# Define the explicit order of categories for all ordinal columns\n",
    "ordinal_column_orders = [\n",
    "    [\"variable\", \"moderate\", \"stable\", \"very_stable\"],  # Order for job_stability\n",
    "    [\"unknown\", \"tier_3\", \"tier_2\", \"tier_1\"]  # Order for city_tier\n",
    "]\n",
    "\n",
    "# Initialize a column transformer \n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), numerical_columns), \n",
    "        (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit column transformer on the training data \n",
    "column_transformer.fit(X_train)\n",
    "\n",
    "# Apply feature scaling and encoding to training, validation and test data\n",
    "X_train_transformed = column_transformer.transform(X_train)\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "# Get transformed column names\n",
    "nominal_encoded_columns = list(column_transformer.named_transformers_[\"nominal_encoder\"].get_feature_names_out())\n",
    "passthrough_columns = list(X_train.columns.difference(numerical_columns + nominal_columns + ordinal_columns, sort=False))\n",
    "transformed_columns = numerical_columns + nominal_encoded_columns + ordinal_columns + passthrough_columns\n",
    "\n",
    "# Convert transformed data from arrays to DataFrames with column names \n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=transformed_columns)\n",
    "X_val_transformed = pd.DataFrame(X_val_transformed, columns=transformed_columns)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=transformed_columns)\n",
    "\n",
    "# Convert transformed data types\n",
    "X_train_transformed[numerical_columns] = X_train_transformed[numerical_columns].astype(float)\n",
    "X_train_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_train_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "X_val_transformed[numerical_columns] = X_val_transformed[numerical_columns].astype(float)\n",
    "X_val_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_val_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "X_test_transformed[numerical_columns] = X_test_transformed[numerical_columns].astype(float)\n",
    "X_test_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_test_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "\n",
    "# Reset the index to match the untransformed DataFrames\n",
    "X_train_transformed.index = X_train_transformed[\"id\"] - 1\n",
    "X_val_transformed.index = X_val_transformed[\"id\"] - 1\n",
    "X_test_transformed.index = X_test_transformed[\"id\"] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c9c2-14c5-4b4a-bad8-f9a2b71f839d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Saving Data</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Save preprocessed data from a Pandas DataFrame to a <code>.csv</code> file in the <code>data</code> directory.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afee6f-836b-4efc-a74a-93e4b8677660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"data\" directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Merge transformed X features and y target variable\n",
    "df_train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "df_val_transformed = pd.concat([X_val_transformed, y_val], axis=1)\n",
    "df_test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "\n",
    "# Save as .csv  \n",
    "df_train_transformed.to_csv(\"data/training_data_preprocessed.csv\", index=False)\n",
    "df_val_transformed.to_csv(\"data/validation_data_preprocessed.csv\", index=False)\n",
    "df_test_transformed.to_csv(\"data/test_data_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dcb62-3ed6-4437-a100-ae5c317edd5e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Exploratory Data Analysis (EDA)</h1>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae0b1c-675f-41b3-89ab-6d4f3cb308f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Univariate EDA</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Analyze the distribution of a single column using descriptive statistics and visualizations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a00fe4-0b5d-4266-94cf-8145c92f3c83",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Examine descriptive statistics (e.g., mean, median, standard deviation) and visualize the distributions (e.g., histograms) of numerical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e14d9-3d91-4860-a5e8-1d536e78716a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br>\n",
    "    ðŸ“Œ Examine descriptive statistics of numerical columns. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfd81f-2d6a-4200-9e00-fa158797e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "X_train[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66f2fc-c59b-44aa-beba-146ee896aef4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    ðŸ“Œ Histogram matrix that shows the distribution of all 6 numerical columns in a 2x3 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb5218-7f14-4612-b73b-af6fab378901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over all numerical columns\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Create histogram for the current column\n",
    "    sns.histplot(X_train[column])\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(column.title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19845aad-bacf-400a-a5cd-b10ccddbb4c7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Customize the histograms of income, current job years, and state default rate for better interpretability.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f3222-63eb-434b-a666-ac1bd989518b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Income</b>: Format histogram x-axis tick labels in millions (M).</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d4e48-891a-4a97-871b-ba7259370d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Histogram \n",
    "sns.histplot(X_train[\"income\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"\")\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1000000:.0f}M\"))   # Format the x-axis tick labels in millions\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075b03a-9881-48ac-94a0-56e689eb0c8c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Current Job Years</b>: Set histogram x-axis tick labels from 0 to 14 in steps of 1.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe712e-ff7d-4dfe-9cc6-6d91fa9eb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram \n",
    "sns.histplot(X_train[\"current_job_yrs\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Current Job Years\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(np.arange(0, 15, 1))  # Set x-axis tick labels from 0 to 14 in steps of 1\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3682a94-0fa2-44e8-a854-f84134303ff5",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>State Default Rate</b>: Format histogram x-axis tick labels in %.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd17dd-fc0b-46cb-a7fe-14f62644cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram \n",
    "sns.histplot(X_train[\"state_default_rate\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"State Default Rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"\")\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x * 100:.1f}%\"))   # Format the x-axis tick labels in %\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9af566-4914-4c75-af6d-a929bee130a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Examine descriptive statistics (absolute and relative frequencies) and visualize the distributions (e.g., bar plots) of categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774077bf-c131-4e01-a869-82ae5c7f02dc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br> \n",
    "    ðŸ“Œ Calculate and show the absolute and relative frequencies of all categorical and boolean columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb3385-b3c3-46f5-b661-9d2bfdf0f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train to have all categorical columns (including features and target variable) in a single DataFrame\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "\n",
    "# Function to create frequency tables for all categorical columns \n",
    "def calculate_frequencies(df, categorical_columns):\n",
    "    # Initialize dictionary to store all frequency tables \n",
    "    frequencies = {}\n",
    "    for column in categorical_columns:\n",
    "        # Calculate absolute and relative frequencies for current column\n",
    "        absolute_frequencies = df[column].value_counts()\n",
    "        relative_frequencies = df[column].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "        # Create frequency table\n",
    "        frequency_table = pd.concat([absolute_frequencies, relative_frequencies], axis=1).reset_index()\n",
    "        frequency_table.columns = [\"categorical_label\", \"absolute_frequency\", \"relative_frequency\"] \n",
    "        \n",
    "        # Add frequency table for current column to dictionary \n",
    "        frequencies[column] = frequency_table\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "# Apply function to create frequency tables\n",
    "frequencies = calculate_frequencies(df_train, categorical_columns + boolean_columns)\n",
    "\n",
    "# Show frequency tables\n",
    "for column, frequency_table in frequencies.items():\n",
    "    print(f\"{column.title()} Frequencies:\")\n",
    "    print(f\"{frequency_table}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d8d0-b416-423b-8b90-02b8b0fc4a79",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Examine profession, city, and state separately for better interpretability due to their large number of categories.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ffe89-f4a2-4d7b-a1c0-f15f5841079d",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Profession</b>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3c41a-f7e7-49a1-8c08-8dc815b3b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies[\"profession\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b658f-25a5-482b-bbf8-1d2eec7d0c54",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>City</b>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6216576-ad25-4fb3-a8c0-4fd06e1877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies[\"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d924e-09c1-4ab9-8a26-25c6c8238dcd",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>State</b>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63990c-2ee5-4bbc-9c73-f82353ce403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies[\"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff349806-af1b-47e3-ab49-3cc4090c8746",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    ðŸ“Œ Bar plot matrix that shows the frequency distribution of all 9 categorical and boolean columns in a 3x3 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2daa752-5f8a-4802-80af-1f302a20c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Iterate over the categorical and boolean columns\n",
    "for i, column in enumerate(boolean_columns + categorical_columns):\n",
    "    # Create a subplot in a 3x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Calculate frequencies for the current column\n",
    "    column_frequencies = df_train[column].value_counts(normalize=True)  # For absolute frequencies: normalize=False\n",
    "    \n",
    "    # Create bar plot for the current column\n",
    "    sns.barplot(x=column_frequencies.index, y=column_frequencies.values)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(column.title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    # Rotate x-axis tick labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba37dc1-2770-4eab-abbc-f9dcdcc10321",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Customize the bar plots of profession, city, and state for better interpretability due to their large number of categories.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaf557-0459-46ad-87f7-b26f76ad1cf9",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Profession</b>: Horizontal bar plot with larger figure size and value labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651052f4-b1e2-49b2-b442-086a243d41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=frequencies[\"profession\"][\"relative_frequency\"], \n",
    "                 y=frequencies[\"profession\"][\"categorical_label\"])\n",
    "\n",
    "# Add value labels inside the bars\n",
    "for i, value in enumerate(frequencies[\"profession\"][\"relative_frequency\"]):\n",
    "    ax.text(value - 0.05,  # x position (0.05% from right end)\n",
    "            i,    # y position\n",
    "            f\"{value:.1f}%\",  # text (frequency with 1 decimal place)\n",
    "            ha=\"right\",  # horizontal alignment\n",
    "            va=\"center\") # vertical alignment\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Profession\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c82360-f5db-4f81-ad51-1d5190a45a40",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>City</b>: Horizontal bar plot of the 50 most frequent cities (317 total) with larger figure size and value labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ff497-b343-4121-86ce-18e311da51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 50 cities for better readability\n",
    "top_cities = frequencies[\"city\"].nlargest(50, \"relative_frequency\")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=top_cities[\"relative_frequency\"], \n",
    "                 y=top_cities[\"categorical_label\"])\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(top_cities[\"relative_frequency\"]):\n",
    "    # Add dynamic positioning: Value label inside for large bars vs. outside for small bars\n",
    "    threshold = max(top_cities[\"relative_frequency\"]) * 0.9  \n",
    "    if value > threshold:\n",
    "        # Place label inside for large bars\n",
    "        ax.text(value - 0.01, i, f\"{value:.2f}%\", ha=\"right\", va=\"center\")\n",
    "    else:\n",
    "        # Place label outside for small bars\n",
    "        ax.text(value + 0.01, i, f\"{value:.2f}%\", ha=\"left\", va=\"center\")\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"City\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea33ea-92e5-43ce-83d8-7252077741ae",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>State</b>: Horizontal bar plot with larger figure size and value labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a31bc-033e-424d-b1f3-314a0218f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=frequencies[\"state\"][\"relative_frequency\"],\n",
    "            y=frequencies[\"state\"][\"categorical_label\"]) \n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(frequencies[\"state\"][\"relative_frequency\"]):\n",
    "    # Add dynamic positioning: Value label inside for large bars vs. outside for small bars\n",
    "    threshold = max(frequencies[\"state\"][\"relative_frequency\"]) * 0.8  \n",
    "    if value > threshold:\n",
    "        # Place label inside for large bars\n",
    "        ax.text(value - 0.1, i, f\"{value:.1f}%\", ha=\"right\", va=\"center\")\n",
    "    else:\n",
    "        # Place label outside for small bars\n",
    "        ax.text(value + 0.1, i, f\"{value:.1f}%\", ha=\"left\", va=\"center\")\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"State\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910582f-4d19-47d1-a4d8-4e40cd9af55d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Bivariate EDA</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Analyze relationships between two columns using correlations and group-wise statistics and visualize relationships using scatter plots and bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a10ff-b9a4-4245-a353-526e855b3d0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical x Numerical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Analyze relationships between two numerical columns using correlation coefficients and visualize relationships using scatter plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e58f08-960c-4c26-89a0-723a7823b7b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Correlations</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Correlation heatmap of all numerical and boolean columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f19c9-ae06-4236-9756-a3baa5cc69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix and round to 2 decimals\n",
    "correlation_matrix = round(df_train[boolean_columns + numerical_columns].corr(), 2)\n",
    "\n",
    "# Format column names\n",
    "formatted_column_names = correlation_matrix.columns.str.title().str.replace(\"_\", \" \")  \n",
    "\n",
    "# Create upper triangle mask (k=1 excludes diagonal)\n",
    "mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) \n",
    "\n",
    "# Set upper triangle to NaN to avoid redundancy\n",
    "correlation_matrix[mask] = np.nan\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    cmap=\"viridis\",  # Colorblind-friendly colormap (other options: \"cividis\", \"magma\", \"YlOrBr\", \"RdBu\") \n",
    "    annot=True,  # Show correlation values\n",
    "    fmt=\".2f\",  # Ensure uniform decimal formatting\n",
    "    linewidth=0.5  # Thin white lines between cells\n",
    ")\n",
    "ax.set_xticklabels(formatted_column_names)\n",
    "ax.set_yticklabels(formatted_column_names)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save heatmap to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"correlation_heatmap.png\")  # In \"images\" directory as .png file \n",
    "if not os.path.exists(image_path):\n",
    "    try:    \n",
    "        plt.savefig(\n",
    "            image_path, \n",
    "            bbox_inches=\"tight\",  # Removes unnecessary whitespace\n",
    "            dpi=144  # Good balance of quality vs. file size\n",
    "        )\n",
    "        print(f\"Correlation heatmap saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving correlation heatmap: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving correlation heatmap: '{image_path}' already exists.\")\n",
    "    \n",
    "# Show heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e014f-9f2c-48f5-8f3a-2cfb30511f7b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Feature-target correlations by order of magnitude.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3ba7d-e006-42e1-8926-4de5c84e07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-target correlations sorted by absolute values in descending order \n",
    "df_train[numerical_columns + boolean_columns].corr()[\"risk_flag\"].sort_values(key=abs, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6411282-2fa8-4a1c-a4d6-4676fd57d6ef",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸ’¡ No numerical or boolean feature has a strong linear relationship with the target variable. However, they may have non-linear relationships.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef0d64-6a45-4a88-a6ba-a42f5658ec10",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Scatter plot matrix that visualizes the relationships between all 6 numerical columns, resulting in 15 scatter plots in a 5x3 matrix.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c2ce6-d558-409d-9101-ee622f7ff6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "def scatterplot_matrix(df, numerical_columns):\n",
    "    # Get all possible pairs of numerical columns\n",
    "    column_pairs = list(itertools.combinations(numerical_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns needed for the subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    # Adjust the multipliers (4, 4) to make plots larger or smaller\n",
    "    plt.figure(figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Create scatter plots for each combination\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Create subplot\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(data=df, x=column_1, y=column_2)\n",
    "        \n",
    "        # Add title and labels\n",
    "        plt.title(f\"{column_2.title()} vs. {column_1.title()}\")\n",
    "        plt.xlabel(column_1.title())\n",
    "        plt.ylabel(column_2.title())\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Use function to create scatter plot matrix\n",
    "scatterplot_matrix(df_train, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e1f18-21a1-4d66-8578-b576a6b114e2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Customize some scatter plots for better interpretability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb1a4b-365d-4b5d-b6ef-c6b5b4bb715b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Experience and Current Job Years</b>: Larger figure size and axis tick labels in 1-year steps.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1e0d-0b8e-424d-b326-cd821c2c2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Create scatter plot \n",
    "sns.scatterplot(data=df_train, x=\"experience\", y=\"current_job_yrs\")\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Relationship Between Years of Work Experience and Years in Current Job\")\n",
    "plt.xlabel(\"Years of Work Experience\")\n",
    "plt.ylabel(\"Years in Current Job\")\n",
    "plt.xticks(range(0, 21, 1))\n",
    "plt.yticks(range(0, 15, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dee6f-bad0-4f41-9428-15b1aa7c5022",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical x Categorical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Analyze relationships between a numerical column and a categorical column using group-wise statistics (e.g., median or mean by category) and visualize relationships using bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0e9de-db02-41d1-b482-0ba3f45baa1b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Group-Wise Statistics</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Descriptive statistics of all 6 numerical columns by risk flag (defaulted on loan: 0=no, 1=yes). Focus on median, mean, and std for easier readability.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4c477-c154-4157-a2e4-a7dd7c21b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_riskflag = df_train[numerical_columns].groupby(df_train[\"risk_flag\"]).agg([\"median\", \"mean\", \"std\"]).transpose()\n",
    "stats_by_riskflag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b233-b8d1-462e-a6f7-79602dfa2631",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Effect Size: Quantify the magnitude of the differences between people who defaulted vs. not defaulted on a loan using Cohen's d.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6469094-f8ab-48f4-8528-f0c178336f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(df, numerical_column, categorical_column, group1, group2):\n",
    "    x1 = df[df[categorical_column] == group1][numerical_column]\n",
    "    x2 = df[df[categorical_column] == group2][numerical_column]\n",
    "    \n",
    "    mean1, mean2 = np.mean(x1), np.mean(x2)\n",
    "    std1, std2 = np.std(x1, ddof=1), np.std(x2, ddof=1)  # Sample standard deviation using Nâˆ’1\n",
    "    n1, n2 = len(x1), len(x2)\n",
    "    \n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    return (mean1 - mean2) / pooled_std if pooled_std != 0 else 0\n",
    "\n",
    "# Use function to calculate Cohen's d for all numerical columns\n",
    "cohens_d_results = {column: cohens_d(df_train, column, \"risk_flag\", 1, 0) for column in numerical_columns}\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "cohens_d_df = pd.DataFrame.from_dict(cohens_d_results, orient=\"index\", columns=[\"Cohen's d\"])\n",
    "cohens_d_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dc890-3977-4a33-9339-2c35e7abbf59",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸ’¡ The difference between people who defaulted vs. not defaulted on a loan is very small across all 6 numerical features, indicating that the numerical features provide very little separation between the two classes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d83310-bdc2-4eb6-aa25-9a1b837d79fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Bar plot matrix that visualizes the relationship between all 6 numerical columns and risk flag in a 2x3 matrix.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a3195-ddf3-47db-adfe-9207c4e62a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterate over all numerical columns\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Create a bar plot for the current numerical column by risk flag\n",
    "    sns.barplot(data=df_train, x=\"risk_flag\", y=column, estimator=np.median, errorbar=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(f\"Median {column.title()} by Risk Flag\")\n",
    "    plt.xlabel(\"Risk Flag\")\n",
    "    plt.ylabel(column.title())\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc8dab-ec4b-4f11-8449-6773e99e3392",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical x Categorical</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Analyze relationships between two categorical columns using contingency tables and visualize relationships using grouped bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1eed5-f837-4289-8ede-854be3b58297",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Contingency Tables</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Contingency table between two categorical columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c5c76-6246-4aa0-b9db-443e717b00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all categorical and boolean columns\n",
    "categorical_columns + boolean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765a2d-0145-4f65-8c6f-2b7486f09653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table with relative frequencies (%)\n",
    "contingency_table = pd.crosstab(df_train[\"married\"], df_train[\"risk_flag\"], normalize=True) * 100\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2a56d-fdd4-405b-af82-d4eaf30405d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of contingency table\n",
    "sns.heatmap(\n",
    "    contingency_table, \n",
    "    cmap=\"viridis\",  # Colorblind-friendly colormap (other options: \"cividis\", \"magma\", \"YlOrBr\", \"RdBu\") \n",
    "    annot=True,  # Show frequencies (%)\n",
    "    fmt=\".1f\",  # Format as float with 1 decimal\n",
    "    linewidth=0.5  # Thin white lines between cells\n",
    ")\n",
    "plt.title(f\"{contingency_table.index.name.title().replace('_', ' ')} vs. {contingency_table.columns.name.title().replace('_', ' ')}\")\n",
    "plt.xlabel(contingency_table.index.name.title().replace('_', ' '))\n",
    "plt.ylabel(contingency_table.columns.name.title().replace('_', ' '))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e332c-1fcb-4b92-b96c-b461fd2e857d",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Contingency tables between all pairs of categorical columns.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a81365-682c-400c-a935-2592c7f6804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store contingency tables\n",
    "crosstabs = {}\n",
    "\n",
    "# Get all possible pairs of categorical and boolean columns\n",
    "categorical_column_pairs = tuple(itertools.combinations(categorical_columns + boolean_columns, 2))\n",
    "\n",
    "# Create contingency tables for all column pairs\n",
    "for column_1, column_2 in categorical_column_pairs:\n",
    "    crosstabs[(column_1, column_2)] = pd.crosstab(df_train[column_1], df_train[column_2], normalize=True) * 100\n",
    "\n",
    "# Display one example\n",
    "for column_pair, contigeny_table in crosstabs.items():\n",
    "    print(f\"\\nContingency Table: {column_pair[0].title().replace('_', ' ')} vs. {column_pair[1].title().replace('_', ' ')}\")\n",
    "    print(contigeny_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebc102-6922-4a18-9c56-23e37ebfe423",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Grouped bar plot between two categorical columns.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d4bcc-0d65-4f6b-b9c9-7abacd6c6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar plot \n",
    "contingency_table.plot(kind=\"bar\")\n",
    "plt.title(f\"{contingency_table.index.name.title()} vs. {contingency_table.columns.name.title()}\")\n",
    "plt.xlabel(contingency_table.index.name.title())\n",
    "plt.ylabel(\"Frequency (%)\")\n",
    "plt.legend(title=f\"{contingency_table.columns.name.title()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbcff8-35ef-45c9-905a-269edd087304",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">ðŸ“Œ Grouped bar plot matrix that visualizes the relationships between pairs of categorical columns, resulting in 15 grouped bar plots in a 5x3 matrix.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996cf73-14a5-4963-b43f-f4c2b6d00964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter categorical columns with low cardinality (max. 10 categories)\n",
    "categorical_columns_low_cardinality = [column for column in categorical_columns if df_train[column].nunique() <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635f3b4-0271-4fbc-9dd5-d2b3be7398c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_barplot_matrix(df, categorical_columns):\n",
    "    # Get all possible pairs of categorical columns\n",
    "    column_pairs = tuple(itertools.combinations(categorical_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns needed for the subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    # Adjust the multipliers (4, 4) to make plots larger or smaller\n",
    "    plt.figure(figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Create grouped bar plots for each pair\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Create subplot\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # Create contingency table with frequency (%)\n",
    "        contingency_table = pd.crosstab(df[column_1], df[column_2], normalize=True) * 100\n",
    "        \n",
    "        # Create grouped bar plot\n",
    "        contingency_table.plot(kind=\"bar\", ax=ax)\n",
    "        \n",
    "        # Add title and labels\n",
    "        plt.title(f\"{column_1.title().replace('_', ' ')} vs. {column_2.title().replace('_', ' ')}\")\n",
    "        plt.xlabel(column_1.title().replace('_', ' '))\n",
    "        plt.ylabel(\"Frequency (%)\")\n",
    "        plt.legend(title=f\"{column_2.title().replace('_', ' ')}\")\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Use function to create grouped bar plot matrix \n",
    "grouped_barplot_matrix(df_train, categorical_columns_low_cardinality + boolean_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b521a-4a4b-40e6-a561-3dbd0d59d458",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Modeling</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Helper functions to save and load models using <code>pickle</code>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641d606-cb48-4ef2-8497-14f782c9c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save model as .pkl file\n",
    "def save_model(model, filename):\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    # Save model as .pkl file \n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "        print(f\"Model saved successfully under 'models/{filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "\n",
    "# Function to load model from .pkl file \n",
    "def load_model(filename):\n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"rb\") as file:  # ensure model is stored in \"models\" directory\n",
    "            model = pickle.load(file)\n",
    "        print(f\"{filename} loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59829c-68d0-44b3-8d4d-137de88a5251",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Baseline Models</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Train 8 baseline models (default hyperparameter values):  \n",
    "    <ul>\n",
    "        <li>Logistic Regression</li>\n",
    "        <li>Elastic Net Logistic Regression</li>\n",
    "        <li>K-Nearest Neighbors Classifier</li>\n",
    "        <li>Support Vector Classifier</li>\n",
    "        <li>Decision Tree Classifier</li>\n",
    "        <li>Random Forest Classifier</li>\n",
    "        <li>Multi-Layer Perceptron Classifier</li>\n",
    "        <li>XGBoost Classifier</li>\n",
    "    </ul>\n",
    "    Train each model with 4 outlier handling methods:\n",
    "    <ul>\n",
    "        <li>3SD Method</li>\n",
    "        <li>1.5 IQR Method</li>\n",
    "        <li>Isolation Forest</li>\n",
    "        <li>No Outlier Handling</li>\n",
    "    </ul>\n",
    "    ðŸŽ¯ Evaluate model performance:  \n",
    "    <ul>\n",
    "        <li>Primary Metric:\n",
    "            <ul>\n",
    "                <li>Area Under the Precision-Recall Curve (AUC-PR)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Secondary Metrics:\n",
    "            <ul>\n",
    "                <li>Recall (Class 1)</li>\n",
    "                <li>Precision (Class 1)</li>\n",
    "                <li>F1-score (Class 1)</li>\n",
    "            </ul>     \n",
    "        </li>\n",
    "        <li>Additional Diagnostics:\n",
    "            <ul>\n",
    "                <li>AUC-PR Comparison by Model and Outlier Handler (Grouped Bar Chart)</li>\n",
    "                <li>Precision-Recall Curves</li>\n",
    "                <li>Classification Report</li>\n",
    "                <li>Confusion Matrix</li>                \n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64394a7b-aa6b-4fa6-a20b-2737bda11484",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Feature selection.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cc49e-61f7-4d20-8b89-d08e46d40561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to be used for model training\n",
    "columns_to_keep = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\", \n",
    "                   \"house_ownership_owned\", \"house_ownership_rented\", \"job_stability\", \"city_tier\", \"married\", \"car_ownership\"]\n",
    "X_train_transformed = X_train_transformed[columns_to_keep]\n",
    "X_val_transformed = X_val_transformed[columns_to_keep]\n",
    "X_test_transformed = X_test_transformed[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135f7f7-1d36-49b6-90a9-c9035074c06d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Training</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Train each baseline model with each outlier handler and store fitted models, predicted values, and evaluation metrics in a results dictionary.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf6996-3d74-4fc6-9a1f-33440c6f55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Elastic Net\": LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True, max_iter=1000),\n",
    "    \"Neural Network\": MLPClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define outlier handlers\n",
    "outlier_handlers = {\n",
    "    \"No Outlier Handling\": None,\n",
    "    \"3SD Method\": OutlierRemover3SD(),\n",
    "    \"1.5 IQR Method\": OutlierRemoverIQR(),\n",
    "    \"Isolation Forest\": IsolationForest(contamination=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate a single model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Fit model on the training data and measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()    \n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate model: AUC-PR\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "    auc_pr = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    # Evaluate model: Accuracy, precision, recall, and f1-score\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred)\n",
    "\n",
    "    # Get model name\n",
    "    model_name = model.__class__.__name__\n",
    "    # Adjust name for Elastic Net Logistic Regression\n",
    "    if isinstance(model, LogisticRegression) and model.penalty == \"elasticnet\":\n",
    "        model_name = \"ElasticNet\" \n",
    "\n",
    "    # Return fitted model, predicted values, and evaluation metrics\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"model_name\": model_name,\n",
    "        \"training_time\": end_time - start_time,\n",
    "        \"y_val_pred\": y_val_pred,\n",
    "        \"y_val_proba\": y_val_proba,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": precision[1],\n",
    "        \"Recall (Class 1)\": recall[1],\n",
    "        \"F1-Score (Class 1)\": f1[1]\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# tree = evaluate_model(models[\"Decision Tree\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "# knn = evaluate_model(models[\"K-Nearest Neighbors\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "# svc = evaluate_model(models[\"Support Vector Machine\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Function to train and evaluate all models \n",
    "def evaluate_all_models(models, X_train, y_train, X_val, y_val):\n",
    "    results = {}   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        result = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "        results[model_name] = result\n",
    "        print(f\"Training Time: {round(result['training_time'], 1)} sec\")\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "# model_results = evaluate_all_models(models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Function to train and evaluate all models with all outlier handling methods\n",
    "def evaluate_all_models_and_outlier_handlers(models, outlier_handlers, X_train, y_train, X_val, y_val, numerical_columns):\n",
    "    results = {}\n",
    "    for outlier_handler_name, outlier_handler in outlier_handlers.items():\n",
    "        # Show current outlier handler\n",
    "        print(f\"\\nOutlier Handling: {outlier_handler_name}...\")\n",
    "    \n",
    "        # Initialize sub-dictionary for current outlier handler\n",
    "        results[outlier_handler_name] = {}\n",
    "    \n",
    "        # Handle outliers depending on method\n",
    "        if outlier_handler_name in [\"3SD Method\", \"1.5 IQR Method\"]:\n",
    "            # Identify and remove outliers on training data\n",
    "            X_train_no_outliers = outlier_handler.fit_transform(X_train, numerical_columns)\n",
    "            y_train_no_outliers = y_train.loc[outlier_handler.final_mask_]\n",
    "            n_outliers_train = (~outlier_handler.final_mask_).sum()\n",
    "            pct_outliers_train = n_outliers_train / len(outlier_handler.final_mask_) * 100\n",
    "            print(f\"Training Data: Removed {n_outliers_train} rows ({pct_outliers_train:.1f}%) with outliers.\")\n",
    "    \n",
    "            # Identify and remove outliers on validation data\n",
    "            X_val_no_outliers = outlier_handler.transform(X_val)\n",
    "            y_val_no_outliers = y_val.loc[outlier_handler.final_mask_]\n",
    "            n_outliers_val = (~outlier_handler.final_mask_).sum()\n",
    "            pct_outliers_val = n_outliers_val / len(outlier_handler.final_mask_) * 100\n",
    "            print(f\"Validation Data: Removed {n_outliers_val} rows ({pct_outliers_val:.1f}%) with outliers.\")\n",
    "        \n",
    "        elif outlier_handler_name == \"Isolation Forest\":\n",
    "            # Fit isolation forest on training data\n",
    "            outlier_handler.fit(X_train[numerical_columns])\n",
    "            \n",
    "            # Identify and remove outliers on training data\n",
    "            outlier_train = outlier_handler.predict(X_train[numerical_columns])\n",
    "            X_train_no_outliers = X_train[outlier_train == 1].copy()\n",
    "            y_train_no_outliers = y_train.iloc[outlier_train == 1].copy()\n",
    "            n_outliers_train = (outlier_train == -1).sum()\n",
    "            pct_outliers_train = n_outliers_train / len(outlier_train) * 100\n",
    "            print(f\"Training Data: Removed {n_outliers_train} rows ({pct_outliers_train:.1f}%) as multivariate outliers.\")\n",
    "            \n",
    "            # Identify and remove outliers on validation data\n",
    "            outlier_val = outlier_handler.predict(X_val[numerical_columns])\n",
    "            X_val_no_outliers = X_val[outlier_val == 1].copy()\n",
    "            y_val_no_outliers = y_val.iloc[outlier_val == 1].copy()\n",
    "            n_outliers_val = (outlier_val == -1).sum()\n",
    "            pct_outliers_val = n_outliers_val / len(outlier_val) * 100\n",
    "            print(f\"Validation Data: Removed {n_outliers_val} rows ({pct_outliers_val:.1f}%) as multivariate outliers.\")    \n",
    "    \n",
    "        else:\n",
    "            # No outlier handling\n",
    "            X_train_no_outliers = X_train.copy()\n",
    "            y_train_no_outliers = y_train.copy()\n",
    "            X_val_no_outliers = X_val.copy()\n",
    "            y_val_no_outliers = y_val.copy()\n",
    "    \n",
    "        # Train all models with current outlier handler\n",
    "        results[outlier_handler_name] = evaluate_all_models(models, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Use function to train 8 baseline models with 4 outlier handling methods\n",
    "# baseline_model_results = evaluate_all_models_and_outlier_handlers(baseline_models, outlier_handlers, X_train_transformed, y_train, X_val_transformed, y_val, numerical_columns)\n",
    "\n",
    "# Save baseline model results as .pkl file (using save_model helper function)  \n",
    "# save_model(baseline_model_results, \"baseline_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e193f6a-60e2-4c1a-acd8-65f8ab015cfb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">AUC-PR Comparison</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Compare AUC-PR by model and outlier handling method using a grouped bar chart.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1695345-dde9-4c91-b2b5-9c2ff825cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model results (using load_model helper function)\n",
    "baseline_model_results = load_model(\"baseline_models.pkl\")\n",
    "\n",
    "# Create DataFrame for grouped bar chart\n",
    "auc_pr_df = pd.DataFrame({\n",
    "    \"Model\": baseline_models.keys(),\n",
    "    \"No Outlier Handling\": [baseline_model_results[\"No Outlier Handling\"][model][\"AUC-PR\"] for model in baseline_models.keys()],\n",
    "    \"3SD Method\": [baseline_model_results[\"3SD Method\"][model][\"AUC-PR\"] for model in baseline_models.keys()],\n",
    "    \"1.5 IQR Method\": [baseline_model_results[\"1.5 IQR Method\"][model][\"AUC-PR\"] for model in baseline_models.keys()],\n",
    "    \"Isolation Forest\": [baseline_model_results[\"Isolation Forest\"][model][\"AUC-PR\"] for model in baseline_models.keys()]\n",
    "})\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "auc_pr_df = pd.melt(\n",
    "    auc_pr_df,\n",
    "    id_vars=[\"Model\"], \n",
    "    value_vars=[\"No Outlier Handling\", \"3SD Method\", \"1.5 IQR Method\", \"Isolation Forest\"],\n",
    "    var_name=\"Outlier Handling\", \n",
    "    value_name=\"AUC-PR\"\n",
    ")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create grouped bar chart\n",
    "ax = sns.barplot(x=\"Model\", y=\"AUC-PR\", hue=\"Outlier Handling\", data=auc_pr_df, palette=\"viridis\")\n",
    "\n",
    "# Add title, labels, ticks, legend, axis limits, and grid\n",
    "plt.title(\"AUC-PR Comparison by Baseline Model and Outlier Handling Method\", fontsize=16, pad=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"AUC-PR\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"aucpr_comparison_baseline.png\")  # In \"images\" directory as .png file \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        plt.savefig(\n",
    "            image_path, \n",
    "            bbox_inches=\"tight\",  # Removes unnecessary whitespace\n",
    "            dpi=144  # Good balance of quality vs. file size\n",
    "        )\n",
    "        print(f\"AUC-PR comparison plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving AUC-PR comparison plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving AUC-PR comparison plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7b55e-2ef7-4e85-bd86-0a64b658621b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Comparison Tables</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Compare baseline model metrics with a separate table for each outlier handling method.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef9bcc-1826-4843-8701-6dbae20febb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract evaluation metrics\n",
    "metrics = {}  \n",
    "for outlier_handler_name in outlier_handlers.keys():\n",
    "    # Extract metrics for each model under the current outlier handler\n",
    "    metrics[outlier_handler_name] = {\n",
    "        model_name: {\n",
    "            metric: baseline_model_results[outlier_handler_name][model_name][metric]\n",
    "            for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "        }\n",
    "        for model_name in baseline_model_results[outlier_handler_name]\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame \n",
    "    metrics[outlier_handler_name] = pd.DataFrame(metrics[outlier_handler_name]).transpose()\n",
    "    \n",
    "# Show model results without outlier handling\n",
    "metrics[\"No Outlier Handling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088007-4de0-4806-a0b1-7c19e1417bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show model results with 3SD outlier handler\n",
    "metrics[\"3SD Method\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a36009-440e-4600-b9df-e4e390bb4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model results with 1.5 IQR outlier handler\n",
    "metrics[\"1.5 IQR Method\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20e3da-df41-46ae-9be4-8bc07e2e5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model results with isolation forest\n",
    "metrics[\"Isolation Forest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce5347-ee5a-4a47-9ae9-dbce70ffe61e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ’¡ Since outlier handling does not improve AUC-PR and may even slightly reduce performance, hyperparameter tuning and other downstream modeling steps will proceed without applying it.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7bac9-5678-4590-9da0-a428275c4588",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Precision-Recall Curves</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Plot precision-recall curves for all baseline models in a single graph (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38613e7-7fbe-4bc1-af0f-cd3b2c849d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot precision-recall curve of one or more models\n",
    "def plot_precision_recall_curve(y_true, model_results, model_stage):\n",
    "    # Convert single model to list \n",
    "    if not isinstance(model_results, list):\n",
    "        model_results = [model_results]\n",
    "  \n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Get colors for different models (colormap \"viridis\" is colorblind-friendly and perceptually uniform)\n",
    "    cmap = plt.get_cmap(\"viridis\", len(model_results))\n",
    "    \n",
    "    # Plot baseline performance of random classifier\n",
    "    baseline = np.sum(y_true) / len(y_true)\n",
    "    plt.axhline(y=baseline, color=\"black\", linestyle=\"--\", alpha=0.5, label=f\"Baseline = {baseline:.2f}\")\n",
    "    \n",
    "    # Plot precision-recall curve for each model\n",
    "    for i, model_result in enumerate(model_results):\n",
    "        # Calculate precision-recall curve\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true, model_result[1][\"y_val_proba\"])\n",
    "        auc_pr = auc(recall_curve, precision_curve)\n",
    "        \n",
    "        # Plot the curve\n",
    "        plt.plot(recall_curve, precision_curve, color=cmap(i), label=f\"{model_result[0]} AUC-PR={auc_pr:.2f}\")\n",
    "    \n",
    "    # Add title, labels, legend, and grid\n",
    "    if model_stage in [\"baseline\", \"final\"]:\n",
    "        title = f\"Precision-Recall Curve{'s' if len(model_results) > 1 else ''}: {model_stage.capitalize()} Model{'s' if len(model_results) > 1 else ''}\"\n",
    "    elif model_stage == \"tuned\":\n",
    "        title = f\"Precision-Recall Curve{'s' if len(model_results) > 1 else ''}: Hyperparameter-{model_stage.capitalize()} Model{'s' if len(model_results) > 1 else ''}\"\n",
    "    else:\n",
    "        title = f\"Precision-Recall Curve{'s' if len(model_results) > 1 else ''}\"\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel(\"Precision\", fontsize=12)\n",
    "    plt.xlabel(\"Recall\", fontsize=12)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Set axis limits\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "\n",
    "    # Save plot to file\n",
    "    os.makedirs(\"images\", exist_ok=True)  \n",
    "    image_path = os.path.join(\"images\", f\"precision_recall_curve{'s' if len(model_results) > 1 else ''}_{model_stage}.png\")  # In \"images\" directory as .png file \n",
    "    if not os.path.exists(image_path):\n",
    "        try:        \n",
    "            plt.savefig(\n",
    "                image_path, \n",
    "                bbox_inches=\"tight\",  # Removes unnecessary whitespace\n",
    "                dpi=144  # Good balance of quality vs. file size\n",
    "            )\n",
    "            print(f\"Precision-recall curve plot saved successfully to '{image_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving precision-recall curve plot: {e}\")\n",
    "    else:\n",
    "        print(f\"Skip saving to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# plot_precision_recall_curve(y_val, [tree, knn])\n",
    "\n",
    "# Use function to plot precision-recall curves of all baseline models with no outlier handling\n",
    "model_results = list(baseline_model_results[\"No Outlier Handling\"].items())\n",
    "plot_precision_recall_curve(y_val, model_results, \"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45c631-db7f-4693-9f50-a228ca2406b4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report and Confusion Matrix</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Create classification report and plot confusion matrix for all baseline models (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f10397-1474-4c88-a8ad-8c124e9a030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report and confusion matrix for each model\n",
    "for model, result in baseline_model_results[\"No Outlier Handling\"].items():\n",
    "    # Evaluate model: Classification report\n",
    "    print(f\"{model}: Classification Report\")\n",
    "    print(classification_report(y_val, result[\"y_val_pred\"]))\n",
    "\n",
    "    # Evaluate model: Confusion matrix\n",
    "    cm = confusion_matrix(y_val, result[\"y_val_pred\"])\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    cm_disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{model}: Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Separation line\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488aa402-6ef8-4e00-a3e5-b82eed8173e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Hyperparameter Tuning</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ðŸ’¡ The following models outperformed the other candidates across the primary evaluation metric (AUC-PR) and the secondary metrics (class-1-specific recall, precision, and f1-score) on the validation data and were selected for hyperparameter tuning:  \n",
    "    <ul>\n",
    "        <li><b>Random Forest</b>: Achieved the highest AUC-PR (0.60) and demonstrated a balanced performance with a recall of 0.53, precision of 0.60, and the highest F1-score (0.56) among all models.</li>\n",
    "        <li><b>XGBoost</b>: Delivered a strong AUC-PR of 0.54, alongside a high precision of 0.64 despite a lower recall (0.20) and an overall F1-score of 0.30.</li>\n",
    "        <li><b>K-Nearest Neighbors</b>: Demonstrated robust performance with an AUC-PR of 0.53 and balanced secondary metricsâ€”recall of 0.50, precision of 0.56, and an F1-score of 0.53.</li>\n",
    "        <li><b>Decision Tree</b>: Achieved a solid AUC-PR of 0.46 with a strong recall of 0.57, indicating good sensitivity in detecting positive cases. Precision (0.52) and F1-score (0.55) also reflect a well-rounded performance.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e876c40-a6cf-46cc-a231-ada757fcd5e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Random Forest</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>â„¹ï¸ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees in the forest.</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features considered for the best split; default <code>\"auto\"</code> uses the square root of all features.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are supposed to have weight one. Use <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies in the input data.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">scikit-learn RandomForestClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c087c9c-d650-44df-8c14-3778fa210ac3",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd49c05-07f9-440c-8ae3-45f6b53eb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "rf_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(5, 31),  # Random integers between 5 and 30            \n",
    "    \"min_samples_split\": randint(2, 21),  # Random integers between 2 and 20\n",
    "    \"min_samples_leaf\": randint(1, 11),  # Random integers between 1 and 10\n",
    "    \"max_features\": uniform(0.1, 0.9),  # Random floats between 0.1 and 1.0  \n",
    "    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=rf_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# rf_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file (using save_model helper function)  \n",
    "# save_model(rf_random_search, \"rf_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbcac9-2c9b-402f-8df1-9de0f4d492ab",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Load random search from <code>.pkl</code> file and show DataFrame of Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2a5eb-230e-43c7-8642-1b4ae41085e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search (using load_model helper function) \n",
    "rf_random_search = load_model(\"rf_random_search.pkl\")\n",
    "\n",
    "# DataFrame of randomized search results\n",
    "rf_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": rf_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": rf_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "rf_random_search_results[\"n_estimators\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "rf_random_search_results[\"max_depth\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "rf_random_search_results[\"min_samples_split\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "rf_random_search_results[\"min_samples_leaf\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "rf_random_search_results[\"max_features\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "rf_random_search_results[\"class_weight\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"class_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_random_search_results = rf_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "rf_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f95ed-8eb7-4761-9124-618bd8530926",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">XGBoost</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>â„¹ï¸ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees (boosting rounds).</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree.</li>\n",
    "        <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li>\n",
    "        <li><code>subsample</code>: Fraction of training samples used per tree.</li>\n",
    "        <li><code>colsample_bytree</code>: Fraction of features used per tree.</li>\n",
    "        <li><code>gamma</code>: Minimum loss reduction required to split a leaf node.</li>\n",
    "        <li><code>min_child_weight</code>: Minimum sum of instance weights (hessian) in a child.</li>\n",
    "        <li><code>scale_pos_weight</code>: Balances positive and negative class weights for imbalanced datasets.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928573d-3194-4c76-be86-1a3dbe0f1454",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc244-2ccd-426f-98ff-a57ffef568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "xgb_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(3, 11),  # Random integers between 3 and 10            \n",
    "    \"learning_rate\": uniform(0.01, 0.29),  # Random floats between 0.01 and 0.30\n",
    "    \"subsample\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"gamma\": uniform(0, 0.5),  # Random floats between 0.0 and 0.5  \n",
    "    \"min_child_weight\": randint(1, 10),  # Random integers between 1 and 9\n",
    "    \"scale_pos_weight\": randint(1, 16)  # Random integers between 1 and 15\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=xgb_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# xgb_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file (using save_model helper function)  \n",
    "# save_model(xgb_random_search, \"xgb_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced2cd5-3c3f-4468-a427-563e1ef8ce86",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Load random search from <code>.pkl</code> file and show DataFrame of Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55717fdd-a4df-4947-96f0-e6174b70d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search (using load_model helper function) \n",
    "xgb_random_search = load_model(\"xgb_random_search.pkl\")\n",
    "\n",
    "# DataFrame of randomized search results\n",
    "xgb_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": xgb_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": xgb_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "xgb_random_search_results[\"n_estimators\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "xgb_random_search_results[\"max_depth\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "xgb_random_search_results[\"learning_rate\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"learning_rate\"])\n",
    "xgb_random_search_results[\"subsample\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"subsample\"])\n",
    "xgb_random_search_results[\"colsample_bytree\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"colsample_bytree\"])\n",
    "xgb_random_search_results[\"gamma\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"gamma\"])\n",
    "xgb_random_search_results[\"min_child_weight\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"min_child_weight\"])\n",
    "xgb_random_search_results[\"scale_pos_weight\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"scale_pos_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_random_search_results = xgb_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "xgb_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ed213-dd35-450d-b11d-6da1efb7e341",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Decision Tree</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>â„¹ï¸ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>max_depth</code>: Maximum depth of the tree. <code>None</code> allows nodes to expand until all leaves are pure or contain fewer samples than <code>min_samples_split</code>.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features to consider for the best split. If <code>None</code>, all features are considered.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are given equal weight. Can be <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies.</li>\n",
    "        <li><code>\"ccp_alpha\"</code>: Complexity parameter for pruning. A higher value encourages pruning by penalizing tree complexity.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\">scikit-learn DecisionTreeClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf937112-0826-4d9e-86d8-cf678970f52f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136f223-9f7b-4269-985e-9387c61d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "tree_param_distributions = {\n",
    "    \"max_depth\": randint(5, 31),  # Random integers between 5 and 30            \n",
    "    \"min_samples_split\": randint(2, 21),  # Random integers between 2 and 20\n",
    "    \"min_samples_leaf\": randint(1, 11),  # Random integers between 1 and 10\n",
    "    \"max_features\": uniform(0.1, 0.9),  # Random floats between 0.1 and 1.0  \n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"ccp_alpha\": uniform(0.0, 0.1)  # Random floats between 0.0 and 0.1\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "tree_random_search = RandomizedSearchCV(\n",
    "    estimator=tree, \n",
    "    param_distributions=tree_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# tree_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file (using save_model helper function)  \n",
    "# save_model(tree_random_search, \"tree_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383846-fd20-4aac-8fff-d6a28b2d54f1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Load random search from <code>.pkl</code> file and show DataFrame of Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa13fab-9525-4031-848c-a366d16a9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search (using load_model helper function) \n",
    "tree_random_search = load_model(\"tree_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "tree_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": tree_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": tree_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "tree_random_search_results[\"max_depth\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "tree_random_search_results[\"min_samples_split\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "tree_random_search_results[\"min_samples_leaf\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "tree_random_search_results[\"max_features\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "tree_random_search_results[\"class_weight\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"class_weight\"])\n",
    "tree_random_search_results[\"ccp_alpha\"] = tree_random_search_results[\"parameters\"].apply(lambda x: x[\"ccp_alpha\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "tree_random_search_results = tree_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "tree_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc51e-b71c-4d4d-b341-54e0b9e3628a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">K-Nearest Neighbors</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>â„¹ï¸ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "                <li><code>n_neighbors</code>: The number of neighbors to use for prediction. A higher value makes the model more general, while a lower value may lead to overfitting.</li>\n",
    "                <li><code>weights</code>: Determines how neighbors are weighted during prediction. <code>\"uniform\"</code> gives equal weight to all neighbors, while <code>\"distance\"</code> gives closer neighbors more influence.</li>\n",
    "                <li><code>p</code>: The power parameter for the Minkowski distance. <code>p=1</code> corresponds to the Manhattan distance and <code>p=2</code> to the Euclidean distance.</li>\n",
    "                <li><code>algorithm</code>: The algorithm used to compute nearest neighbors. <code>\"auto\"</code> selects the best algorithm based on the dataset (options include <code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>, and <code>\"brute\"</code>).</li>\n",
    "                <li><code>leaf_size</code>: Relevant for tree-based algorithms (<code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>), ignored for <code>\"brute\"</code>.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\">scikit-learn KNeighborsClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722dae3-e64f-4ae3-a5b7-8bb0d2fedae5",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dda29f-8792-463d-bb71-573ae5b8de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "knn_param_distributions = {\n",
    "    \"n_neighbors\": randint(3, 31),  # Random integers between 3 and 30            \n",
    "    \"weights\": [\"uniform\", \"distance\"],  # \"distance\" can help with imbalanced classes\n",
    "    \"p\": [1, 2],  # p=1 (Manhattan) and p=2 (Euclidean)\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  \n",
    "    \"leaf_size\": randint(20, 51)\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "knn_random_search = RandomizedSearchCV(\n",
    "    estimator=knn, \n",
    "    param_distributions=knn_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# knn_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file (using save_model helper function)  \n",
    "# save_model(knn_random_search, \"knn_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6409f90-4ed1-4275-ad7c-60db299c691f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Load random search from <code>.pkl</code> file and show DataFrame of Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180292e2-2d86-4513-bb25-e0bb26a45593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search (using load_model helper function) \n",
    "knn_random_search = load_model(\"knn_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "knn_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": knn_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": knn_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "knn_random_search_results[\"n_neighbors\"] = knn_random_search_results[\"parameters\"].apply(lambda x: x[\"n_neighbors\"])\n",
    "knn_random_search_results[\"weights\"] = knn_random_search_results[\"parameters\"].apply(lambda x: x[\"weights\"])\n",
    "knn_random_search_results[\"p\"] = knn_random_search_results[\"parameters\"].apply(lambda x: x[\"p\"])\n",
    "knn_random_search_results[\"algorithm\"] = knn_random_search_results[\"parameters\"].apply(lambda x: x[\"algorithm\"])\n",
    "knn_random_search_results[\"leaf_size\"] = knn_random_search_results[\"parameters\"].apply(lambda x: x[\"leaf_size\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "knn_random_search_results = knn_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "knn_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8ece1-4d37-43c7-85fc-298ae5d26488",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Selection</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Retrain the best model of each class on the full training dataset. Evaluate each model on the validation dataset using additional metrics (AUC-PR, class-1-specific recall, precision, and F1-score) and diagnostics (precision-recall curves, classification report, and confusion matrix). Select the final model based on overall performance.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a82e27-9b3a-4664-ad59-5899da11f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best-in-class models\n",
    "tuned_models = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(**knn_random_search.best_params_),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(**tree_random_search.best_params_, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_random_search.best_params_, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(**xgb_random_search.best_params_, random_state=42)\n",
    "}\n",
    "\n",
    "# Retrain 4 best-in-class models from hyperparameter tuning\n",
    "# tuned_model_results = evaluate_all_models(tuned_models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Save hyperparameter-tuned model results as .pkl file \n",
    "# save_model(tuned_model_results, \"tuned_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a443f-50da-42ca-861d-b8d1026fbf03",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Model comparison table.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40736775-ae3a-48c0-ae07-d6d526da0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter-tuned model results\n",
    "tuned_model_results = load_model(\"tuned_models.pkl\")\n",
    "\n",
    "# Extract evaluation metrics\n",
    "metrics = {\n",
    "    model_name: {\n",
    "        metric: tuned_model_results[model_name][metric]\n",
    "        for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "    }\n",
    "    for model_name in tuned_model_results\n",
    "}\n",
    "# Convert the dictionary to a DataFrame \n",
    "metrics = pd.DataFrame(metrics).transpose()\n",
    "    \n",
    "# Show model results \n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3e39b-2fb8-44d8-86f4-e08250e3e822",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Precision-recall curves.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9513d-8d7a-4b5c-96ac-b158049b678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curves of hyperparameter-tuned models\n",
    "model_results = list(tuned_model_results.items())\n",
    "plot_precision_recall_curve(y_val, model_results, \"tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e203d94-c899-4e0d-b580-9aae27de5d44",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Classification report and confusion matrix.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a784f67-2d91-4d71-9969-6f4e61f22aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report and confusion matrix for each model\n",
    "for model, result in tuned_model_results.items():\n",
    "    # Evaluate model: Classification report\n",
    "    print(f\"{model}: Classification Report\")\n",
    "    print(classification_report(y_val, result[\"y_val_pred\"]))\n",
    "\n",
    "    # Evaluate model: Confusion matrix\n",
    "    cm = confusion_matrix(y_val, result[\"y_val_pred\"])\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    cm_disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{model}: Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Separation line\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ef99d-9222-4772-a9b4-0447b9c1a5ca",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ’¡ Random forest and XGBoost demonstrate the best AUC-PR (0.60 each).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470af27b-2fb6-4b8b-948d-8259b82a5912",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Recall is typically more important in loan default prediction tasks than precision. Prioritizing recall helps catch more potential defaults, even at the cost of some false positives. This is typically preferable because missing a default (false negative) is generally more costly than flagging a non-default as risky (false positive).\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    ðŸ“Œ Determine the best threshold for each model that optimizes the F1-score while satisfying a minimum recall of 0.80 and a minimum precision of 0.50.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114ed5-9e1a-49ff-84c9-3fe95d92cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate single model across multiple decision thresholds \n",
    "def evaluate_thresholds(y_true, y_pred_proba, thresholds=None, min_recall=0, min_precision=0, model_name=None):\n",
    "    # Use 1% to 99% in 1%-steps in the absence of custom thresholds\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "    # Calculate metrics for each threshold and store in a results DataFrame\n",
    "    threshold_results = []    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred)        \n",
    "        threshold_results.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"Precision (Class 1)\": precision[1],\n",
    "            \"Recall (Class 1)\": recall[1],\n",
    "            \"F1-Score (Class 1)\": f1[1]\n",
    "        })    \n",
    "    threshold_results = pd.DataFrame(threshold_results)\n",
    "\n",
    "    # Determine the best threshold that optimizes F1-score while satisfying minimum recall and precision\n",
    "    filtered_thresholds = threshold_results[\n",
    "        (threshold_results[\"Recall (Class 1)\"] >= min_recall) & \n",
    "        (threshold_results[\"Precision (Class 1)\"] >= min_precision)\n",
    "    ]\n",
    "    best_threshold = filtered_thresholds.loc[filtered_thresholds[\"F1-Score (Class 1)\"].idxmax(), \"threshold\"]\n",
    "\n",
    "    # Plot metrics by threshold \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cmap = plt.get_cmap(\"viridis\", 3)\n",
    "    plt.plot(threshold_results[\"threshold\"], threshold_results[\"Precision (Class 1)\"], label=\"Precision\", color=cmap(0))\n",
    "    plt.plot(threshold_results[\"threshold\"], threshold_results[\"Recall (Class 1)\"], label=\"Recall\", color=cmap(1))\n",
    "    plt.plot(threshold_results[\"threshold\"], threshold_results[\"F1-Score (Class 1)\"], label=\"F1-Score\", color=cmap(2))\n",
    "\n",
    "    # Add line for best threshold \n",
    "    plt.axvline(x=best_threshold, color=\"gray\", linestyle=\"--\")\n",
    "    plt.text(best_threshold+0.02, 0.01, f\"Best Threshold: {best_threshold:.2f}\", rotation=90)\n",
    "    \n",
    "    # Add title, labels, ticks, legend, and grid\n",
    "    plt.title(f\"{model_name + ': ' if model_name else ''}Class-1 Metrics by Threshold\", fontsize=14)\n",
    "    plt.xlabel(\"Threshold\", fontsize=12)\n",
    "    plt.ylabel(\"Metric Value\", fontsize=12)\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to file\n",
    "    os.makedirs(\"images\", exist_ok=True)  \n",
    "    image_path = os.path.join(\"images\", f\"{'xgb' if model_name=='XGBoost' else 'rf'}_metrics_by_threshold_tuned.png\")  \n",
    "    if not os.path.exists(image_path):\n",
    "        try:        \n",
    "            plt.savefig(\n",
    "                image_path, \n",
    "                bbox_inches=\"tight\",  # Removes unnecessary whitespace\n",
    "                dpi=144  # Good balance of quality vs. file size\n",
    "            )\n",
    "            print(f\"Plot saved successfully to '{image_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "    else:\n",
    "        print(f\"Skip saving plot: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold, threshold_results\n",
    "\n",
    "# Use function to determine threshold with best F1-score while satisfying minimum recall 0.80 and precision 0.5\n",
    "rf_threshold, rf_threshold_results = evaluate_thresholds(\n",
    "    y_val, \n",
    "    tuned_model_results[\"Random Forest\"][\"y_val_proba\"], \n",
    "    min_recall=0.8, \n",
    "    min_precision=0.5,\n",
    "    model_name=\"Random Forest\")\n",
    "\n",
    "xgb_threshold, xgb_threshold_results = evaluate_thresholds(\n",
    "    y_val, \n",
    "    tuned_model_results[\"XGBoost\"][\"y_val_proba\"], \n",
    "    min_recall=0.8,\n",
    "    min_precision=0.5,\n",
    "    model_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d1e4b-0612-49d3-a1e3-79d234d99bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add best threshold and resulting predicted values to model results dictionary\n",
    "tuned_model_results[\"Random Forest\"][\"best_threshold\"] = rf_threshold\n",
    "tuned_model_results[\"Random Forest\"][\"y_val_pred_best\"] = (tuned_model_results[\"Random Forest\"][\"y_val_proba\"] >= rf_threshold).astype(int)\n",
    "tuned_model_results[\"XGBoost\"][\"best_threshold\"] = xgb_threshold\n",
    "tuned_model_results[\"XGBoost\"][\"y_val_pred_best\"] = (tuned_model_results[\"XGBoost\"][\"y_val_proba\"] >= xgb_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d806b49-7162-4df0-a532-a0eb470ecc85",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Classification report and confusion matrix using optimized threshold.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d8b4e-3645-4fd8-8343-893d4252312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report and confusion matrix for random forest and XGBoost with optimized thresholds\n",
    "for model, result in {model: tuned_model_results[model] for model in [\"Random Forest\", \"XGBoost\"]}.items():\n",
    "    # Evaluate model: Classification report\n",
    "    print(f\"{model} (Best Threshold): Classification Report\")\n",
    "    print(classification_report(y_val, result[\"y_val_pred_best\"]))\n",
    "\n",
    "    # Evaluate model: Confusion matrix\n",
    "    cm = confusion_matrix(y_val, result[\"y_val_pred_best\"])\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    cm_disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "    plt.title(f\"{model} (Best Threshold): Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Separation line\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab749b-552e-4a9c-9772-a38d438a427b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ’¡ Random Forest and XGBoost, with optimized decision thresholds, show similar performance. Given Random Forest's interpretability and regulatory compliance advantages in finance, it is selected as the final model. It will be further evaluated on test data to confirm its generalizability.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635ddf4-7c1c-4767-b8c0-aad06537395b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Final Model</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ The final model is a Random Forest with a decision threshold of 0.26 and the following hyperparameters:\n",
    "    <ul>\n",
    "        <li><code>n_estimators=225</code></li>\n",
    "        <li><code>max_depth=26</code></li>\n",
    "        <li><code>min_samples_split=2</code></li>\n",
    "        <li><code>min_samples_leaf=1</code></li>\n",
    "        <li><code>max_features=0.13</code></li>\n",
    "        <li><code>class_weight=\"balanced\"</code></li>\n",
    "    </ul>\n",
    "    ðŸŽ¯ Evaluate the final model on the test dataset: \n",
    "    <ul>\n",
    "        <li>Classification report</li>\n",
    "        <li>Confusion matrix</li>\n",
    "        <li>Feature relationships with misclassifications</li>\n",
    "    </ul>\n",
    "    Analyze feature importance, examine model prediction examples and save the model to file.\n",
    "</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d142bc1-12ba-4787-8f86-ee0a9bbc391d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Training</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    ðŸ“Œ Retrain the best model.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa992c-4526-49d9-b36a-97cb9171472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with tuned hyperparameters\n",
    "rf_final_model = RandomForestClassifier(**rf_random_search.best_params_, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "start_time = time.time()\n",
    "rf_final_model.fit(X_train_transformed, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13878f90-dedc-4bc9-857c-436893588136",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Evaluation</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    â„¹ï¸ Evaluate model performance (with decision threshold of 0.26) using primary metric (AUC-PR) and secondary metrics (class-1-specific recall, precision, and F1-score) for training, validation, and test data.\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>AUC-PR</strong> <br> \n",
    "    ðŸ“Œ Comparison table for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad2ea4-7fad-4cb1-bcaa-70f37d6a3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for class-1 (default) on the training, validation and test data\n",
    "y_train_proba = rf_final_model.predict_proba(X_train_transformed)[:, 1]\n",
    "y_val_proba = rf_final_model.predict_proba(X_val_transformed)[:, 1]\n",
    "y_test_proba = rf_final_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Apply threshold to convert probabilities to binary predictions\n",
    "threshold = 0.26\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "y_test_pred = (y_test_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f942bd-c3bb-4e55-baf4-4f659c628772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-PR on the training, validation, and test data\n",
    "precision_curve_train, recall_curve_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "precision_curve_val, recall_curve_val, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "precision_curve_test, recall_curve_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "\n",
    "auc_pr_train = auc(recall_curve_train, precision_curve_train)\n",
    "auc_pr_val = auc(recall_curve_val, precision_curve_val)\n",
    "auc_pr_test = auc(recall_curve_test, precision_curve_test)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Data\": [\"Training\", \"Validation\", \"Test\"],\n",
    "    \"AUC-PR\": [auc_pr_train, auc_pr_val, auc_pr_test]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873ddd-1a31-4188-a89a-bfa97d1522ea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Classification Report</strong> <br> \n",
    "    ðŸ“Œ Show classification report with class-1-specific recall, precision, and F1-score for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64847875-0e79-4dbf-87bb-5505fa991897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report: Training\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Classification Report: Validation\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Classification Report: Test\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5aea4-09c7-4c75-923e-a5edd5e40160",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Confusion Matrix</strong> <br> \n",
    "    ðŸ“Œ Show misclassifications using a confusion matrix for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c58d7-3fc3-44e4-813c-5ccc23752944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=[\"Non-Defaulter\", \"Defaulter\"])\n",
    "cm_disp_train.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix: Training\")\n",
    "plt.show()\n",
    "\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=[\"Non-Defaulter\", \"Defaulter\"])\n",
    "cm_disp_val.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix: Validation\")\n",
    "plt.show()\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "cm_disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=[\"Non-Defaulter\", \"Defaulter\"])\n",
    "cm_disp_test.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix: Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f164c-99e2-4e19-8250-12150972800c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Feature Relationships with Misclassifications</strong> <br> \n",
    "    ðŸ“Œ Explore relationships between the features and misclassifications through correlations, scatterplots, and box plots on the test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bdc9f-da1a-41b5-b21b-9e7b785c25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine transformed features with actual and predicted target values from the test data into a single DataFrame\n",
    "df_test = X_test_transformed.copy()  \n",
    "df_test[\"Actual\"] = y_test\n",
    "df_test[\"Predicted\"] = y_test_pred\n",
    "\n",
    "# Calculate misclassifications \n",
    "df_test[\"Misclassification\"] = df_test[\"Predicted\"] != df_test[\"Actual\"]\n",
    "\n",
    "# Define transformed boolean columns\n",
    "boolean_columns_transformed = [\"married\", \"car_ownership\", \"house_ownership_owned\", \"house_ownership_rented\"]\n",
    "\n",
    "# Correlations between features and misclassifications\n",
    "df_test[numerical_columns + boolean_columns_transformed + [\"Misclassification\"]].corr()[\"Misclassification\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de43ea8-dcf1-4661-863f-2757a377d611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scatterplot matrix between numerical features and misclassifications\n",
    "# Define the number of columns\n",
    "num_cols = 3  \n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_rows = math.ceil(len(numerical_columns) / num_cols)\n",
    "\n",
    "# Set the figure size dynamically based on the grid size\n",
    "plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "# Iterate over the numerical features \n",
    "for i, feature in enumerate(numerical_columns):\n",
    "    # Create a subplot in the grid \n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    \n",
    "    # Create a scatterplot between the current feature and the misclassifications\n",
    "    sns.scatterplot(data=df_test, x=feature, y=\"Misclassification\")\n",
    "    \n",
    "    # Add title and axis labels\n",
    "    plt.title(f\"Misclassifications by {feature}\")\n",
    "    plt.xlabel(f\"{feature}\")\n",
    "    plt.ylabel(\"Misclassification\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093e9f9-15ed-476d-8a16-d0c20300b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot matrix between numerical features and misclassifications\n",
    "# Define the number of columns\n",
    "num_cols = 3  \n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_rows = math.ceil(len(numerical_columns) / num_cols)\n",
    "\n",
    "# Set the figure size dynamically based on the grid size\n",
    "plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "# Iterate over the numerical features \n",
    "for i, feature in enumerate(numerical_columns):\n",
    "    # Create a subplot in the grid \n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    \n",
    "    # Create a scatterplot between the current feature and the misclassifications\n",
    "    sns.boxplot(data=df_test, x=\"Misclassification\", y=feature)\n",
    "    \n",
    "    # Add title and axis labels\n",
    "    plt.title(f\"{feature.title()} by Misclassification\")\n",
    "    plt.xlabel(\"Misclassification\")\n",
    "    plt.ylabel(f\"{feature.title()}\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c923a-c73a-4da9-bf97-5ee42157452f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Importance</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    ðŸ“Œ Visualize feature importances using a bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1ffda-51aa-489b-a576-ef584fec0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = rf_final_model.feature_importances_\n",
    "\n",
    "# Get feature names (and apply proper formatting)\n",
    "feature_names = X_train_transformed.columns.str.title().str.replace(\"_\", \" \")  \n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot of top 10 features\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.head(10), hue=\"feature\", palette=\"viridis\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Random Forest: Top 10 Most Important Features\", fontsize=14)\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(feature_importance_df[\"importance\"].head(10)):\n",
    "    ax.text(v + 0.001, i, f\"{v:.2f}\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"rf_feature_importance_final.png\")  \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "        print(f\"Feature importance plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature importance plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving feature importance plot: '{image_path}' already exists.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa50fb3-a933-4733-a7d0-cd088409a9ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Future Improvements</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815f4f-0f4f-456f-8eb3-6eaebf035b19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Data Enrichment**:  \n",
    "To enhance the model's performance and business value, data enrichment with the following financial features is recommended:\n",
    "- Loan amount \n",
    "- Loan duration\n",
    "- Interest rate\n",
    "- Type of loan (e.g., personal, home, vehicle)\n",
    "- Existing debt\n",
    "- Credit score\n",
    "\n",
    "**Enhanced Analysis Capabilities**:  \n",
    "The addition of these features, particularly loan amount, would enable more precise risk assessment and better alignment with business objectives through cost-sensitive evaluation metrics incorporating actual monetary values. Cost-sensitive metrics or expected monetary value incorporate the actual cost of defaults (false negatives) and the opportunity cost of rejecting good loans (false positives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-default-venv",
   "language": "python",
   "name": "loan-default-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
