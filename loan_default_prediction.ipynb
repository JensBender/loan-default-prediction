{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4275851-2816-4903-91c3-8b4a4c489b12",
   "metadata": {},
   "source": [
    "# Project Overview: Predicting Loan Defaults\n",
    "\n",
    "**Summary**  \n",
    "This project aims to develop a machine learning model to predict whether the customers of a financial institution will default on a loan based on data from their loan application. By accurately identifying potential defaulters, financial institutions can make more informed lending decisions, reduce losses, improve profitability, and increase operational efficiency through the automation of risk assessment.\n",
    "\n",
    "**Problem**  \n",
    "Predicting loan defaults is a challenging task due to the multitude of influencing factors such as customers' demographic, financial, location, and behavioral attributes. Traditional default prediction models often oversimplify complex relationships between customer features and default risk. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in loan application data, enabling more accurate predictions of loan default risk.\n",
    "\n",
    "**Objectives**  \n",
    "- Develop a machine learning model to predict loan defaults using customer data from loan applications.\n",
    "- Compare multiple models (e.g., Logistic Regression, Random Forest, XGBoost) using a suitable evaluation metric (such as AUC-PR).\n",
    "- Identify key factors influencing loan default risk through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project enables financial institutions to reduce loan default rates and make better and faster lending decisions by leveraging machine learning for automated and improved risk assessment. \n",
    "\n",
    "**Business Goals**  \n",
    "- Reduce losses by 5M-10M INR within 12 months of model deployment by decreasing the loan default rate by 10%-20%.\n",
    "- Decrease loan processing time by 25%-40% by automating risk assessment, leading to less time spent on manual evaluations.\n",
    "- Ensure 100% compliance with regulatory requirements and fair lending practices.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains information provided by customers of a financial institution during the loan application process. It is sourced from the \"Loan Prediction Based on Customer Behavior\" dataset by Subham Jain, available on [Kaggle](https://www.kaggle.com/datasets/subhamjain/loan-prediction-based-on-customer-behavior). The dataset consists of three `.csv` files:\n",
    "1. `Training Data.csv`: Contains the features, target variable (`Risk Flag`), and `ID` column from the training data. \n",
    "2. `Test Data.csv`: Contains the features and `ID` column from the test data.\n",
    "3. `Sample Prediction Dataset.csv`: Contains the target variable (`Risk Flag`) and `ID` column from the test data. \n",
    "\n",
    "Dataset Statistics:\n",
    "- Training set size: 252,000 records \n",
    "- Test set size: 28,000 records \n",
    "- Target variable: Risk flag (training: 12.3% defaults, test: 12.8% defaults)\n",
    "- Features: 11 \n",
    "  - Demographic: Age, married, profession\n",
    "  - Financial: Income, house ownership, car ownership\n",
    "  - Location: City, state\n",
    "  - Behavioral: Experience, current job years, current house years\n",
    "\n",
    "Data Overview Table:\n",
    "\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Training Data Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Risk Flag | Defaulted on loan (0: No, 1: Yes) | Integer | Categorical (Binary) | [0, 1] | [0, 1] |\n",
    "| Income | Income of the applicant | Integer | Numerical | [0, ∞] | [10K, 10M] |\n",
    "| Age | Age of the applicant (in years) | Integer | Numerical | [18, ∞] | [21, 79] |\n",
    "| Experience | Work experience (in years) | Integer | Numerical | [0, ∞] | [0, 20] |\n",
    "| Profession | Applicant's profession | String | Categorical (Nominal) | Any profession [e.g., \"Architect\", \"Dentist\"] | 51 unique professions |\n",
    "| Married | Marital status | String | Categorical (Binary) | [\"single\", \"married\"] | [\"single\", \"married\"] |\n",
    "| House Ownership | Applicant owns or rents a house | String | Categorical (Nominal) | [\"rented\", \"owned\", \"norent_noown\"] | [\"rented\", \"owned\", \"norent_noown\"] |\n",
    "| Car Ownership | Whether applicant owns a car | String | Categorical (Binary) | [\"yes\", \"no\"] | [\"yes\", \"no\"] |\n",
    "| Current Job Years | Years in the current job | Integer | Numerical | [0, ∞] | [0, 14] |\n",
    "| Current House Years | Years in the current house | Integer | Numerical | [0, ∞] | [10, 14] |\n",
    "| City | City of residence | String | Categorical (Nominal) | Any city [e.g., \"Mumbai\", \"Bangalore\"] | 317 unique cities |\n",
    "| State | State of residence | String | Categorical (Nominal) | Any state [e.g., \"Maharashtra\", \"Tamil_Nadu\"] | 29 unique states |\n",
    "\n",
    "Example Training Data:\n",
    "\n",
    "| Risk Flag | Income    | Age | Experience | Profession         | Married | House Ownership | Car Ownership | Current Job Years | Current House Years | City      | State         |\n",
    "| :-------- | :-------- | :-- | :--------- | :----------------- | :------ | :-------------- | :------------ | :---------------- | :------------------ | :-------- | :------------ |\n",
    "| 0         | 1,303,834 | 23  | 3          | Mechanical_engineer | single  | rented          | no            | 3                 | 13                   | Rewa      | Madhya_Pradesh |\n",
    "| 1         | 6,256,451 | 41  | 2          | Software_Developer | single  | rented          | yes           | 2                 | 12                   | Bangalore | Tamil_Nadu    |\n",
    "| 0         | 3,991,815 | 66  | 4          | Technical_writer   | married | rented          | no            | 4                 | 10                   | Alappuzha | Kerala        |\n",
    "\n",
    "**Technical Requirements**  \n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, data types, missing values, and outliers.\n",
    "  - Extract features, scale numerical features, and encode categorical features.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models and perform hyperparameter tuning for binary classification task with `sklearn` and `xgboost`.\n",
    "  - Baseline models: Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest, Multi-Layer Perceptron, XGBoost.\n",
    "  - Evaluate model performance using Area Under the Precision-Recall Curve (AUC-PR).\n",
    "    - AUC-PR is more suitable to address class imbalance (12.3% defaults) with a focus on the positive class (preventing defaults) than accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "    - Success criterion: Minimum AUC-PR of 0.70 on the test data.\n",
    "  - Potentially use additional techniques to address class imbalance (e.g., SMOTE, class weights).\n",
    "  - Visualize feature importance, show model prediction examples, and save the final model with `pickle`.\n",
    "- Deployment:\n",
    "  - Expose the final model via a REST API for easy integration with existing loan processing systems.\n",
    "  - Implement efficient batch processing capabilities to handle up to 10K predictions in under 30 seconds.\n",
    "  - Deploy using cloud infrastructure to ensure scalability and security.\n",
    "  - Set up model performance monitoring and data drift detection.\n",
    "- Stakeholders:\n",
    "  - Loan officers: Direct users of the model predictions in day-to-day loan approvals.\n",
    "  - Credit risk analysts: Provide subject matter expertise on loan default risk.\n",
    "  - Compliance officers: Ensure the model complies with any legal and regulatory guidelines.\n",
    "  - IT department: Manage the IT infrastructure and ensure data access for the model's development and deployment. \n",
    "\n",
    "By fulfilling these objectives and requirements, the project will provide a valuable tool for predicting loan defaults, thereby enhancing decision-making for financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9b38-149a-4c08-bb6a-c1ad97d33d28",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd00e3c4-73f9-45bb-a474-58fc6049796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7316e-f065-4b92-8f2c-5cd5681b5a69",
   "metadata": {},
   "source": [
    "# Data Loading and Inspection\n",
    "Load data from the three `.csv` files into three Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1e0114-5f1a-44ce-85e7-3136fe031aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"data/training_data.csv\")\n",
    "    X_test = pd.read_csv(\"data/test_data.csv\")\n",
    "    y_test = pd.read_csv(\"data/sample_prediction_dataset.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866b8d1-12e6-4f47-b54e-8132b55b4c1e",
   "metadata": {},
   "source": [
    "Initial data inspection to understand the structure of the dataset and detect obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc38bee-eefa-48ce-9ddb-b9464e6caa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252000 entries, 0 to 251999\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Id                 252000 non-null  int64 \n",
      " 1   Income             252000 non-null  int64 \n",
      " 2   Age                252000 non-null  int64 \n",
      " 3   Experience         252000 non-null  int64 \n",
      " 4   Married/Single     252000 non-null  object\n",
      " 5   House_Ownership    252000 non-null  object\n",
      " 6   Car_Ownership      252000 non-null  object\n",
      " 7   Profession         252000 non-null  object\n",
      " 8   CITY               252000 non-null  object\n",
      " 9   STATE              252000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    252000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  252000 non-null  int64 \n",
      " 12  Risk_Flag          252000 non-null  int64 \n",
      "dtypes: int64(7), object(6)\n",
      "memory usage: 25.0+ MB\n",
      "None\n",
      "\n",
      "Test Data - Features:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 28000 non-null  int64 \n",
      " 1   Income             28000 non-null  int64 \n",
      " 2   Age                28000 non-null  int64 \n",
      " 3   Experience         28000 non-null  int64 \n",
      " 4   Married/Single     28000 non-null  object\n",
      " 5   House_Ownership    28000 non-null  object\n",
      " 6   Car_Ownership      28000 non-null  object\n",
      " 7   Profession         28000 non-null  object\n",
      " 8   CITY               28000 non-null  object\n",
      " 9   STATE              28000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    28000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  28000 non-null  int64 \n",
      "dtypes: int64(6), object(6)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "Test Data - Target Variable:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   id         28000 non-null  int64\n",
      " 1   risk_flag  28000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 437.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.info())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ea3692-d2c8-46f1-b0ad-36372c1fa4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Married/Single</th>\n",
       "      <th>House_Ownership</th>\n",
       "      <th>Car_Ownership</th>\n",
       "      <th>Profession</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>CURRENT_JOB_YRS</th>\n",
       "      <th>CURRENT_HOUSE_YRS</th>\n",
       "      <th>Risk_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1303834</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Mechanical_engineer</td>\n",
       "      <td>Rewa</td>\n",
       "      <td>Madhya_Pradesh</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7574516</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Software_Developer</td>\n",
       "      <td>Parbhani</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3991815</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>married</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Technical_writer</td>\n",
       "      <td>Alappuzha</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6256451</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Software_Developer</td>\n",
       "      <td>Bhubaneswar</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5768871</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Civil_servant</td>\n",
       "      <td>Tiruchirappalli[10]</td>\n",
       "      <td>Tamil_Nadu</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Income  Age  Experience Married/Single House_Ownership Car_Ownership  \\\n",
       "0   1  1303834   23           3         single          rented            no   \n",
       "1   2  7574516   40          10         single          rented            no   \n",
       "2   3  3991815   66           4        married          rented            no   \n",
       "3   4  6256451   41           2         single          rented           yes   \n",
       "4   5  5768871   47          11         single          rented            no   \n",
       "\n",
       "            Profession                 CITY           STATE  CURRENT_JOB_YRS  \\\n",
       "0  Mechanical_engineer                 Rewa  Madhya_Pradesh                3   \n",
       "1   Software_Developer             Parbhani     Maharashtra                9   \n",
       "2     Technical_writer            Alappuzha          Kerala                4   \n",
       "3   Software_Developer          Bhubaneswar          Odisha                2   \n",
       "4        Civil_servant  Tiruchirappalli[10]      Tamil_Nadu                3   \n",
       "\n",
       "   CURRENT_HOUSE_YRS  Risk_Flag  \n",
       "0                 13          0  \n",
       "1                 13          0  \n",
       "2                 10          0  \n",
       "3                 12          1  \n",
       "4                 14          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the training data\n",
    "print(\"Training Data:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9890b354-634e-46b6-aaf0-0868ad775bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data - Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Married/Single</th>\n",
       "      <th>House_Ownership</th>\n",
       "      <th>Car_Ownership</th>\n",
       "      <th>Profession</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>CURRENT_JOB_YRS</th>\n",
       "      <th>CURRENT_HOUSE_YRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7393090</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Geologist</td>\n",
       "      <td>Malda</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1215004</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Firefighter</td>\n",
       "      <td>Jalna</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8901342</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>Thane</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1944421</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>married</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Latur</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>13429</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Comedian</td>\n",
       "      <td>Berhampore</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Income  Age  Experience Married/Single House_Ownership Car_Ownership  \\\n",
       "0   1  7393090   59          19         single          rented            no   \n",
       "1   2  1215004   25           5         single          rented            no   \n",
       "2   3  8901342   50          12         single          rented            no   \n",
       "3   4  1944421   49           9        married          rented           yes   \n",
       "4   5    13429   25          18         single          rented           yes   \n",
       "\n",
       "    Profession        CITY        STATE  CURRENT_JOB_YRS  CURRENT_HOUSE_YRS  \n",
       "0    Geologist       Malda  West Bengal                4                 13  \n",
       "1  Firefighter       Jalna  Maharashtra                5                 10  \n",
       "2       Lawyer       Thane  Maharashtra                9                 14  \n",
       "3      Analyst       Latur  Maharashtra                3                 12  \n",
       "4     Comedian  Berhampore  West Bengal               13                 11  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the test data features\n",
    "print(\"Test Data - Features:\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad411a4-8229-41b9-843b-ba280a6b6599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data - Target Variable:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>risk_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  risk_flag\n",
       "0   1          0\n",
       "1   2          0\n",
       "2   3          1\n",
       "3   4          0\n",
       "4   5          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the test data target variable\n",
    "print(\"Test Data - Target Variable:\")\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9596-7328-43a4-8cf6-a18d1b64b4f7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef27083-1da0-42ce-a1b0-ad15b1b7a495",
   "metadata": {},
   "source": [
    "## Standardizing Column Names and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af4f32-03ee-43b6-9289-6821b800328b",
   "metadata": {},
   "source": [
    "### Column Names\n",
    "Convert all column names to snake_case for consistency, improved readability, and to minimize the risk of errors. This also resolves inconsistencies in column names between the training and test datasets (e.g., \"Id\" vs. \"ID\", \"Risk_Flag\" vs. \"risk_flag\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c0e7b5f-3c5a-40c8-9f56-9d6040da7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake_case\n",
    "df_train.columns = (\n",
    "    df_train.columns\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  # Replace spaces and special characters with \"_\"\n",
    "    .str.replace(\"_single\", \"\")  # Shorten \"married_single\" to \"married\"\n",
    ")\n",
    "\n",
    "X_test.columns = (\n",
    "    X_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True) \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")\n",
    "\n",
    "y_test.columns = (\n",
    "    y_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7733ee-1f2d-48f3-92ae-1504a4b7f6d1",
   "metadata": {},
   "source": [
    "### Categorical Labels\n",
    "Standardize the state names (categorical labels) of the \"state\" column to resolve inconsistencies between the training and test datasets (e.g., \"Uttar_Pradesh\" vs. \"Uttar Pradesh\", \"Jammu_and_Kashmir\" vs. \"Jammu and Kashmir\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8c5e3b2-0cff-4253-8e60-ca868377217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_state_name(state_name):\n",
    "    # Replace spaces with \"_\" \n",
    "    return state_name.replace(\" \", \"_\")\n",
    "\n",
    "df_train[\"state\"] = df_train[\"state\"].apply(standardize_state_name)\n",
    "X_test[\"state\"] = X_test[\"state\"].apply(standardize_state_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3912db-7304-42d3-ac5d-bd1b413a5bd7",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2864227-c6dc-4d89-a603-060387350f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_test and y_test\n",
    "df_test = pd.merge(X_test, y_test, on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565d7a-e2b0-472d-91dd-f860878e384b",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28532f-00c9-4543-a9c1-29de96341820",
   "metadata": {},
   "source": [
    "Identify and remove duplicates based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d98e50-9696-47fe-b1ad-51ec59375e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated().value_counts())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b48ba-dc42-4591-bc44-e490092a79f2",
   "metadata": {},
   "source": [
    "No duplicates were found based on all columns in both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029181-c3e9-4588-8055-7770ab866eff",
   "metadata": {},
   "source": [
    "Identify and remove duplicates based on the ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e53c-dcfa-42eb-8542-7f215b4d3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.duplicated(subset=[\"id\"]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57954bf9-09ec-49df-8cb7-ed39d546d14f",
   "metadata": {},
   "source": [
    "No duplicates were found based on the ID column in both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89142d-7b4a-4b23-98b3-3c9e63ee5eb8",
   "metadata": {},
   "source": [
    "## Handling Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391177e-efa2-4e4c-ba49-ecb7e59fa56b",
   "metadata": {},
   "source": [
    "Identify and convert incorrect storage data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be913b0-2788-4e8d-8797-37233709dfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify storage data types\n",
    "print(\"Training Data:\")\n",
    "print(df_train.dtypes)\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da11a2-50d5-4c7a-907e-3b71830f0295",
   "metadata": {},
   "source": [
    "No incorrect storage data types were found at first glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb80b7-c29b-4dd9-83ce-1b6a94abdb67",
   "metadata": {},
   "source": [
    "Identify object columns with two unique categories and convert them to boolean columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0d39d-104b-47b4-8538-ffd2f55a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object columns with two unique categories \n",
    "print(\"Training Data:\")\n",
    "print(df_train[df_train.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test[df_test.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876603a-e2d3-493e-a1a7-cdf2233a8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert married and car_ownership column from object to boolean\n",
    "df_train[\"married\"] = df_train[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df_test[\"married\"] = df_test[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df_train[\"car_ownership\"] = df_train[\"car_ownership\"].map({\"yes\": True, \"no\": False})\n",
    "df_test[\"car_ownership\"] = df_test[\"car_ownership\"].map({\"yes\": True, \"no\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01e2e3-70e4-4659-bb73-d6ec7dcdc358",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split  \n",
    "The dataset is initially divided into a training set (90%) and a test set (10%). Split the training set further to achieve the following train-validation-test split: \n",
    "\n",
    "| Data           | Size (%) | Size (Total) | \n",
    "|:---------------|----------|---------|\n",
    "| Training Set   | 80%      | 224,000 |\n",
    "| Validation Set | 10%      | 28,000  | \n",
    "| Test Set       | 10%      | 28,000  | \n",
    "\n",
    "This results in 80% of the total data being used for training, 10% for validation, and 10% for testing, while keeping the original test set size unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee908d5-43d1-4098-8c78-8a4c61aacebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set further into training and validation sets (validation set size same as test set size)\n",
    "df_train, df_val = train_test_split(df_train, test_size=X_test.shape[0]/df_train.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413ec8b-66cd-4d93-a071-7b076db6fbf4",
   "metadata": {},
   "source": [
    "## Engineering New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07a4d2-f28d-4ad1-a618-fde9e0753d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore number of unique categories in categorical columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(df_val[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965f441-5b9b-4c4f-8d6d-1a3f40998771",
   "metadata": {},
   "source": [
    "### Profession-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856622d-2390-4f7a-8f9e-b020b5eabb0f",
   "metadata": {},
   "source": [
    "**Job Stability**  \n",
    "Derive job stability from profession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63250d-c76a-42b9-b8c6-a4f0d94b9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_job_stability(profession):\n",
    "    # Government and highly regulated roles with exceptional job security\n",
    "    very_stable = {\n",
    "        \"Civil_servant\", \"Army_officer\", \"Police_officer\", \"Magistrate\",\n",
    "        \"Official\", \"Air_traffic_controller\", \"Firefighter\", \"Librarian\"\n",
    "    }\n",
    "    \n",
    "    # Licensed/regulated professionals with strong job security\n",
    "    stable = {\n",
    "        \"Physician\", \"Surgeon\", \"Dentist\", \"Chartered_Accountant\",\n",
    "        \"Civil_engineer\", \"Mechanical_engineer\", \"Chemical_engineer\",\n",
    "        \"Petroleum_Engineer\", \"Biomedical_Engineer\", \"Engineer\"\n",
    "    }\n",
    "    \n",
    "    # Corporate roles with steady demand\n",
    "    moderate = {\n",
    "        \"Software_Developer\", \"Computer_hardware_engineer\", \"Financial_Analyst\",\n",
    "        \"Industrial_Engineer\", \"Statistician\", \"Microbiologist\", \"Scientist\",\n",
    "        \"Geologist\", \"Economist\", \"Technology_specialist\", \"Design_Engineer\",\n",
    "        \"Architect\", \"Surveyor\", \"Secretary\", \"Flight_attendant\",\n",
    "        \"Hotel_Manager\", \"Computer_operator\", \"Technician\"\n",
    "    }\n",
    "    \n",
    "    # Project-based or variable demand roles\n",
    "    variable = {\n",
    "        \"Web_designer\", \"Fashion_Designer\", \"Graphic_Designer\", \"Designer\",\n",
    "        \"Consultant\", \"Technical_writer\", \"Artist\", \"Comedian\", \"Chef\",\n",
    "        \"Analyst\", \"Psychologist\", \"Drafter\", \"Aviator\", \"Politician\",\n",
    "        \"Lawyer\"\n",
    "    }\n",
    "    \n",
    "    if profession in very_stable:\n",
    "        return \"very_stable\"\n",
    "    elif profession in stable:\n",
    "        return \"stable\"\n",
    "    elif profession in moderate:\n",
    "        return \"moderate\"\n",
    "    elif profession in variable:\n",
    "        return \"variable\"\n",
    "    else:\n",
    "        return \"moderate\"  # Default category\n",
    "\n",
    "\n",
    "# Apply function to create job stability feature in training, validation, and test data\n",
    "df_train[\"job_stability\"] = df_train[\"profession\"].apply(derive_job_stability)\n",
    "df_val[\"job_stability\"] = df_val[\"profession\"].apply(derive_job_stability)\n",
    "df_test[\"job_stability\"] = df_test[\"profession\"].apply(derive_job_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b0bae-6b21-4c17-9b87-f2c63d1ca0cd",
   "metadata": {},
   "source": [
    "### Location-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d876e3a-aabd-4364-ae8a-3b7d90fc3be0",
   "metadata": {},
   "source": [
    "**City Tier**  \n",
    "Derive city tier from city.   \n",
    "Categorize cities into three tiers that reflect differences in employment opportunities, income levels, cost of living, population densitiy, and economic activity.\n",
    "- Tier 1: Large metropolitan cities with high population density, significant economic activity, and robust infrastructure. India's most developed and urbanized cities.\n",
    "- Tier 2: Medium-sized cities with growing industries, regional importance, and moderate economic activity. Less urbanized than Tier 1.\n",
    "- Tier 3: Smaller cities or towns with limited industrial and economic activity, often rural or semi-urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eca54-93ec-43b5-a815-3221da489f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_city_tier(city):\n",
    "    tier_1 = [\n",
    "        \"New_Delhi\", \"Navi_Mumbai\", \"Kolkata\", \"Bangalore\", \"Chennai\", \"Hyderabad\",\n",
    "        \"Mumbai\", \"Pune\", \"Ahmedabad\", \"Jaipur\", \"Lucknow\", \"Noida\", \"Coimbatore\",\n",
    "        \"Surat\", \"Nagpur\", \"Kochi\", \"Thiruvananthapuram\", \"Kanpur\", \"Patna\"\n",
    "    ]\n",
    "    \n",
    "    tier_2 = [\n",
    "        \"Bhopal\", \"Vijayawada\", \"Indore\", \"Jodhpur\", \"Vadodara\", \"Ludhiana\", \"Madurai\",\n",
    "        \"Agra\", \"Mysore[7][8][9]\", \"Rajkot\", \"Nashik\", \"Amritsar\", \"Ranchi\",\n",
    "        \"Chandigarh_city\", \"Allahabad\", \"Bhubaneswar\", \"Varanasi\", \"Jabalpur\",\n",
    "        \"Guwahati\", \"Tiruppur\", \"Raipur\", \"Udaipur\", \"Gwalior\"\n",
    "    ]\n",
    "\n",
    "    tier_3 = [\n",
    "        \"Vijayanagaram\", \"Bulandshahr\", \"Saharsa[29]\", \"Hajipur[31]\", \"Satara\",\n",
    "        \"Ongole\", \"Bellary\", \"Giridih\", \"Hospet\", \"Khammam\", \"Danapur\", \"Bareilly\",\n",
    "        \"Satna\", \"Howrah\", \"Thanjavur\", \"Farrukhabad\", \"Buxar[37]\", \"Arrah\",\n",
    "        \"Thrissur\", \"Proddatur\", \"Bahraich\", \"Nandyal\", \"Siwan[32]\", \"Barasat\",\n",
    "        \"Dhule\", \"Begusarai\", \"Khandwa\", \"Guntakal\", \"Latur\", \"Karaikudi\"\n",
    "    ]\n",
    "    \n",
    "    if city in tier_1:\n",
    "        return \"tier_1\"\n",
    "    elif city in tier_2:\n",
    "        return \"tier_2\"\n",
    "    elif city in tier_3:\n",
    "        return \"tier_3\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# Apply function to create city tier feature in training, validation, and test data\n",
    "df_train[\"city_tier\"] = df_train[\"city\"].apply(derive_city_tier)\n",
    "df_val[\"city_tier\"] = df_val[\"city\"].apply(derive_city_tier)\n",
    "df_test[\"city_tier\"] = df_test[\"city\"].apply(derive_city_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d2bf-5854-406a-98d7-77bb5cd7f083",
   "metadata": {},
   "source": [
    "**State Default Rate**  \n",
    "Derive state default rate from state using target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab2f19-eade-4898-a091-0b58aa213179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default rate by state based on the training data\n",
    "default_rate_by_state = df_train.groupby(\"state\")[\"risk_flag\"].mean()\n",
    "\n",
    "# Create state default rate feature in training, validation, and test data by replacing the state with its corresponding default rate\n",
    "df_train[\"state_default_rate\"] = df_train[\"state\"].map(default_rate_by_state)\n",
    "df_val[\"state_default_rate\"] = df_val[\"state\"].map(default_rate_by_state)\n",
    "df_test[\"state_default_rate\"] = df_test[\"state\"].map(default_rate_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0420a-3d11-4dfd-ac78-fb7e54a7a5f1",
   "metadata": {},
   "source": [
    "## Defining Semantic Type  \n",
    "Define semantic column types (numerical, categorical, boolean) for downstream tasks like additional preprocessing steps, exploratory data analysis, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d07e51-218e-4826-99cc-73de5abf3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic column types manually\n",
    "numerical_columns = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\"]\n",
    "categorical_columns = [\"house_ownership\", \"job_stability\", \"city_tier\", \"profession\", \"city\", \"state\"]\n",
    "boolean_columns = [\"risk_flag\", \"married\", \"car_ownership\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332edd65-5789-48e4-8aa6-d055d94711b5",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37db6-2329-44af-90ab-ec03a0e6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(df_val.isnull().sum())\n",
    "print(\"\\nTest Data:\")\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e30f60-8cfa-419d-a915-e37a9ddde4ac",
   "metadata": {},
   "source": [
    "No missing values were found in any of the columns in the training, validation, and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386d0a-d26f-462c-96f4-4b557da2943c",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7375e-b714-44f3-b184-c28af316e22a",
   "metadata": {},
   "source": [
    "### 3SD Method  \n",
    "Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b38504-167d-42e9-9bd5-ab44c552929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "            \n",
    "        # Calculate statistics (mean, standard deviation, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"mean\"] = df[self.numerical_columns_].mean()\n",
    "        self.stats_[\"sd\"] = df[self.numerical_columns_].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"sd\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"sd\"]\n",
    "        \n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "     \n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        # Show outliers across all columns\n",
    "        if len(self.numerical_columns_) == 1:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in the '{self.numerical_columns_[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in one or more numerical columns.\")\n",
    " \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        masks = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        final_mask = masks.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"Removed {(~final_mask).sum()} rows ({(~final_mask).sum() / len(final_mask) * 100:.1f}%) with outliers.\")\n",
    "        return df[final_mask]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_3sd.fit(df_train, numerical_columns)\n",
    "\n",
    "# Show mean, sd, cutoff values, and outliers by column for training data\n",
    "print(\"\\nOutliers by column:\")\n",
    "round(outlier_remover_3sd.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dd47c-ef92-425a-89c0-bca0ae30982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "print(\"Training Data:\")\n",
    "df_train_clean = outlier_remover_3sd.transform(df_train)\n",
    "print(\"\\nValidation Data:\")\n",
    "df_val_clean = outlier_remover_3sd.transform(df_val)\n",
    "print(\"\\nTest Data:\")\n",
    "df_test_clean = outlier_remover_3sd.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858552d1-4764-4364-8440-30409e324a57",
   "metadata": {},
   "source": [
    "### 1.5 IQR Method  \n",
    "Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7b0d-c5dd-4820-974e-445571f160da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "        \n",
    "        # Calculate statistics (first quartile, third quartile, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"Q1\"] = df[self.numerical_columns_].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[self.numerical_columns_].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        # Show outliers across all columns\n",
    "        if len(self.numerical_columns_) == 1:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in the '{numerical_columns[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in one or more numerical columns.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        masks = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        final_mask = masks.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"Removed {(~final_mask).sum()} rows ({(~final_mask).sum() / len(final_mask) * 100:.1f}%) with outliers.\")\n",
    "        return df[final_mask]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_iqr.fit(df_train, numerical_columns)\n",
    "\n",
    "# Show outliers by column for training data\n",
    "print(\"\\nOutliers by column:\")\n",
    "round(outlier_remover_iqr.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6555a-4991-4861-a177-e21b6c8d356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default rate by state\n",
    "df_train.groupby(\"state\").max()[\"state_default_rate\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c757cd-3857-4b44-80bd-9284973c7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "print(\"Training Data:\")\n",
    "df_train_clean = outlier_remover_iqr.transform(df_train)\n",
    "print(\"\\nValidation Data:\")\n",
    "df_val_clean = outlier_remover_iqr.transform(df_val)\n",
    "print(\"\\nTest Data:\")\n",
    "df_test_clean = outlier_remover_iqr.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0468a71-229a-4f6b-9520-7a400a3539e7",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "Identify and remove multivariate outliers using the isolation forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8d2e0-bca4-4d41-a331-4de343e9852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize isolation forest\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Create list of numerical and boolean features (without the target variable \"risk_flag\")\n",
    "numerical_boolean_features = numerical_columns + [\"married\", \"car_ownership\"]\n",
    "\n",
    "# Fit isolation forest on training data\n",
    "isolation_forest.fit(df_train[numerical_boolean_features])\n",
    "\n",
    "# Predict outliers on training, validation, and test data\n",
    "df_train[\"outlier\"] = isolation_forest.predict(df_train[numerical_boolean_features])\n",
    "df_train[\"outlier_score\"] = isolation_forest.decision_function(df_train[numerical_boolean_features])\n",
    "df_val[\"outlier\"] = isolation_forest.predict(df_val[numerical_boolean_features])\n",
    "df_val[\"outlier_score\"] = isolation_forest.decision_function(df_val[numerical_boolean_features])\n",
    "df_test[\"outlier\"] = isolation_forest.predict(df_test[numerical_boolean_features])\n",
    "df_test[\"outlier_score\"] = isolation_forest.decision_function(df_test[numerical_boolean_features])\n",
    "\n",
    "# Show number of outliers\n",
    "n_outliers_train = df_train[\"outlier\"].value_counts()[-1]\n",
    "contamination_train = df_train[\"outlier\"].value_counts()[-1] / df_train[\"outlier\"].value_counts().sum()\n",
    "print(f\"Training Data: Identified {n_outliers_train} rows ({100 * contamination_train:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_val = df_val[\"outlier\"].value_counts()[-1]\n",
    "contamination_val = df_val[\"outlier\"].value_counts()[-1] / df_val[\"outlier\"].value_counts().sum()\n",
    "print(f\"Validation Data: Identified {n_outliers_val} rows ({100 * contamination_val:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_test = df_test[\"outlier\"].value_counts()[-1]\n",
    "contamination_test = df_test[\"outlier\"].value_counts()[-1] / df_test[\"outlier\"].value_counts().sum()\n",
    "print(f\"Test Data: Identified {n_outliers_test} rows ({100 * contamination_test:.1f}%) as multivariate outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7c69a-dd5a-4d3f-9686-92c0f1ded562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to visualize outliers for a subsample of the training data\n",
    "df_train_subsample = df_train[numerical_boolean_features + [\"outlier\"]].sample(n=5000, random_state=42)\n",
    "sns.pairplot(df_train_subsample, hue=\"outlier\", palette={1: \"#4F81BD\", -1: \"#D32F2F\"}, plot_kws={\"alpha\":0.6, \"s\":40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2890-0ad1-4ef6-8438-373b5d958c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df_train_clean = df_train[df_train[\"outlier\"] == 1]\n",
    "df_val_clean = df_val[df_val[\"outlier\"] == 1]\n",
    "df_test_clean = df_test[df_test[\"outlier\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd389f-1faf-4170-b152-ce6dc77d69bc",
   "metadata": {},
   "source": [
    "## Feature Scaling and Encoding  \n",
    "Use a `ColumnTransformer` to preprocess columns based on their semantic type. This allows the appropriate transformation to each semantic column type in a single step.  \n",
    "- Scale numerical columns: `StandardScaler` to transform numerical columns to have mean = 0 and standard deviation = 1.\n",
    "- Encode categorical columns:\n",
    "    - Nominal columns (unordered categories): `OneHotEncoder` to convert string categories into binary (one-hot) encoded columns.\n",
    "    - Ordinal columns (ordered categories): `OrdinalEncoder` to convert string categories into integers.\n",
    "- Retain boolean columns: Pass through boolean columns unchanged using `remainder=\"passthrough\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74802f13-c326-403e-ba65-0786a78246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nominal and ordinal columns\n",
    "nominal_columns = [\"house_ownership\"]\n",
    "ordinal_columns = [\"job_stability\", \"city_tier\"]\n",
    "\n",
    "# Define the explicit order of categories for all ordinal columns\n",
    "ordinal_column_orders = [\n",
    "    [\"variable\", \"moderate\", \"stable\", \"very_stable\"],  # Order for job_stability\n",
    "    [\"unknown\", \"tier_3\", \"tier_2\", \"tier_1\"]  # Order for city_tier\n",
    "]\n",
    "\n",
    "# Initialize a column transformer \n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), numerical_columns), \n",
    "        (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit column transformer on the training data \n",
    "column_transformer.fit(df_train)\n",
    "\n",
    "# Apply feature scaling and encoding to training, validation and test data\n",
    "df_train_transformed = column_transformer.transform(df_train)\n",
    "df_val_transformed = column_transformer.transform(df_val)\n",
    "df_test_transformed = column_transformer.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c9c2-14c5-4b4a-bad8-f9a2b71f839d",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "Save preprocessed data from a Pandas DataFrame to a `.csv` file in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afee6f-836b-4efc-a74a-93e4b8677660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save as .csv  \n",
    "df_train.to_csv(\"data/training_data_preprocessed.csv\", index=False)\n",
    "df_test.to_csv(\"data/test_data_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dcb62-3ed6-4437-a100-ae5c317edd5e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae0b1c-675f-41b3-89ab-6d4f3cb308f5",
   "metadata": {},
   "source": [
    "## Univariate EDA  \n",
    "Analyze the distribution of a single column using descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a00fe4-0b5d-4266-94cf-8145c92f3c83",
   "metadata": {},
   "source": [
    "### Numerical Columns  \n",
    "Examine descriptive statistics (e.g., mean, median, standard deviation, quartiles) and visualize the distributions (e.g., histogram) of numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfd81f-2d6a-4200-9e00-fa158797e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "df_train[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f3222-63eb-434b-a666-ac1bd989518b",
   "metadata": {},
   "source": [
    "**Income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d4e48-891a-4a97-871b-ba7259370d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of income \n",
    "sns.histplot(df_train[\"income\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Distribution of income\")\n",
    "plt.xlabel(\"income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3682a94-0fa2-44e8-a854-f84134303ff5",
   "metadata": {},
   "source": [
    "**State Default Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd17dd-fc0b-46cb-a7fe-14f62644cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of income \n",
    "sns.histplot(df_train[\"state_default_rate\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Distribution of State Default Rate\")\n",
    "plt.xlabel(\"State Default Rate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9db47f-a247-411d-a964-387fb6af476b",
   "metadata": {},
   "source": [
    "### Categorical Columns  \n",
    "Examine descriptive statistics (e.g., absolute and relative frequencies) and visualize the distributions (e.g., bar plot) of categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a43ad5-2e8b-4d4e-955b-1465e1928b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns + boolean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3a51e-902d-45b1-bd9f-56a5782f9304",
   "metadata": {},
   "source": [
    "**Risk Flag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380553d9-93dd-4447-8e7f-1e6e94410c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"risk_flag\"].value_counts()\n",
    "relative_frequencies = df_train[\"risk_flag\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955cab5-c9b0-4ce0-ac60-7a37dd7a9a8f",
   "metadata": {},
   "source": [
    "**Married**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22539a24-ec78-4970-ad97-28f53da7ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"married\"].value_counts()\n",
    "relative_frequencies = df_train[\"married\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c28a63-1fbe-4f1a-8e57-94a0300455c2",
   "metadata": {},
   "source": [
    "**House Ownership**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44a427-91a6-46c9-8bac-1a1872b55290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"house_ownership\"].value_counts()\n",
    "relative_frequencies = df_train[\"house_ownership\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bf8c8-7de3-44d8-b888-7a609bdf052b",
   "metadata": {},
   "source": [
    "**Car Ownership**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1b8c-f9dd-4db7-babf-2bd3143c8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"car_ownership\"].value_counts()\n",
    "relative_frequencies = df_train[\"car_ownership\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddead-be65-4114-a39f-419cee4892cf",
   "metadata": {},
   "source": [
    "**Profession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbdaee-3785-4677-b803-4e7142e8b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"profession\"].value_counts()\n",
    "relative_frequencies = df_train[\"profession\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6629ea-28f4-4486-87cb-22e6f8859064",
   "metadata": {},
   "source": [
    "**Job Stability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b707d38-45b6-48f0-ad21-6a21e2166b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"job_stability\"].value_counts()\n",
    "relative_frequencies = df_train[\"job_stability\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b658f-25a5-482b-bbf8-1d2eec7d0c54",
   "metadata": {},
   "source": [
    "**City**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6216576-ad25-4fb3-a8c0-4fd06e1877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"city\"].value_counts()\n",
    "relative_frequencies = df_train[\"city\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71c8d1-b26b-450a-9ef6-b7d03a5c39e0",
   "metadata": {},
   "source": [
    "**City Tier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5ac5d-bb23-4df3-ba49-221471b12e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"city_tier\"].value_counts()\n",
    "relative_frequencies = df_train[\"city_tier\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d924e-09c1-4ab9-8a26-25c6c8238dcd",
   "metadata": {},
   "source": [
    "**State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63990c-2ee5-4bbc-9c73-f82353ce403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"state\"].value_counts()\n",
    "relative_frequencies = df_train[\"state\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bd2bc-70c3-4520-8101-49f943cb29d6",
   "metadata": {},
   "source": [
    "**Visualize Distributions**  \n",
    "Bar plot matrix to show the frequency distribution for all 9 categorical and boolean columns in a 3x3 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2daa752-5f8a-4802-80af-1f302a20c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Iterate over the categorical and boolean columns\n",
    "for i, column in enumerate(boolean_columns + categorical_columns):\n",
    "    # Create a subplot in a 3x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Calculate frequencies for the current column\n",
    "    frequencies = df_train[column].value_counts(normalize=True)  # For absolute frequencies: normalize=False\n",
    "    \n",
    "    # Create bar plot for the current column\n",
    "    sns.barplot(x=frequencies.index, y=frequencies.values, estimator=np.median, errorbar=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(column.title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    # Rotate x-axis tick labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815f4f-0f4f-456f-8eb3-6eaebf035b19",
   "metadata": {},
   "source": [
    "# Future Improvements\n",
    "\n",
    "**Data Enrichment**:  \n",
    "To enhance the model's performance and business value, data enrichment with the following financial features is recommended:\n",
    "- Loan amount \n",
    "- Loan duration\n",
    "- Interest rate\n",
    "- Type of loan (e.g., personal, home, vehicle)\n",
    "- Existing debt\n",
    "- Credit score\n",
    "\n",
    "**Enhanced Analysis Capabilities**:  \n",
    "The addition of these features, particularly loan amount, would enable more precise risk assessment and better alignment with business objectives through cost-sensitive evaluation metrics incorporating actual monetary values. Cost-sensitive metrics or expected monetary value incorporate the actual cost of defaults (false negatives) and the opportunity cost of rejecting good loans (false positives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-default-venv",
   "language": "python",
   "name": "loan-default-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
