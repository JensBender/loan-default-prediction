{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4275851-2816-4903-91c3-8b4a4c489b12",
   "metadata": {},
   "source": [
    "# Project Overview: Predicting Loan Defaults\n",
    "\n",
    "**Summary**  \n",
    "This project aims to develop a machine learning model to predict whether the customers of a financial institution will default on a loan based on data from their loan application. By accurately identifying potential defaulters, financial institutions can make more informed lending decisions, reduce losses, improve profitability, and increase operational efficiency through the automation of risk assessment.\n",
    "\n",
    "**Problem**  \n",
    "Predicting loan defaults is a challenging task due to the multitude of influencing factors such as customers' demographic, financial, location, and behavioral attributes. Traditional default prediction models often oversimplify complex relationships between customer features and default risk. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in loan application data, enabling more accurate predictions of loan default risk.\n",
    "\n",
    "**Objectives**  \n",
    "- Develop a machine learning model to predict loan defaults using customer data from loan applications.\n",
    "- Compare multiple models (e.g., Logistic Regression, Random Forest, XGBoost) using a suitable evaluation metric (such as AUC-PR).\n",
    "- Identify key factors influencing loan default risk through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project enables financial institutions to reduce loan default rates and make better and faster lending decisions by leveraging machine learning for automated and improved risk assessment. \n",
    "\n",
    "**Business Goals**  \n",
    "- Reduce losses by 5M-10M INR within 12 months of model deployment by decreasing the loan default rate by 10%-20%.\n",
    "- Decrease loan processing time by 25%-40% by automating risk assessment, leading to less time spent on manual evaluations.\n",
    "- Ensure 100% compliance with regulatory requirements and fair lending practices.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains information provided by customers of a financial institution during the loan application process. It is sourced from the \"Loan Prediction Based on Customer Behavior\" dataset by Subham Jain, available on [Kaggle](https://www.kaggle.com/datasets/subhamjain/loan-prediction-based-on-customer-behavior). The dataset consists of three `.csv` files:\n",
    "1. `Training Data.csv`: Contains the features, target variable (`Risk Flag`), and `ID` column from the training data. \n",
    "2. `Test Data.csv`: Contains the features and `ID` column from the test data.\n",
    "3. `Sample Prediction Dataset.csv`: Contains the target variable (`Risk Flag`) and `ID` column from the test data. \n",
    "\n",
    "Dataset Statistics:\n",
    "- Training set size: 252,000 records \n",
    "- Test set size: 28,000 records \n",
    "- Target variable: Risk flag (training: 12.3% defaults, test: 12.8% defaults)\n",
    "- Features: 11 \n",
    "  - Demographic: Age, married, profession\n",
    "  - Financial: Income, house ownership, car ownership\n",
    "  - Location: City, state\n",
    "  - Behavioral: Experience, current job years, current house years\n",
    "\n",
    "Data Overview Table:\n",
    "\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Training Data Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Risk Flag | Defaulted on loan (0: No, 1: Yes) | Integer | Categorical (Binary) | [0, 1] | [0, 1] |\n",
    "| Income | Income of the applicant | Integer | Numerical | [0, ∞] | [10K, 10M] |\n",
    "| Age | Age of the applicant (in years) | Integer | Numerical | [18, ∞] | [21, 79] |\n",
    "| Experience | Work experience (in years) | Integer | Numerical | [0, ∞] | [0, 20] |\n",
    "| Profession | Applicant's profession | String | Categorical (Nominal) | Any profession [e.g., \"Architect\", \"Dentist\"] | 51 unique professions |\n",
    "| Married | Marital status | String | Categorical (Binary) | [\"single\", \"married\"] | [\"single\", \"married\"] |\n",
    "| House Ownership | Applicant owns or rents a house | String | Categorical (Nominal) | [\"rented\", \"owned\", \"norent_noown\"] | [\"rented\", \"owned\", \"norent_noown\"] |\n",
    "| Car Ownership | Whether applicant owns a car | String | Categorical (Binary) | [\"yes\", \"no\"] | [\"yes\", \"no\"] |\n",
    "| Current Job Years | Years in the current job | Integer | Numerical | [0, ∞] | [0, 14] |\n",
    "| Current House Years | Years in the current house | Integer | Numerical | [0, ∞] | [10, 14] |\n",
    "| City | City of residence | String | Categorical (Nominal) | Any city [e.g., \"Mumbai\", \"Bangalore\"] | 317 unique cities |\n",
    "| State | State of residence | String | Categorical (Nominal) | Any state [e.g., \"Maharashtra\", \"Tamil_Nadu\"] | 29 unique states |\n",
    "\n",
    "Example Training Data:\n",
    "\n",
    "| Risk Flag | Income    | Age | Experience | Profession         | Married | House Ownership | Car Ownership | Current Job Years | Current House Years | City      | State         |\n",
    "| :-------- | :-------- | :-- | :--------- | :----------------- | :------ | :-------------- | :------------ | :---------------- | :------------------ | :-------- | :------------ |\n",
    "| 0         | 1,303,834 | 23  | 3          | Mechanical_engineer | single  | rented          | no            | 3                 | 13                   | Rewa      | Madhya_Pradesh |\n",
    "| 1         | 6,256,451 | 41  | 2          | Software_Developer | single  | rented          | yes           | 2                 | 12                   | Bangalore | Tamil_Nadu    |\n",
    "| 0         | 3,991,815 | 66  | 4          | Technical_writer   | married | rented          | no            | 4                 | 10                   | Alappuzha | Kerala        |\n",
    "\n",
    "**Technical Requirements**  \n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, data types, missing values, and outliers.\n",
    "  - Extract features, scale numerical features, and encode categorical features.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models and perform hyperparameter tuning for binary classification task with `sklearn` and `xgboost`.\n",
    "  - Baseline models: Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest, Multi-Layer Perceptron, XGBoost.\n",
    "  - Evaluate model performance using Area Under the Precision-Recall Curve (AUC-PR).\n",
    "    - AUC-PR is more suitable to address class imbalance (12.3% defaults) with a focus on the positive class (preventing defaults) than accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "    - Success criterion: Minimum AUC-PR of 0.70 on the test data.\n",
    "  - Potentially use additional techniques to address class imbalance (e.g., SMOTE, class weights).\n",
    "  - Visualize feature importance, show model prediction examples, and save the final model with `pickle`.\n",
    "- Deployment:\n",
    "  - Expose the final model via a REST API for easy integration with existing loan processing systems.\n",
    "  - Implement efficient batch processing capabilities to handle up to 10K predictions in under 30 seconds.\n",
    "  - Deploy using cloud infrastructure to ensure scalability and security.\n",
    "  - Set up model performance monitoring and data drift detection.\n",
    "- Stakeholders:\n",
    "  - Loan officers: Direct users of the model predictions in day-to-day loan approvals.\n",
    "  - Credit risk analysts: Provide subject matter expertise on loan default risk.\n",
    "  - Compliance officers: Ensure the model complies with any legal and regulatory guidelines.\n",
    "  - IT department: Manage the IT infrastructure and ensure data access for the model's development and deployment. \n",
    "\n",
    "By fulfilling these objectives and requirements, the project will provide a valuable tool for predicting loan defaults, thereby enhancing decision-making for financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9b38-149a-4c08-bb6a-c1ad97d33d28",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd00e3c4-73f9-45bb-a474-58fc6049796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7316e-f065-4b92-8f2c-5cd5681b5a69",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load data from the three `.csv` files into three Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f1e0114-5f1a-44ce-85e7-3136fe031aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"data/training_data.csv\")\n",
    "    X_test = pd.read_csv(\"data/test_data.csv\")\n",
    "    y_test = pd.read_csv(\"data/sample_prediction_dataset.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866b8d1-12e6-4f47-b54e-8132b55b4c1e",
   "metadata": {},
   "source": [
    "# Initial Data Inspection  \n",
    "Basic exploration of the dataset to understand its structure and detect obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc38bee-eefa-48ce-9ddb-b9464e6caa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252000 entries, 0 to 251999\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Id                 252000 non-null  int64 \n",
      " 1   Income             252000 non-null  int64 \n",
      " 2   Age                252000 non-null  int64 \n",
      " 3   Experience         252000 non-null  int64 \n",
      " 4   Married/Single     252000 non-null  object\n",
      " 5   House_Ownership    252000 non-null  object\n",
      " 6   Car_Ownership      252000 non-null  object\n",
      " 7   Profession         252000 non-null  object\n",
      " 8   CITY               252000 non-null  object\n",
      " 9   STATE              252000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    252000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  252000 non-null  int64 \n",
      " 12  Risk_Flag          252000 non-null  int64 \n",
      "dtypes: int64(7), object(6)\n",
      "memory usage: 25.0+ MB\n",
      "None\n",
      "\n",
      "Test Data - Features:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 28000 non-null  int64 \n",
      " 1   Income             28000 non-null  int64 \n",
      " 2   Age                28000 non-null  int64 \n",
      " 3   Experience         28000 non-null  int64 \n",
      " 4   Married/Single     28000 non-null  object\n",
      " 5   House_Ownership    28000 non-null  object\n",
      " 6   Car_Ownership      28000 non-null  object\n",
      " 7   Profession         28000 non-null  object\n",
      " 8   CITY               28000 non-null  object\n",
      " 9   STATE              28000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    28000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  28000 non-null  int64 \n",
      "dtypes: int64(6), object(6)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "Test Data - Target Variable:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   id         28000 non-null  int64\n",
      " 1   risk_flag  28000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 437.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.info())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ea3692-d2c8-46f1-b0ad-36372c1fa4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Married/Single</th>\n",
       "      <th>House_Ownership</th>\n",
       "      <th>Car_Ownership</th>\n",
       "      <th>Profession</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>CURRENT_JOB_YRS</th>\n",
       "      <th>CURRENT_HOUSE_YRS</th>\n",
       "      <th>Risk_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1303834</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Mechanical_engineer</td>\n",
       "      <td>Rewa</td>\n",
       "      <td>Madhya_Pradesh</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7574516</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Software_Developer</td>\n",
       "      <td>Parbhani</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3991815</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>married</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Technical_writer</td>\n",
       "      <td>Alappuzha</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6256451</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Software_Developer</td>\n",
       "      <td>Bhubaneswar</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5768871</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Civil_servant</td>\n",
       "      <td>Tiruchirappalli[10]</td>\n",
       "      <td>Tamil_Nadu</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Income  Age  Experience Married/Single House_Ownership Car_Ownership  \\\n",
       "0   1  1303834   23           3         single          rented            no   \n",
       "1   2  7574516   40          10         single          rented            no   \n",
       "2   3  3991815   66           4        married          rented            no   \n",
       "3   4  6256451   41           2         single          rented           yes   \n",
       "4   5  5768871   47          11         single          rented            no   \n",
       "\n",
       "            Profession                 CITY           STATE  CURRENT_JOB_YRS  \\\n",
       "0  Mechanical_engineer                 Rewa  Madhya_Pradesh                3   \n",
       "1   Software_Developer             Parbhani     Maharashtra                9   \n",
       "2     Technical_writer            Alappuzha          Kerala                4   \n",
       "3   Software_Developer          Bhubaneswar          Odisha                2   \n",
       "4        Civil_servant  Tiruchirappalli[10]      Tamil_Nadu                3   \n",
       "\n",
       "   CURRENT_HOUSE_YRS  Risk_Flag  \n",
       "0                 13          0  \n",
       "1                 13          0  \n",
       "2                 10          0  \n",
       "3                 12          1  \n",
       "4                 14          1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the training data\n",
    "print(\"Training Data:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9890b354-634e-46b6-aaf0-0868ad775bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data - Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Married/Single</th>\n",
       "      <th>House_Ownership</th>\n",
       "      <th>Car_Ownership</th>\n",
       "      <th>Profession</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>CURRENT_JOB_YRS</th>\n",
       "      <th>CURRENT_HOUSE_YRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7393090</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Geologist</td>\n",
       "      <td>Malda</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1215004</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Firefighter</td>\n",
       "      <td>Jalna</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8901342</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>no</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>Thane</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1944421</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>married</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Latur</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>13429</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>single</td>\n",
       "      <td>rented</td>\n",
       "      <td>yes</td>\n",
       "      <td>Comedian</td>\n",
       "      <td>Berhampore</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Income  Age  Experience Married/Single House_Ownership Car_Ownership  \\\n",
       "0   1  7393090   59          19         single          rented            no   \n",
       "1   2  1215004   25           5         single          rented            no   \n",
       "2   3  8901342   50          12         single          rented            no   \n",
       "3   4  1944421   49           9        married          rented           yes   \n",
       "4   5    13429   25          18         single          rented           yes   \n",
       "\n",
       "    Profession        CITY        STATE  CURRENT_JOB_YRS  CURRENT_HOUSE_YRS  \n",
       "0    Geologist       Malda  West Bengal                4                 13  \n",
       "1  Firefighter       Jalna  Maharashtra                5                 10  \n",
       "2       Lawyer       Thane  Maharashtra                9                 14  \n",
       "3      Analyst       Latur  Maharashtra                3                 12  \n",
       "4     Comedian  Berhampore  West Bengal               13                 11  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the test data features\n",
    "print(\"Test Data - Features:\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad411a4-8229-41b9-843b-ba280a6b6599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data - Target Variable:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>risk_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  risk_flag\n",
       "0   1          0\n",
       "1   2          0\n",
       "2   3          1\n",
       "3   4          0\n",
       "4   5          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows of the test data target variable\n",
    "print(\"Test Data - Target Variable:\")\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9596-7328-43a4-8cf6-a18d1b64b4f7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af4f32-03ee-43b6-9289-6821b800328b",
   "metadata": {},
   "source": [
    "## Handling Column Names\n",
    "Convert all column names to snake_case for consistency, improved readability, and to minimize the risk of errors. This also resolves inconsistencies in column names between the training and test datasets (e.g., \"Id\" vs. \"ID\", \"Risk_Flag\" vs. \"risk_flag\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c0e7b5f-3c5a-40c8-9f56-9d6040da7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake_case\n",
    "df_train.columns = (\n",
    "    df_train.columns\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  # Replace spaces and special characters with \"_\"\n",
    "    .str.replace(\"_single\", \"\")  # Shorten \"married_single\" to \"married\"\n",
    ")\n",
    "\n",
    "X_test.columns = (\n",
    "    X_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True) \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")\n",
    "\n",
    "y_test.columns = (\n",
    "    y_test.columns\n",
    "    .str.strip()  \n",
    "    .str.lower()  \n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  \n",
    "    .str.replace(\"_single\", \"\") \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565d7a-e2b0-472d-91dd-f860878e384b",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28532f-00c9-4543-a9c1-29de96341820",
   "metadata": {},
   "source": [
    "Identify and remove duplicates based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d98e50-9696-47fe-b1ad-51ec59375e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "False    252000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data - Features:\n",
      "False    28000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data - Target Variable:\n",
      "False    28000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicates based on all columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated().value_counts())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.duplicated().value_counts())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b48ba-dc42-4591-bc44-e490092a79f2",
   "metadata": {},
   "source": [
    "No duplicates were found based on all columns in both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029181-c3e9-4588-8055-7770ab866eff",
   "metadata": {},
   "source": [
    "Identify and remove duplicates based on the ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8469e53c-dcfa-42eb-8542-7f215b4d3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "False    252000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data - Features:\n",
      "False    28000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data - Target Variable:\n",
      "False    28000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.duplicated(subset=[\"id\"]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57954bf9-09ec-49df-8cb7-ed39d546d14f",
   "metadata": {},
   "source": [
    "No duplicates were found based on the ID column in both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89142d-7b4a-4b23-98b3-3c9e63ee5eb8",
   "metadata": {},
   "source": [
    "## Handling Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391177e-efa2-4e4c-ba49-ecb7e59fa56b",
   "metadata": {},
   "source": [
    "Identify and convert incorrect storage data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be913b0-2788-4e8d-8797-37233709dfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "id                    int64\n",
      "income                int64\n",
      "age                   int64\n",
      "experience            int64\n",
      "married              object\n",
      "house_ownership      object\n",
      "car_ownership        object\n",
      "profession           object\n",
      "city                 object\n",
      "state                object\n",
      "current_job_yrs       int64\n",
      "current_house_yrs     int64\n",
      "risk_flag             int64\n",
      "dtype: object\n",
      "\n",
      "Test Data - Features:\n",
      "id                    int64\n",
      "income                int64\n",
      "age                   int64\n",
      "experience            int64\n",
      "married              object\n",
      "house_ownership      object\n",
      "car_ownership        object\n",
      "profession           object\n",
      "city                 object\n",
      "state                object\n",
      "current_job_yrs       int64\n",
      "current_house_yrs     int64\n",
      "dtype: object\n",
      "\n",
      "Test Data - Target Variable:\n",
      "id           int64\n",
      "risk_flag    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Identify storage data types\n",
    "print(\"Training Data:\")\n",
    "print(df_train.dtypes)\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.dtypes)\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da11a2-50d5-4c7a-907e-3b71830f0295",
   "metadata": {},
   "source": [
    "No incorrect storage data types were found at first glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb80b7-c29b-4dd9-83ce-1b6a94abdb67",
   "metadata": {},
   "source": [
    "Identify object columns with two unique categories and convert them to boolean columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39d0d39d-104b-47b4-8538-ffd2f55a0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "married              2\n",
      "house_ownership      3\n",
      "car_ownership        2\n",
      "profession          51\n",
      "city               317\n",
      "state               29\n",
      "dtype: int64\n",
      "\n",
      "Test Data - Features:\n",
      "married              2\n",
      "house_ownership      3\n",
      "car_ownership        2\n",
      "profession          51\n",
      "city               317\n",
      "state               29\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify object columns with two unique categories \n",
    "print(\"Training Data:\")\n",
    "print(df_train[df_train.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test[X_test.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9876603a-e2d3-493e-a1a7-cdf2233a8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert married and car_ownership column from object to boolean\n",
    "df_train[\"married\"] = df_train[\"married\"].map({\"married\": True, \"single\": False})\n",
    "X_test[\"married\"] = X_test[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df_train[\"car_ownership\"] = df_train[\"car_ownership\"].map({\"yes\": True, \"no\": False})\n",
    "X_test[\"car_ownership\"] = X_test[\"car_ownership\"].map({\"yes\": True, \"no\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01e2e3-70e4-4659-bb73-d6ec7dcdc358",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split  \n",
    "Data is already split into training (90%) and test set (10%). Split the training set further to achieve the following train-validation-test split: \n",
    "\n",
    "| Data           | Size (%) | Size (Total) | \n",
    "|:---------------|----------|---------|\n",
    "| Training Set   | 80%      | 224,000 |\n",
    "| Validation Set | 10%      | 28,000  | \n",
    "| Test Set       | 10%      | 28,000  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ee908d5-43d1-4098-8c78-8a4c61aacebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set further into training and validation sets (validation set size same as test set size)\n",
    "df_train, df_val = train_test_split(df_train, test_size=X_test.shape[0]/df_train.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413ec8b-66cd-4d93-a071-7b076db6fbf4",
   "metadata": {},
   "source": [
    "## Engineering New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef07a4d2-f28d-4ad1-a618-fde9e0753d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "house_ownership      3\n",
      "profession          51\n",
      "city               317\n",
      "state               29\n",
      "dtype: int64\n",
      "\n",
      "Test Data - Features:\n",
      "house_ownership      3\n",
      "profession          51\n",
      "city               317\n",
      "state               29\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore number of unique categories in categorical columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965f441-5b9b-4c4f-8d6d-1a3f40998771",
   "metadata": {},
   "source": [
    "### Profession-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856622d-2390-4f7a-8f9e-b020b5eabb0f",
   "metadata": {},
   "source": [
    "**Job Stability**  \n",
    "Derive job stability from profession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a63250d-c76a-42b9-b8c6-a4f0d94b9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_job_stability(profession):\n",
    "    # Government and highly regulated roles with exceptional job security\n",
    "    very_stable = {\n",
    "        \"Civil_servant\", \"Army_officer\", \"Police_officer\", \"Magistrate\",\n",
    "        \"Official\", \"Air_traffic_controller\", \"Firefighter\", \"Librarian\"\n",
    "    }\n",
    "    \n",
    "    # Licensed/regulated professionals with strong job security\n",
    "    stable = {\n",
    "        \"Physician\", \"Surgeon\", \"Dentist\", \"Chartered_Accountant\",\n",
    "        \"Civil_engineer\", \"Mechanical_engineer\", \"Chemical_engineer\",\n",
    "        \"Petroleum_Engineer\", \"Biomedical_Engineer\", \"Engineer\"\n",
    "    }\n",
    "    \n",
    "    # Corporate roles with steady demand\n",
    "    moderate = {\n",
    "        \"Software_Developer\", \"Computer_hardware_engineer\", \"Financial_Analyst\",\n",
    "        \"Industrial_Engineer\", \"Statistician\", \"Microbiologist\", \"Scientist\",\n",
    "        \"Geologist\", \"Economist\", \"Technology_specialist\", \"Design_Engineer\",\n",
    "        \"Architect\", \"Surveyor\", \"Secretary\", \"Flight_attendant\",\n",
    "        \"Hotel_Manager\", \"Computer_operator\", \"Technician\"\n",
    "    }\n",
    "    \n",
    "    # Project-based or variable demand roles\n",
    "    variable = {\n",
    "        \"Web_designer\", \"Fashion_Designer\", \"Graphic_Designer\", \"Designer\",\n",
    "        \"Consultant\", \"Technical_writer\", \"Artist\", \"Comedian\", \"Chef\",\n",
    "        \"Analyst\", \"Psychologist\", \"Drafter\", \"Aviator\", \"Politician\",\n",
    "        \"Lawyer\"\n",
    "    }\n",
    "    \n",
    "    if profession in very_stable:\n",
    "        return \"very_stable\"\n",
    "    elif profession in stable:\n",
    "        return \"stable\"\n",
    "    elif profession in moderate:\n",
    "        return \"moderate\"\n",
    "    elif profession in variable:\n",
    "        return \"variable\"\n",
    "    else:\n",
    "        return \"moderate\"  # Default category\n",
    "\n",
    "# Usage:     \n",
    "df_train[\"job_stability\"] = df_train[\"profession\"].apply(derive_job_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b0bae-6b21-4c17-9b87-f2c63d1ca0cd",
   "metadata": {},
   "source": [
    "### Location-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d876e3a-aabd-4364-ae8a-3b7d90fc3be0",
   "metadata": {},
   "source": [
    "**City Tier**  \n",
    "Derive city tier from city.   \n",
    "Categorize cities into three tiers that reflect differences in employment opportunities, income levels, cost of living, population densitiy, and economic activity.\n",
    "- Tier 1: Large metropolitan cities with high population density, significant economic activity, and robust infrastructure. India's most developed and urbanized cities.\n",
    "- Tier 2: Medium-sized cities with growing industries, regional importance, and moderate economic activity. Less urbanized than Tier 1.\n",
    "- Tier 3: Smaller cities or towns with limited industrial and economic activity, often rural or semi-urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e50eca54-93ec-43b5-a815-3221da489f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_city_tier(city):\n",
    "    tier_1 = [\n",
    "        \"New_Delhi\", \"Navi_Mumbai\", \"Kolkata\", \"Bangalore\", \"Chennai\", \"Hyderabad\",\n",
    "        \"Mumbai\", \"Pune\", \"Ahmedabad\", \"Jaipur\", \"Lucknow\", \"Noida\", \"Coimbatore\",\n",
    "        \"Surat\", \"Nagpur\", \"Kochi\", \"Thiruvananthapuram\", \"Kanpur\", \"Patna\"\n",
    "    ]\n",
    "    \n",
    "    tier_2 = [\n",
    "        \"Bhopal\", \"Vijayawada\", \"Indore\", \"Jodhpur\", \"Vadodara\", \"Ludhiana\", \"Madurai\",\n",
    "        \"Agra\", \"Mysore[7][8][9]\", \"Rajkot\", \"Nashik\", \"Amritsar\", \"Ranchi\",\n",
    "        \"Chandigarh_city\", \"Allahabad\", \"Bhubaneswar\", \"Varanasi\", \"Jabalpur\",\n",
    "        \"Guwahati\", \"Tiruppur\", \"Raipur\", \"Udaipur\", \"Gwalior\"\n",
    "    ]\n",
    "\n",
    "    tier_3 = [\n",
    "        \"Vijayanagaram\", \"Bulandshahr\", \"Saharsa[29]\", \"Hajipur[31]\", \"Satara\",\n",
    "        \"Ongole\", \"Bellary\", \"Giridih\", \"Hospet\", \"Khammam\", \"Danapur\", \"Bareilly\",\n",
    "        \"Satna\", \"Howrah\", \"Thanjavur\", \"Farrukhabad\", \"Buxar[37]\", \"Arrah\",\n",
    "        \"Thrissur\", \"Proddatur\", \"Bahraich\", \"Nandyal\", \"Siwan[32]\", \"Barasat\",\n",
    "        \"Dhule\", \"Begusarai\", \"Khandwa\", \"Guntakal\", \"Latur\", \"Karaikudi\"\n",
    "    ]\n",
    "    \n",
    "    if city in tier_1:\n",
    "        return \"tier_1\"\n",
    "    elif city in tier_2:\n",
    "        return \"tier_2\"\n",
    "    elif city in tier_3:\n",
    "        return \"tier_3\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Apply function to create city tier feature\n",
    "df_train[\"city_tier\"] = df_train[\"city\"].apply(derive_city_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d2bf-5854-406a-98d7-77bb5cd7f083",
   "metadata": {},
   "source": [
    "**State Default Rate**  \n",
    "Derive state default rate from state using target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52ab2f19-eade-4898-a091-0b58aa213179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default rate by state\n",
    "default_rate_by_state = df_train.groupby(\"state\")[\"risk_flag\"].mean()\n",
    "\n",
    "# Create the state default rate feature by replacing the state with its corresponding default rate\n",
    "df_train[\"state_default_rate\"] = df_train[\"state\"].map(default_rate_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0420a-3d11-4dfd-ac78-fb7e54a7a5f1",
   "metadata": {},
   "source": [
    "## Defining Semantic Type  \n",
    "Define semantic column types (numerical, categorical, boolean) for downstream tasks like additional preprocessing steps, exploratory data analysis, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d07e51-218e-4826-99cc-73de5abf3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic column types manually\n",
    "numerical_columns = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\"]\n",
    "categorical_columns = [\"house_ownership\", \"profession\", \"city\", \"state\", \"job_stability\", \"city_tier\"]\n",
    "boolean_columns = [\"married\", \"car_ownership\", \"risk_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332edd65-5789-48e4-8aa6-d055d94711b5",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37db6-2329-44af-90ab-ec03a0e6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e30f60-8cfa-419d-a915-e37a9ddde4ac",
   "metadata": {},
   "source": [
    "No missing values were found in any of the columns in both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386d0a-d26f-462c-96f4-4b557da2943c",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7375e-b714-44f3-b184-c28af316e22a",
   "metadata": {},
   "source": [
    "### 3SD Method  \n",
    "Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b38504-167d-42e9-9bd5-ab44c552929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            numerical_columns = [numerical_columns]\n",
    "            \n",
    "        # Calculate statistics (mean, standard deviation, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=numerical_columns)\n",
    "        self.stats_[\"mean\"] = df[numerical_columns].mean()\n",
    "        self.stats_[\"sd\"] = df[numerical_columns].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"sd\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"sd\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[numerical_columns] >= self.stats_[\"lower_cutoff\"]) & (df[numerical_columns] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        # Show statistics and number of outliers\n",
    "        print(f\"Statistics and outliers: \\n{self.stats_}\")\n",
    "        if len(numerical_columns) == 1:\n",
    "            print(f\"\\n{self.outliers_} rows contain outliers in the '{numerical_columns[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\n{self.outliers_} rows contain outliers in one or more numerical columns.\")\n",
    "  \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"{self.outliers_} rows with outliers removed.\")\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7efcbf-bec0-49f6-82bc-ec0d98ca51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Identify and remove outliers\n",
    "df_train = outlier_remover_3sd.fit_transform(df_train, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858552d1-4764-4364-8440-30409e324a57",
   "metadata": {},
   "source": [
    "### 1.5 IQR Method  \n",
    "Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7b0d-c5dd-4820-974e-445571f160da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            numerical_columns = [numerical_columns]\n",
    "        \n",
    "        # Calculate statistics (first quartile, third quartile, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=numerical_columns)\n",
    "        self.stats_[\"Q1\"] = df[numerical_columns].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[numerical_columns].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[numerical_columns] >= self.stats_[\"lower_cutoff\"]) & (df[numerical_columns] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "               \n",
    "        # Show statistics and number of outliers\n",
    "        print(f\"Statistics and outliers: \\n{self.stats_}\")\n",
    "        if len(numerical_columns) == 1:\n",
    "            print(f\"\\n{self.outliers_} rows contain outliers in the '{numerical_columns[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\n{self.outliers_} rows contain outliers in one or more numerical columns.\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"{self.outliers_} rows with outliers removed.\")\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6dc1b-0947-4db4-98d7-77371aa6bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Identify and remove outliers\n",
    "df_train = outlier_remover_iqr.fit_transform(df_train, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0468a71-229a-4f6b-9520-7a400a3539e7",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "Identify and remove multivariate outliers using the isolation forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8d2e0-bca4-4d41-a331-4de343e9852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize isolation forest\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Create list of numerical and boolean features (without the target variable \"risk_flag\")\n",
    "numerical_boolean_features = numerical_columns + [\"married\", \"car_ownership\"]\n",
    "\n",
    "# Fit isolation forest\n",
    "isolation_forest.fit(df_train[numerical_boolean_features])\n",
    "\n",
    "# Predict outliers\n",
    "df_train[\"outlier\"] = isolation_forest.predict(df_train[numerical_boolean_features])\n",
    "df_train[\"outlier_score\"] = isolation_forest.decision_function(df_train[numerical_boolean_features])\n",
    "\n",
    "# Show number of outliers\n",
    "n_outliers = df_train[\"outlier\"].value_counts()[-1]\n",
    "contamination = df_train[\"outlier\"].value_counts()[-1] / df_train[\"outlier\"].value_counts().sum()\n",
    "print(f\"{n_outliers} rows ({100 * contamination:.1f}%) were identified as multivariate outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7c69a-dd5a-4d3f-9686-92c0f1ded562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to visualize outliers\n",
    "df_train_subsample = df_train[numerical_boolean_features + [\"outlier\"]].sample(n=5000, random_state=42)\n",
    "sns.pairplot(df_train_subsample, hue=\"outlier\", palette={1: \"#4F81BD\", -1: \"#D32F2F\"}, plot_kws={\"alpha\":0.6, \"s\":40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2890-0ad1-4ef6-8438-373b5d958c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df_train = df_train[df_train[\"outlier\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c9c2-14c5-4b4a-bad8-f9a2b71f839d",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "Save preprocessed data from a Pandas DataFrame to a `.csv` file in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afee6f-836b-4efc-a74a-93e4b8677660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save as .csv  \n",
    "df_train.to_csv(\"data/training_data_preprocessed.csv\", index=False)\n",
    "X_test.to_csv(\"data/test_data_preprocessed.csv\", index=False)\n",
    "y_test.to_csv(\"data/sample_prediction_dataset_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dcb62-3ed6-4437-a100-ae5c317edd5e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae0b1c-675f-41b3-89ab-6d4f3cb308f5",
   "metadata": {},
   "source": [
    "## Univariate EDA  \n",
    "Analyze the distribution of a single column using descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a00fe4-0b5d-4266-94cf-8145c92f3c83",
   "metadata": {},
   "source": [
    "### Numerical Columns  \n",
    "Examine descriptive statistics (e.g., mean, median, standard deviation, quartiles) and visualize the distributions (e.g., histogram) of numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfd81f-2d6a-4200-9e00-fa158797e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "pd.set_option(\"display.float_format\", \"{:,.1f}\".format)\n",
    "df_train[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d4e48-891a-4a97-871b-ba7259370d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of income \n",
    "sns.histplot(df_train[\"income\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Distribution of income\")\n",
    "plt.xlabel(\"income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9db47f-a247-411d-a964-387fb6af476b",
   "metadata": {},
   "source": [
    "### Categorical Columns  \n",
    "Examine descriptive statistics (e.g., absolute and relative frequencies) and visualize the distributions (e.g., bar plot) of categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddead-be65-4114-a39f-419cee4892cf",
   "metadata": {},
   "source": [
    "Profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbdaee-3785-4677-b803-4e7142e8b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"profession\"].value_counts()\n",
    "relative_frequencies = df_train[\"profession\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b658f-25a5-482b-bbf8-1d2eec7d0c54",
   "metadata": {},
   "source": [
    "City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6216576-ad25-4fb3-a8c0-4fd06e1877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"city\"].value_counts()\n",
    "relative_frequencies = df_train[\"city\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d924e-09c1-4ab9-8a26-25c6c8238dcd",
   "metadata": {},
   "source": [
    "State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad63990c-2ee5-4bbc-9c73-f82353ce403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>absolute_frequency</th>\n",
       "      <th>relative_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uttar_Pradesh</td>\n",
       "      <td>28400</td>\n",
       "      <td>11.269841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>25562</td>\n",
       "      <td>10.143651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andhra_Pradesh</td>\n",
       "      <td>25297</td>\n",
       "      <td>10.038492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West_Bengal</td>\n",
       "      <td>23483</td>\n",
       "      <td>9.318651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bihar</td>\n",
       "      <td>19780</td>\n",
       "      <td>7.849206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tamil_Nadu</td>\n",
       "      <td>16537</td>\n",
       "      <td>6.562302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Madhya_Pradesh</td>\n",
       "      <td>14122</td>\n",
       "      <td>5.603968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>11855</td>\n",
       "      <td>4.704365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>11408</td>\n",
       "      <td>4.526984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>9174</td>\n",
       "      <td>3.640476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>8965</td>\n",
       "      <td>3.557540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>7890</td>\n",
       "      <td>3.130952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Telangana</td>\n",
       "      <td>7524</td>\n",
       "      <td>2.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assam</td>\n",
       "      <td>7062</td>\n",
       "      <td>2.802381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kerala</td>\n",
       "      <td>5805</td>\n",
       "      <td>2.303571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>5490</td>\n",
       "      <td>2.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Punjab</td>\n",
       "      <td>4720</td>\n",
       "      <td>1.873016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>4658</td>\n",
       "      <td>1.848413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>3834</td>\n",
       "      <td>1.521429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>1874</td>\n",
       "      <td>0.743651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jammu_and_Kashmir</td>\n",
       "      <td>1780</td>\n",
       "      <td>0.706349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Puducherry</td>\n",
       "      <td>1433</td>\n",
       "      <td>0.568651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Manipur</td>\n",
       "      <td>849</td>\n",
       "      <td>0.336905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mizoram</td>\n",
       "      <td>849</td>\n",
       "      <td>0.336905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Himachal_Pradesh</td>\n",
       "      <td>833</td>\n",
       "      <td>0.330556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Tripura</td>\n",
       "      <td>809</td>\n",
       "      <td>0.321032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Uttar_Pradesh[5]</td>\n",
       "      <td>743</td>\n",
       "      <td>0.294841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>656</td>\n",
       "      <td>0.260317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sikkim</td>\n",
       "      <td>608</td>\n",
       "      <td>0.241270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                state  absolute_frequency  relative_frequency\n",
       "0       Uttar_Pradesh               28400           11.269841\n",
       "1         Maharashtra               25562           10.143651\n",
       "2      Andhra_Pradesh               25297           10.038492\n",
       "3         West_Bengal               23483            9.318651\n",
       "4               Bihar               19780            7.849206\n",
       "5          Tamil_Nadu               16537            6.562302\n",
       "6      Madhya_Pradesh               14122            5.603968\n",
       "7           Karnataka               11855            4.704365\n",
       "8             Gujarat               11408            4.526984\n",
       "9           Rajasthan                9174            3.640476\n",
       "10          Jharkhand                8965            3.557540\n",
       "11            Haryana                7890            3.130952\n",
       "12          Telangana                7524            2.985714\n",
       "13              Assam                7062            2.802381\n",
       "14             Kerala                5805            2.303571\n",
       "15              Delhi                5490            2.178571\n",
       "16             Punjab                4720            1.873016\n",
       "17             Odisha                4658            1.848413\n",
       "18       Chhattisgarh                3834            1.521429\n",
       "19        Uttarakhand                1874            0.743651\n",
       "20  Jammu_and_Kashmir                1780            0.706349\n",
       "21         Puducherry                1433            0.568651\n",
       "22            Manipur                 849            0.336905\n",
       "23            Mizoram                 849            0.336905\n",
       "24   Himachal_Pradesh                 833            0.330556\n",
       "25            Tripura                 809            0.321032\n",
       "26   Uttar_Pradesh[5]                 743            0.294841\n",
       "27         Chandigarh                 656            0.260317\n",
       "28             Sikkim                 608            0.241270"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"state\"].value_counts()\n",
    "relative_frequencies = df_train[\"state\"].value_counts(normalize=True) * 100  # in percent\n",
    "\n",
    "# Show frequencies\n",
    "pd.concat([absolute_frequencies, relative_frequencies], axis=1, keys=[\"absolute_frequency\", \"relative_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815f4f-0f4f-456f-8eb3-6eaebf035b19",
   "metadata": {},
   "source": [
    "# Future Improvements\n",
    "\n",
    "**Data Enrichment**:  \n",
    "To enhance the model's performance and business value, data enrichment with the following financial features is recommended:\n",
    "- Loan amount \n",
    "- Loan duration\n",
    "- Interest rate\n",
    "- Type of loan (e.g., personal, home, vehicle)\n",
    "- Existing debt\n",
    "- Credit score\n",
    "\n",
    "**Enhanced Analysis Capabilities**:  \n",
    "The addition of these features, particularly loan amount, would enable more precise risk assessment and better alignment with business objectives through cost-sensitive evaluation metrics incorporating actual monetary values. Cost-sensitive metrics or expected monetary value incorporate the actual cost of defaults (false negatives) and the opportunity cost of rejecting good loans (false positives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-default-venv",
   "language": "python",
   "name": "loan-default-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
