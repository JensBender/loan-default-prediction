{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d843a7-8378-4685-bc72-96cf409b3000",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:36px; font-weight:bold; color:#4A4A4A; background-color:#fff6e4; padding:10px; border:3px solid #f5ecda; border-radius:6px\">\n",
    "    Predicting Loan Defaults\n",
    "    <p style=\"text-align:center; font-size:14px; font-weight:normal; color:#4A4A4A; margin-top:12px;\">\n",
    "        Author: Jens Bender <br> \n",
    "        Created: January 2025<br>\n",
    "        Last updated: May 2025\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf3cd6-69e4-4aa0-a083-987254cd77f0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Project Overview</h1>\n",
    "</div> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4275851-2816-4903-91c3-8b4a4c489b12",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "This project aims to develop a machine learning model to predict whether the customers of a financial institution will default on a loan based on data from their loan application. By accurately identifying potential defaulters, financial institutions can make more informed lending decisions, reduce losses, improve profitability, and increase operational efficiency through the automation of risk assessment.\n",
    "\n",
    "**Problem**  \n",
    "Predicting loan defaults is a challenging task due to the multitude of influencing factors such as customers' demographic, financial, location, and behavioral attributes. Traditional default prediction models often oversimplify complex relationships between customer features and default risk. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in loan application data, enabling more accurate predictions of loan default risk.\n",
    "\n",
    "**Objectives**  \n",
    "- Develop a machine learning model to predict loan defaults using customer data from loan applications.\n",
    "- Compare multiple models (e.g., Logistic Regression, Random Forest, XGBoost) using a suitable evaluation metric (such as AUC-PR).\n",
    "- Identify key factors influencing loan default risk through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project enables financial institutions to reduce loan default rates and make better and faster lending decisions by leveraging machine learning for automated and improved risk assessment. \n",
    "\n",
    "**Business Goals**  \n",
    "- Reduce losses by 5M-10M INR within 12 months of model deployment by decreasing the loan default rate by 10%-20%.\n",
    "- Decrease loan processing time by 25%-40% by automating risk assessment, leading to less time spent on manual evaluations.\n",
    "- Ensure 100% compliance with regulatory requirements and fair lending practices.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains information provided by customers of a financial institution during the loan application process. It is sourced from the \"Loan Prediction Based on Customer Behavior\" dataset by Subham Jain, available on [Kaggle](https://www.kaggle.com/datasets/subhamjain/loan-prediction-based-on-customer-behavior). Stored in `Training Data.csv`, it contains the features, target variable (`Risk Flag`), and `ID` column. \n",
    "\n",
    "Dataset Statistics:\n",
    "- Dataset size: 252,000 records \n",
    "- Target variable: Risk flag (12.3% defaults)\n",
    "- Features: 11 \n",
    "  - Demographic: Age, married, profession\n",
    "  - Financial: Income, house ownership, car ownership\n",
    "  - Location: City, state\n",
    "  - Behavioral: Experience, current job years, current house years\n",
    "\n",
    "Data Overview Table:\n",
    "\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Observed Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Risk Flag | Defaulted on loan (0: No, 1: Yes) | Integer | Categorical (Binary) | [0, 1] | [0, 1] |\n",
    "| Income | Income of the applicant | Integer | Numerical | [0, ∞] | [10K, 10M] |\n",
    "| Age | Age of the applicant (in years) | Integer | Numerical | [18, ∞] | [21, 79] |\n",
    "| Experience | Work experience (in years) | Integer | Numerical | [0, ∞] | [0, 20] |\n",
    "| Profession | Applicant's profession | String | Categorical (Nominal) | Any profession [e.g., \"Architect\", \"Dentist\"] | 51 unique professions |\n",
    "| Married | Marital status | String | Categorical (Binary) | [\"single\", \"married\"] | [\"single\", \"married\"] |\n",
    "| House Ownership | Applicant owns or rents a house | String | Categorical (Nominal) | [\"rented\", \"owned\", \"norent_noown\"] | [\"rented\", \"owned\", \"norent_noown\"] |\n",
    "| Car Ownership | Whether applicant owns a car | String | Categorical (Binary) | [\"yes\", \"no\"] | [\"yes\", \"no\"] |\n",
    "| Current Job Years | Years in the current job | Integer | Numerical | [0, ∞] | [0, 14] |\n",
    "| Current House Years | Years in the current house | Integer | Numerical | [0, ∞] | [10, 14] |\n",
    "| City | City of residence | String | Categorical (Nominal) | Any city [e.g., \"Mumbai\", \"Bangalore\"] | 317 unique cities |\n",
    "| State | State of residence | String | Categorical (Nominal) | Any state [e.g., \"Maharashtra\", \"Tamil_Nadu\"] | 29 unique states |\n",
    "\n",
    "Example Data:\n",
    "\n",
    "| Risk Flag | Income    | Age | Experience | Profession         | Married | House Ownership | Car Ownership | Current Job Years | Current House Years | City      | State         |\n",
    "| :-------- | :-------- | :-- | :--------- | :----------------- | :------ | :-------------- | :------------ | :---------------- | :------------------ | :-------- | :------------ |\n",
    "| 0         | 1,303,834 | 23  | 3          | Mechanical_engineer | single  | rented          | no            | 3                 | 13                   | Rewa      | Madhya_Pradesh |\n",
    "| 1         | 6,256,451 | 41  | 2          | Software_Developer | single  | rented          | yes           | 2                 | 12                   | Bangalore | Tamil_Nadu    |\n",
    "| 0         | 3,991,815 | 66  | 4          | Technical_writer   | married | rented          | no            | 4                 | 10                   | Alappuzha | Kerala        |\n",
    "\n",
    "**Technical Requirements**  \n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, data types, missing values, and outliers.\n",
    "  - Perform train-validation-test split, feature engineering, scaling, and encoding.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models, perform hyperparameter tuning, and optimize thresholds using `sklearn` and `xgboost`.\n",
    "  - Baseline models: Logistic Regression, Elastic Net, K-Nearest Neighbors, Support Vector Machine, Multi-Layer Perceptron, Decision Tree, Random Forest, XGBoost.\n",
    "  - Evaluate model performance for binary classification task: \n",
    "    - Primary metric: Area Under the Precision-Recall Curve (AUC-PR), as it suits class imbalance (12.3% defaults) with a focus on preventing defaults.\n",
    "    - Secondary metrics: Class-1-specific recall, precision, and F1-score.\n",
    "    - Additional diagnostics: Precision-recall curves, classification report, confusion matrix, overfitting, feature misclassification analysis.\n",
    "    - Success criteria: Minimum class-1 recall of 0.75 and class-1 precision of 0.50 on the test data. \n",
    "  - Visualize feature importance, show model prediction examples, and save the final model.\n",
    "- Deployment:\n",
    "  - Expose the final model via a REST API for easy integration with existing loan processing systems.\n",
    "  - Implement efficient batch processing capabilities to handle up to 10K predictions in under 30 seconds.\n",
    "  - Deploy using cloud infrastructure to ensure scalability and security.\n",
    "  - Set up model performance monitoring and data drift detection.\n",
    "- Stakeholders:\n",
    "  - Loan officers: Direct users of the model predictions in day-to-day loan approvals.\n",
    "  - Credit risk analysts: Provide subject matter expertise on loan default risk.\n",
    "  - Compliance officers: Ensure the model complies with any legal and regulatory guidelines.\n",
    "  - IT department: Manage the IT infrastructure and ensure data access for the model's development and deployment. \n",
    "\n",
    "By fulfilling these requirements, the project will provide a valuable tool for predicting loan defaults, thereby enhancing decision-making for financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9b38-149a-4c08-bb6a-c1ad97d33d28",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Imports</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd00e3c4-73f9-45bb-a474-58fc6049796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, auc, accuracy_score, precision_recall_fscore_support\n",
    "from scipy.stats import randint, uniform\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7316e-f065-4b92-8f2c-5cd5681b5a69",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Loading and Inspection</h1>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Load data from the <code>.csv</code> file into a Pandas DataFrames.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1e0114-5f1a-44ce-85e7-3136fe031aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"data/training_data.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866b8d1-12e6-4f47-b54e-8132b55b4c1e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Initial data inspection to understand the structure of the dataset and detect obvious issues.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc38bee-eefa-48ce-9ddb-b9464e6caa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3692-d2c8-46f1-b0ad-36372c1fa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the training data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9596-7328-43a4-8cf6-a18d1b64b4f7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Preprocessing</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef27083-1da0-42ce-a1b0-ad15b1b7a495",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Standardizing Names and Labels</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Column Names</strong> <br> \n",
    "    📌 Convert all column names to snake_case for consistency, improved readability, and to minimize the risk of errors.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0e7b5f-3c5a-40c8-9f56-9d6040da7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake_case\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  # Replace spaces and special characters with \"_\"\n",
    "    .str.replace(\"_single\", \"\")  # Shorten \"married_single\" to \"married\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7733ee-1f2d-48f3-92ae-1504a4b7f6d1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Categorical Labels</strong> <br> \n",
    "    📌 Convert all categorical labels to snake_case for consistency, improved readability, and to minimize the risk of errors.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5e3b2-0cff-4253-8e60-ca868377217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_labels(categorical_label):\n",
    "    return (\n",
    "        categorical_label\n",
    "        .strip()  # Remove leading/trailing spaces\n",
    "        .lower()  # Convert to lowercase\n",
    "        .replace(\"-\", \"_\")  # Replace hyphens with \"_\"\n",
    "        .replace(\"/\", \"_\")  # Replace slashes with \"_\"\n",
    "        .replace(\" \", \"_\")  # Replace spaces with \"_\"\n",
    "    )\n",
    "\n",
    "# Define categorical columns to standardize labels\n",
    "columns_to_standardize = [\"profession\", \"city\", \"state\"]\n",
    "\n",
    "# Apply standardization of categorical labels\n",
    "for column in columns_to_standardize:\n",
    "    df[column] = df[column].apply(standardize_categorical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565d7a-e2b0-472d-91dd-f860878e384b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Duplicates</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify and remove duplicates based on all columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d98e50-9696-47fe-b1ad-51ec59375e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns\n",
    "print(df.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b48ba-dc42-4591-bc44-e490092a79f2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ✅ No duplicates were found based on all columns in both the training and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029181-c3e9-4588-8055-7770ab866eff",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify and remove duplicates based on the ID column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e53c-dcfa-42eb-8542-7f215b4d3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "print(df.duplicated(subset=[\"id\"]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57954bf9-09ec-49df-8cb7-ed39d546d14f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ✅ No duplicates were found based on the ID column in both the training and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89142d-7b4a-4b23-98b3-3c9e63ee5eb8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Data Types</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify and convert incorrect storage data types.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be913b0-2788-4e8d-8797-37233709dfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify storage data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da11a2-50d5-4c7a-907e-3b71830f0295",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ✅ No incorrect storage data types were found at first glance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb80b7-c29b-4dd9-83ce-1b6a94abdb67",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify object columns with two unique categories and convert them to boolean columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0d39d-104b-47b4-8538-ffd2f55a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object columns with two unique categories \n",
    "print(df[df.select_dtypes(include=[\"object\"]).columns.tolist()].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876603a-e2d3-493e-a1a7-cdf2233a8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert married and car_ownership column from object to boolean\n",
    "df[\"married\"] = df[\"married\"].map({\"married\": True, \"single\": False})\n",
    "df[\"car_ownership\"] = df[\"car_ownership\"].map({\"yes\": True, \"no\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35470664-5860-4f0f-ac00-333004e26263",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Train-Validation-Test Split</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Split the data into 80% for training, 10% for validation, and 10% for testing.\n",
    "    <table style=\"margin-left:0; margin-top:20px; margin-bottom:20px\">\n",
    "        <tr>\n",
    "            <th style=\"background-color:#f5ecda;\">Data</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Size (%)</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Size (Total)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4;\">Training Set</td>\n",
    "            <td style=\"background-color:#fff6e4;\">80%</td>\n",
    "            <td style=\"background-color:#fff6e4;\">201,600</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#f5ecda;\">Validation Set</td>\n",
    "            <td style=\"background-color:#f5ecda;\">10%</td>\n",
    "            <td style=\"background-color:#f5ecda;\">25,200</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4;\">Test Set</td>\n",
    "            <td style=\"background-color:#fff6e4;\">10%</td>\n",
    "            <td style=\"background-color:#fff6e4;\">25,200</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb2ee21-930d-44fa-8740-41255d97336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"risk_flag\", axis=1)\n",
    "y = df[\"risk_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee908d5-43d1-4098-8c78-8a4c61aacebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (80% train, 20% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary data into validation and test sets (50% each)\n",
    "# Note: This accomplishes a 80% training, 10% validation and 10% test set size\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Delete the temporary data to free up memory\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413ec8b-66cd-4d93-a071-7b076db6fbf4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Engineering New Features</h2>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07a4d2-f28d-4ad1-a618-fde9e0753d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore number of unique categories in categorical columns\n",
    "print(\"Training Data:\")\n",
    "print(X_train[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(X_val[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())\n",
    "print(\"\\nTest Data:\")\n",
    "print(X_test[[\"house_ownership\", \"profession\", \"city\", \"state\"]].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965f441-5b9b-4c4f-8d6d-1a3f40998771",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Profession-Based Features</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Job Stability</strong> <br>\n",
    "    📌 Derive job stability from profession.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63250d-c76a-42b9-b8c6-a4f0d94b9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_job_stability(profession):\n",
    "    job_stability_map = {\n",
    "        # Government and highly regulated roles with exceptional job security\n",
    "        \"civil_servant\": \"very_stable\",\n",
    "        \"army_officer\": \"very_stable\",\n",
    "        \"police_officer\": \"very_stable\",\n",
    "        \"magistrate\": \"very_stable\",\n",
    "        \"official\": \"very_stable\",\n",
    "        \"air_traffic_controller\": \"very_stable\",\n",
    "        \"firefighter\": \"very_stable\",\n",
    "        \"librarian\": \"very_stable\",\n",
    "        \n",
    "        # Licensed/regulated professionals with strong job security\n",
    "        \"physician\": \"stable\",\n",
    "        \"surgeon\": \"stable\",\n",
    "        \"dentist\": \"stable\",\n",
    "        \"chartered_accountant\": \"stable\",\n",
    "        \"civil_engineer\": \"stable\",\n",
    "        \"mechanical_engineer\": \"stable\",\n",
    "        \"chemical_engineer\": \"stable\",\n",
    "        \"petroleum_engineer\": \"stable\",\n",
    "        \"biomedical_engineer\": \"stable\",\n",
    "        \"engineer\": \"stable\",\n",
    "        \n",
    "        # Corporate roles with steady demand\n",
    "        \"software_developer\": \"moderate\",\n",
    "        \"computer_hardware_engineer\": \"moderate\",\n",
    "        \"financial_analyst\": \"moderate\",\n",
    "        \"industrial_engineer\": \"moderate\",\n",
    "        \"statistician\": \"moderate\",\n",
    "        \"microbiologist\": \"moderate\",\n",
    "        \"scientist\": \"moderate\",\n",
    "        \"geologist\": \"moderate\",\n",
    "        \"economist\": \"moderate\",\n",
    "        \"technology_specialist\": \"moderate\",\n",
    "        \"design_engineer\": \"moderate\",\n",
    "        \"architect\": \"moderate\",\n",
    "        \"surveyor\": \"moderate\",\n",
    "        \"secretary\": \"moderate\",\n",
    "        \"flight_attendant\": \"moderate\",\n",
    "        \"hotel_manager\": \"moderate\",\n",
    "        \"computer_operator\": \"moderate\",\n",
    "        \"technician\": \"moderate\",\n",
    "        \n",
    "        # Project-based or variable demand roles\n",
    "        \"web_designer\": \"variable\",\n",
    "        \"fashion_designer\": \"variable\",\n",
    "        \"graphic_designer\": \"variable\",\n",
    "        \"designer\": \"variable\",\n",
    "        \"consultant\": \"variable\",\n",
    "        \"technical_writer\": \"variable\",\n",
    "        \"artist\": \"variable\",\n",
    "        \"comedian\": \"variable\",\n",
    "        \"chef\": \"variable\",\n",
    "        \"analyst\": \"variable\",\n",
    "        \"psychologist\": \"variable\",\n",
    "        \"drafter\": \"variable\",\n",
    "        \"aviator\": \"variable\",\n",
    "        \"politician\": \"variable\",\n",
    "        \"lawyer\": \"variable\"\n",
    "    }\n",
    "\n",
    "    # Return the job stability score based on the profession (default to \"moderate\" for unknown categories)\n",
    "    return job_stability_map.get(profession, \"moderate\")\n",
    "    \n",
    "# Apply function to create job stability feature in training, validation, and test data\n",
    "X_train[\"job_stability\"] = X_train[\"profession\"].map(derive_job_stability)\n",
    "X_val[\"job_stability\"] = X_val[\"profession\"].map(derive_job_stability)\n",
    "X_test[\"job_stability\"] = X_test[\"profession\"].map(derive_job_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b0bae-6b21-4c17-9b87-f2c63d1ca0cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Location-Based Features</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "    <strong>City Tier</strong> </br>\n",
    "    📌 Derive city tier from city. Specifically, categorize cities into three tiers that reflect differences in employment opportunities, income levels, cost of living, population densitiy, and economic activity.\n",
    "<ul>\n",
    "  <li>Tier 1: Large metropolitan cities with high population density, significant economic activity, and robust infrastructure. India's most developed and urbanized cities.</li>\n",
    "  <li>Tier 2: Medium-sized cities with growing industries, regional importance, and moderate economic activity. Less urbanized than Tier 1.</li>\n",
    "  <li>Tier 3: Smaller cities or towns with limited industrial and economic activity, often rural or semi-urban areas.</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eca54-93ec-43b5-a815-3221da489f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_city_tier(city):\n",
    "    tier_map = {\n",
    "        # Tier 1 cities\n",
    "        \"new_delhi\": \"tier_1\",\n",
    "        \"navi_mumbai\": \"tier_1\",\n",
    "        \"kolkata\": \"tier_1\",\n",
    "        \"bangalore\": \"tier_1\",\n",
    "        \"chennai\": \"tier_1\",\n",
    "        \"hyderabad\": \"tier_1\",\n",
    "        \"mumbai\": \"tier_1\",\n",
    "        \"pune\": \"tier_1\",\n",
    "        \"ahmedabad\": \"tier_1\",\n",
    "        \"jaipur\": \"tier_1\",\n",
    "        \"lucknow\": \"tier_1\",\n",
    "        \"noida\": \"tier_1\",\n",
    "        \"coimbatore\": \"tier_1\",\n",
    "        \"surat\": \"tier_1\",\n",
    "        \"nagpur\": \"tier_1\",\n",
    "        \"kochi\": \"tier_1\",\n",
    "        \"thiruvananthapuram\": \"tier_1\",\n",
    "        \"kanpur\": \"tier_1\",\n",
    "        \"patna\": \"tier_1\",\n",
    "        \n",
    "        # Tier 2 cities\n",
    "        \"bhopal\": \"tier_2\",\n",
    "        \"vijayawada\": \"tier_2\",\n",
    "        \"indore\": \"tier_2\",\n",
    "        \"jodhpur\": \"tier_2\",\n",
    "        \"vadodara\": \"tier_2\",\n",
    "        \"ludhiana\": \"tier_2\",\n",
    "        \"madurai\": \"tier_2\",\n",
    "        \"agra\": \"tier_2\",\n",
    "        \"mysore[7][8][9]\": \"tier_2\",\n",
    "        \"rajkot\": \"tier_2\",\n",
    "        \"nashik\": \"tier_2\",\n",
    "        \"amritsar\": \"tier_2\",\n",
    "        \"ranchi\": \"tier_2\",\n",
    "        \"chandigarh_city\": \"tier_2\",\n",
    "        \"allahabad\": \"tier_2\",\n",
    "        \"bhubaneswar\": \"tier_2\",\n",
    "        \"varanasi\": \"tier_2\",\n",
    "        \"jabalpur\": \"tier_2\",\n",
    "        \"guwahati\": \"tier_2\",\n",
    "        \"tiruppur\": \"tier_2\",\n",
    "        \"raipur\": \"tier_2\",\n",
    "        \"udaipur\": \"tier_2\",\n",
    "        \"gwalior\": \"tier_2\",\n",
    "        \n",
    "        # Tier 3 cities\n",
    "        \"vijayanagaram\": \"tier_3\",\n",
    "        \"bulandshahr\": \"tier_3\",\n",
    "        \"saharsa[29]\": \"tier_3\",\n",
    "        \"hajipur[31]\": \"tier_3\",\n",
    "        \"satara\": \"tier_3\",\n",
    "        \"ongole\": \"tier_3\",\n",
    "        \"bellary\": \"tier_3\",\n",
    "        \"giridih\": \"tier_3\",\n",
    "        \"hospet\": \"tier_3\",\n",
    "        \"khammam\": \"tier_3\",\n",
    "        \"danapur\": \"tier_3\",\n",
    "        \"bareilly\": \"tier_3\",\n",
    "        \"satna\": \"tier_3\",\n",
    "        \"howrah\": \"tier_3\",\n",
    "        \"thanjavur\": \"tier_3\",\n",
    "        \"farrukhabad\": \"tier_3\",\n",
    "        \"buxar[37]\": \"tier_3\",\n",
    "        \"arrah\": \"tier_3\",\n",
    "        \"thrissur\": \"tier_3\",\n",
    "        \"proddatur\": \"tier_3\",\n",
    "        \"bahraich\": \"tier_3\",\n",
    "        \"nandyal\": \"tier_3\",\n",
    "        \"siwan[32]\": \"tier_3\",\n",
    "        \"barasat\": \"tier_3\",\n",
    "        \"dhule\": \"tier_3\",\n",
    "        \"begusarai\": \"tier_3\",\n",
    "        \"khandwa\": \"tier_3\",\n",
    "        \"guntakal\": \"tier_3\",\n",
    "        \"latur\": \"tier_3\",\n",
    "        \"karaikudi\": \"tier_3\"\n",
    "    }\n",
    "    \n",
    "    # Return city tier based on the city (default to \"unknown\" for unknown categories)\n",
    "    return tier_map.get(city, \"unknown\")\n",
    "\n",
    "# Apply function to create city tier feature in training, validation, and test data\n",
    "X_train[\"city_tier\"] = X_train[\"city\"].map(derive_city_tier)\n",
    "X_val[\"city_tier\"] = X_val[\"city\"].map(derive_city_tier)\n",
    "X_test[\"city_tier\"] = X_test[\"city\"].map(derive_city_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d2bf-5854-406a-98d7-77bb5cd7f083",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "    <strong>State Default Rate</strong> </br>\n",
    "    📌 Derive state default rate from state using target encoding.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab2f19-eade-4898-a091-0b58aa213179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Calculate default rate by state based on the training data\n",
    "default_rate_by_state = df_train.groupby(\"state\")[\"risk_flag\"].mean()\n",
    "\n",
    "# Create state default rate feature in training, validation, and test data by replacing the state with its corresponding default rate\n",
    "X_train[\"state_default_rate\"] = X_train[\"state\"].map(default_rate_by_state)\n",
    "X_val[\"state_default_rate\"] = X_val[\"state\"].map(default_rate_by_state)\n",
    "X_test[\"state_default_rate\"] = X_test[\"state\"].map(default_rate_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0420a-3d11-4dfd-ac78-fb7e54a7a5f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Defining Semantic Type</h2>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Define semantic column types (numerical, categorical, boolean) for downstream tasks like additional preprocessing steps, exploratory data analysis, and machine learning.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d07e51-218e-4826-99cc-73de5abf3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic column types manually\n",
    "numerical_columns = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\"]\n",
    "categorical_columns = [\"house_ownership\", \"job_stability\", \"city_tier\", \"profession\", \"city\", \"state\"]\n",
    "boolean_columns = [\"risk_flag\", \"married\", \"car_ownership\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332edd65-5789-48e4-8aa6-d055d94711b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Missing Values</h2>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b2a2c441-9490-4ad7-9f7b-61811201ea5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input X must be a pandas DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 356\u001b[0m\n\u001b[0;32m    343\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    344\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_value_checker\u001b[39m\u001b[38;5;124m\"\u001b[39m, MissingValueChecker(critical_features\u001b[38;5;241m=\u001b[39mCRITICAL_FEATURES, non_critical_features\u001b[38;5;241m=\u001b[39mNON_CRITICAL_FEATURES)),\n\u001b[0;32m    345\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_value_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, ColumnTransformer(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_default_rate_target_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, StateDefaultRateTargetEncoder())\n\u001b[0;32m    353\u001b[0m ])\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Fit pipeline on training data\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Use pipeline to transform training, validation, and test data\u001b[39;00m\n\u001b[0;32m    359\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtransform(X_train)\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\pipeline.py:652\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     )\n\u001b[0;32m    651\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 652\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\pipeline.py:586\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    580\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    581\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    582\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    583\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    584\u001b[0m )\n\u001b[1;32m--> 586\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\DataScience\\Projects\\loan-default-prediction\\loan-default-venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[132], line 231\u001b[0m, in \u001b[0;36mCategoricalLabelStandardizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X must be a pandas DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m     X_transformed \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    234\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m X_transformed\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# Use provided columns, otherwise all columns\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input X must be a pandas DataFrame."
     ]
    }
   ],
   "source": [
    "# --- Pipeline: Data Preprocessing and Model ---\n",
    "# --- Step 0: Define mappings and semantic column types for data preprocessing ---\n",
    "# Map binary categorical columns to boolean\n",
    "BOOLEAN_COLUMN_MAPPINGS = {\n",
    "    \"married\": {\"married\": True, \"single\": False},\n",
    "    \"car_ownership\": {\"yes\": True, \"no\": False}\n",
    "}\n",
    "\n",
    "# Map profession to job stability tier (very stable, stable, moderate, variable)\n",
    "JOB_STABILITY_MAP = {\n",
    "    # Government and highly regulated roles with exceptional job security\n",
    "    \"civil_servant\": \"very_stable\",\n",
    "    \"army_officer\": \"very_stable\",\n",
    "    \"police_officer\": \"very_stable\",\n",
    "    \"magistrate\": \"very_stable\",\n",
    "    \"official\": \"very_stable\",\n",
    "    \"air_traffic_controller\": \"very_stable\",\n",
    "    \"firefighter\": \"very_stable\",\n",
    "    \"librarian\": \"very_stable\",\n",
    "    \n",
    "    # Licensed/regulated professionals with strong job security\n",
    "    \"physician\": \"stable\",\n",
    "    \"surgeon\": \"stable\",\n",
    "    \"dentist\": \"stable\",\n",
    "    \"chartered_accountant\": \"stable\",\n",
    "    \"civil_engineer\": \"stable\",\n",
    "    \"mechanical_engineer\": \"stable\",\n",
    "    \"chemical_engineer\": \"stable\",\n",
    "    \"petroleum_engineer\": \"stable\",\n",
    "    \"biomedical_engineer\": \"stable\",\n",
    "    \"engineer\": \"stable\",\n",
    "    \n",
    "    # Corporate roles with steady demand\n",
    "    \"software_developer\": \"moderate\",\n",
    "    \"computer_hardware_engineer\": \"moderate\",\n",
    "    \"financial_analyst\": \"moderate\",\n",
    "    \"industrial_engineer\": \"moderate\",\n",
    "    \"statistician\": \"moderate\",\n",
    "    \"microbiologist\": \"moderate\",\n",
    "    \"scientist\": \"moderate\",\n",
    "    \"geologist\": \"moderate\",\n",
    "    \"economist\": \"moderate\",\n",
    "    \"technology_specialist\": \"moderate\",\n",
    "    \"design_engineer\": \"moderate\",\n",
    "    \"architect\": \"moderate\",\n",
    "    \"surveyor\": \"moderate\",\n",
    "    \"secretary\": \"moderate\",\n",
    "    \"flight_attendant\": \"moderate\",\n",
    "    \"hotel_manager\": \"moderate\",\n",
    "    \"computer_operator\": \"moderate\",\n",
    "    \"technician\": \"moderate\",\n",
    "    \n",
    "    # Project-based or variable demand roles\n",
    "    \"web_designer\": \"variable\",\n",
    "    \"fashion_designer\": \"variable\",\n",
    "    \"graphic_designer\": \"variable\",\n",
    "    \"designer\": \"variable\",\n",
    "    \"consultant\": \"variable\",\n",
    "    \"technical_writer\": \"variable\",\n",
    "    \"artist\": \"variable\",\n",
    "    \"comedian\": \"variable\",\n",
    "    \"chef\": \"variable\",\n",
    "    \"analyst\": \"variable\",\n",
    "    \"psychologist\": \"variable\",\n",
    "    \"drafter\": \"variable\",\n",
    "    \"aviator\": \"variable\",\n",
    "    \"politician\": \"variable\",\n",
    "    \"lawyer\": \"variable\"\n",
    "}\n",
    "\n",
    "# Map city to city tier\n",
    "CITY_TIER_MAP = {\n",
    "    # Tier 1 cities\n",
    "    \"new_delhi\": \"tier_1\",\n",
    "    \"navi_mumbai\": \"tier_1\",\n",
    "    \"kolkata\": \"tier_1\",\n",
    "    \"bangalore\": \"tier_1\",\n",
    "    \"chennai\": \"tier_1\",\n",
    "    \"hyderabad\": \"tier_1\",\n",
    "    \"mumbai\": \"tier_1\",\n",
    "    \"pune\": \"tier_1\",\n",
    "    \"ahmedabad\": \"tier_1\",\n",
    "    \"jaipur\": \"tier_1\",\n",
    "    \"lucknow\": \"tier_1\",\n",
    "    \"noida\": \"tier_1\",\n",
    "    \"coimbatore\": \"tier_1\",\n",
    "    \"surat\": \"tier_1\",\n",
    "    \"nagpur\": \"tier_1\",\n",
    "    \"kochi\": \"tier_1\",\n",
    "    \"thiruvananthapuram\": \"tier_1\",\n",
    "    \"kanpur\": \"tier_1\",\n",
    "    \"patna\": \"tier_1\",\n",
    "    \n",
    "    # Tier 2 cities\n",
    "    \"bhopal\": \"tier_2\",\n",
    "    \"vijayawada\": \"tier_2\",\n",
    "    \"indore\": \"tier_2\",\n",
    "    \"jodhpur\": \"tier_2\",\n",
    "    \"vadodara\": \"tier_2\",\n",
    "    \"ludhiana\": \"tier_2\",\n",
    "    \"madurai\": \"tier_2\",\n",
    "    \"agra\": \"tier_2\",\n",
    "    \"mysore[7][8][9]\": \"tier_2\",\n",
    "    \"rajkot\": \"tier_2\",\n",
    "    \"nashik\": \"tier_2\",\n",
    "    \"amritsar\": \"tier_2\",\n",
    "    \"ranchi\": \"tier_2\",\n",
    "    \"chandigarh_city\": \"tier_2\",\n",
    "    \"allahabad\": \"tier_2\",\n",
    "    \"bhubaneswar\": \"tier_2\",\n",
    "    \"varanasi\": \"tier_2\",\n",
    "    \"jabalpur\": \"tier_2\",\n",
    "    \"guwahati\": \"tier_2\",\n",
    "    \"tiruppur\": \"tier_2\",\n",
    "    \"raipur\": \"tier_2\",\n",
    "    \"udaipur\": \"tier_2\",\n",
    "    \"gwalior\": \"tier_2\",\n",
    "    \n",
    "    # Tier 3 cities\n",
    "    \"vijayanagaram\": \"tier_3\",\n",
    "    \"bulandshahr\": \"tier_3\",\n",
    "    \"saharsa[29]\": \"tier_3\",\n",
    "    \"hajipur[31]\": \"tier_3\",\n",
    "    \"satara\": \"tier_3\",\n",
    "    \"ongole\": \"tier_3\",\n",
    "    \"bellary\": \"tier_3\",\n",
    "    \"giridih\": \"tier_3\",\n",
    "    \"hospet\": \"tier_3\",\n",
    "    \"khammam\": \"tier_3\",\n",
    "    \"danapur\": \"tier_3\",\n",
    "    \"bareilly\": \"tier_3\",\n",
    "    \"satna\": \"tier_3\",\n",
    "    \"howrah\": \"tier_3\",\n",
    "    \"thanjavur\": \"tier_3\",\n",
    "    \"farrukhabad\": \"tier_3\",\n",
    "    \"buxar[37]\": \"tier_3\",\n",
    "    \"arrah\": \"tier_3\",\n",
    "    \"thrissur\": \"tier_3\",\n",
    "    \"proddatur\": \"tier_3\",\n",
    "    \"bahraich\": \"tier_3\",\n",
    "    \"nandyal\": \"tier_3\",\n",
    "    \"siwan[32]\": \"tier_3\",\n",
    "    \"barasat\": \"tier_3\",\n",
    "    \"dhule\": \"tier_3\",\n",
    "    \"begusarai\": \"tier_3\",\n",
    "    \"khandwa\": \"tier_3\",\n",
    "    \"guntakal\": \"tier_3\",\n",
    "    \"latur\": \"tier_3\",\n",
    "    \"karaikudi\": \"tier_3\"\n",
    "}\n",
    "\n",
    "# Define semantic column types \n",
    "NUMERICAL_COLUMNS = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\"]\n",
    "CATEGORICAL_COLUMNS = [\"house_ownership\", \"job_stability\", \"city_tier\", \"profession\", \"city\", \"state\"]\n",
    "BOOLEAN_COLUMNS = [\"risk_flag\", \"married\", \"car_ownership\"]\n",
    "\n",
    "# Define critical vs. non-critical features (for input validation and missing value handling, based on feature importance scores)\n",
    "CRITICAL_FEATURES = [\"income\", \"age\", \"experience\", \"profession\", \"city\", \"state\", \"current_job_yrs\", \"current_house_yrs\"]\n",
    "NON_CRITICAL_FEATURES = [\"married\", \"car_ownership\", \"house_ownership\"]\n",
    "\n",
    "\n",
    "# --- Step 1: Check missing values ---\n",
    "class MissingValueChecker(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, critical_features=None, non_critical_features=None):\n",
    "        if critical_features is None or non_critical_features is None:\n",
    "            raise ValueError(\"'critical_features' and 'non_critical_features' cannot be None. Provide both lists.\")\n",
    "\n",
    "        self.critical_features = critical_features\n",
    "        self.non_critical_features = non_critical_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # --- Critical features ---\n",
    "        # Calculate total number of missing values  \n",
    "        n_missing_total_critical = X_transformed[self.critical_features].isnull().sum().sum()\n",
    "        # Calculate number of rows with missing values  \n",
    "        n_missing_rows_critical = X_transformed[self.critical_features].isnull().any(axis=1).sum()\n",
    "        # Create dictionary with number of missing values by column \n",
    "        n_missing_by_column_critical = X_transformed[self.critical_features].isnull().sum().to_dict()\n",
    "        # Raise error  \n",
    "        if n_missing_total_critical > 0:\n",
    "            values = \"value\" if n_missing_total_critical == 1 else \"values\"\n",
    "            rows = \"row\" if n_missing_rows_critical == 1 else \"rows\"\n",
    "            raise ValueError(\n",
    "                f\"{n_missing_total_critical} missing {values} found in critical features \"\n",
    "                f\"across {n_missing_rows_critical} {rows}. Please provide missing {values}.\\n\"\n",
    "                f\"Missing values by column: {n_missing_by_column_critical}\"\n",
    "            )\n",
    "\n",
    "        # --- Non-critical features ---\n",
    "        # Calculate total number of missing values \n",
    "        n_missing_total_noncritical = X_transformed[self.non_critical_features].isnull().sum().sum()        \n",
    "        # Calculate number of rows with missing values \n",
    "        n_missing_rows_noncritical = X_transformed[self.non_critical_features].isnull().any(axis=1).sum()\n",
    "        # Create dictionary with number of missing values by column \n",
    "        n_missing_by_column_noncritical = X_transformed[self.non_critical_features].isnull().sum().to_dict()\n",
    "        # Display warning message\n",
    "        if n_missing_total_noncritical > 0:\n",
    "            values = \"value\" if n_missing_total_noncritical == 1 else \"values\"\n",
    "            rows = \"row\" if n_missing_rows_noncritical == 1 else \"rows\"\n",
    "            print(\n",
    "                f\"Warning: {n_missing_total_noncritical} missing {values} found in non-critical features \"\n",
    "                f\"across {n_missing_rows_noncritical} {rows}. Missing {values} will be imputed.\\n\"\n",
    "                f\"Missing values by column: {n_missing_by_column_noncritical}\"\n",
    "            )\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Step 2: Impute missing values ---\n",
    "# Impute most frequent category with SimpleImputer inside a ColumnTransform\n",
    "\n",
    "\n",
    "# --- Step 3: Standardize categorical labels to snake_case ---\n",
    "class CategoricalLabelStandardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "            \n",
    "        X_transformed = X.copy()\n",
    "        columns = self.columns if self.columns else X_transformed.columns  # Use provided columns, otherwise all columns\n",
    "        for column in columns:\n",
    "            X_transformed[column] = X_transformed[column].apply(\n",
    "                lambda categorical_label: (\n",
    "                    categorical_label\n",
    "                    .strip()  # Remove leading/trailing spaces\n",
    "                    .lower()  # Convert to lowercase\n",
    "                    .replace(\"-\", \"_\")  # Replace hyphens with \"_\"\n",
    "                    .replace(\"/\", \"_\")  # Replace slashes with \"_\"\n",
    "                    .replace(\" \", \"_\")  # Replace spaces with \"_\"\n",
    "                    if isinstance(categorical_label, str) else categorical_label\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Step 4: Convert binary categorical columns to boolean columns ---\n",
    "class BooleanColumnTransformer(BaseEstimator, TransformerMixin):  \n",
    "    def __init__(self, boolean_column_mappings=None):\n",
    "        if boolean_column_mappings is None:\n",
    "            raise ValueError(\"'boolean_column_mappings' cannot be None. It must be a dictionary specifying the mappings.\")\n",
    "            \n",
    "        self.boolean_column_mappings = boolean_column_mappings \n",
    "            \n",
    "    def fit(self, X, y=None):  \n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "            \n",
    "        X_transformed = X.copy()\n",
    "        for column, mapping in self.boolean_column_mappings.items():\n",
    "            if column in X_transformed.columns:\n",
    "                X_transformed[column] = X_transformed[column].map(mapping)\n",
    "                \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Step 5: Derive job stability from profession ---\n",
    "class JobStabilityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, job_stability_map=None):\n",
    "        if job_stability_map is None:\n",
    "            raise ValueError(\"'job_stability_map' cannot be None. It must be a dictionary specifying the mappings from 'profession' to 'job_stability'.\")\n",
    "\n",
    "        self.job_stability_map = job_stability_map \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):  \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "\n",
    "        # Create job stability column by mapping professions to job stability tiers (default to \"moderate\" for unknown professions)\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[\"job_stability\"] = X_transformed[\"profession\"].map(self.job_stability_map).fillna(\"moderate\")\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Step 6: Derive city tier from city ---\n",
    "class CityTierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, city_tier_map=None):\n",
    "        if city_tier_map is None:\n",
    "            raise ValueError(\"'city_tier_map' cannot be None. It must be a dictionary specifying the mappings from 'city' to 'city_tier'.\")\n",
    "\n",
    "        self.city_tier_map = city_tier_map \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):        \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "\n",
    "        # Create city tier column by mapping cities to city tiers (default to \"unknown\" for unknown cities)\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[\"city_tier\"] = X_transformed[\"city\"].map(self.city_tier_map).fillna(\"unknown\")\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Step 7: Target encoding of state default rate ---\n",
    "class StateDefaultRateTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y):\n",
    "        # Merge X and y\n",
    "        df = X.copy()\n",
    "        df[\"target\"] = y.values\n",
    "        \n",
    "        # Calculate default rate by state \n",
    "        self.default_rate_by_state_ = df.groupby(\"state\")[\"target\"].mean()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "\n",
    "        # Create state default rate column by mapping the state to its corresponding default rate\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[\"state_default_rate\"] = X_transformed[\"state\"].map(self.default_rate_by_state_)\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# --- Pipeline ---\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"missing_value_checker\", MissingValueChecker(critical_features=CRITICAL_FEATURES, non_critical_features=NON_CRITICAL_FEATURES)),\n",
    "    (\"missing_value_handler\", ColumnTransformer(\n",
    "        transformers=[(\"categorical_imputer\", SimpleImputer(strategy=\"most_frequent\"), NON_CRITICAL_FEATURES)],  # mode imputation\n",
    "        remainder=\"passthrough\")),\n",
    "    (\"categorical_label_standardizer\", CategoricalLabelStandardizer(columns=[\"profession\", \"city\", \"state\"])),\n",
    "    (\"boolean_column_transformer\", BooleanColumnTransformer(boolean_column_mappings=BOOLEAN_COLUMN_MAPPINGS)),\n",
    "    (\"job_stability_transformer\", JobStabilityTransformer(job_stability_map=JOB_STABILITY_MAP)),\n",
    "    (\"city_tier_transformer\", CityTierTransformer(city_tier_map=CITY_TIER_MAP)),\n",
    "    (\"state_default_rate_target_encoder\", StateDefaultRateTargetEncoder())\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to transform training, validation, and test data\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_val_transformed = pipeline.transform(X_val)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# Sample missing value test data\n",
    "X_test_missing = pd.DataFrame({\n",
    "    \"income\": [50000, 1, 40000],               # Missing in critical\n",
    "    \"age\": [25, 35, 45],                          # No missing\n",
    "    \"experience\": [2, 2, 10],                  # Missing in critical\n",
    "    \"profession\": [\"Engineer\", \"Doctor\", \"Lawyer\"],  # No missing\n",
    "    \"city\": [\"New York\", \"Brussels\", \"Chicago\"],        # Missing in critical\n",
    "    \"state\": [\"NY\", \"CA\", \"IL\"],                  # No missing\n",
    "    \"current_job_yrs\": [1, 2, 3],                 # No missing\n",
    "    \"current_house_yrs\": [3, 4, 5],            # Missing in critical\n",
    "    \"married\": [\"Yes\", \"No\", \"Yes\"],               # Missing in non-critical\n",
    "    \"car_ownership\": [\"Yes\", None, \"Yes\"],         # Missing in non-critical\n",
    "    \"house_ownership\": [None, None, \"No\"],       # Missing in non-critical\n",
    "})\n",
    "\n",
    "X_test_missing_transformed = pipeline.transform(X_test_missing)\n",
    "X_test_missing_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37db6-2329-44af-90ab-ec03a0e6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "print(\"Training Data - Features:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(\"\\nTraining Data - Target Variable:\")\n",
    "print(y_train.isnull().sum())\n",
    "\n",
    "print(\"\\nValidation Data - Features:\")\n",
    "print(X_val.isnull().sum())\n",
    "print(\"\\nValidation Data - Target Variable:\")\n",
    "print(y_val.isnull().sum())\n",
    "\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e30f60-8cfa-419d-a915-e37a9ddde4ac",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ✅ No missing values were found in any of the columns in the training, validation, and test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386d0a-d26f-462c-96f4-4b557da2943c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Outliers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7375e-b714-44f3-b184-c28af316e22a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">3SD Method</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b38504-167d-42e9-9bd5-ab44c552929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "            \n",
    "        # Calculate statistics (mean, standard deviation, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"mean\"] = df[self.numerical_columns_].mean()\n",
    "        self.stats_[\"sd\"] = df[self.numerical_columns_].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"sd\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"sd\"]\n",
    "        \n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "     \n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_3sd.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show outliers in training data\n",
    "print(f\"Training data: Identified {outlier_remover_3sd.outliers_} rows ({outlier_remover_3sd.outliers_ / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "print(\"Statistics and outliers by column:\")\n",
    "round(outlier_remover_3sd.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dd47c-ef92-425a-89c0-bca0ae30982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = outlier_remover_3sd.transform(X_train)\n",
    "print(f\"Training Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_val_no_outliers = outlier_remover_3sd.transform(X_val)\n",
    "print(f\"Validation Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_test_no_outliers = outlier_remover_3sd.transform(X_test)\n",
    "print(f\"Test Data: Removed {(~outlier_remover_3sd.final_mask_).sum()} rows ({(~outlier_remover_3sd.final_mask_).sum() / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858552d1-4764-4364-8440-30409e324a57",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">1.5 IQR Method </h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> 📌 Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7b0d-c5dd-4820-974e-445571f160da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "        \n",
    "        # Calculate statistics (first quartile, third quartile, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"Q1\"] = df[self.numerical_columns_].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[self.numerical_columns_].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "               \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_iqr.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show outliers by column for training data\n",
    "print(f\"Training data: Identified {outlier_remover_iqr.outliers_} rows ({outlier_remover_iqr.outliers_ / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "print(\"Statistics and outliers by column:\")\n",
    "round(outlier_remover_iqr.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6555a-4991-4861-a177-e21b6c8d356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default rate by state\n",
    "default_rate_by_state.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c757cd-3857-4b44-80bd-9284973c7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = outlier_remover_iqr.transform(X_train)\n",
    "print(f\"Training Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_val_no_outliers = outlier_remover_iqr.transform(X_val)\n",
    "print(f\"Validation Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")\n",
    "X_test_no_outliers = outlier_remover_iqr.transform(X_test)\n",
    "print(f\"Test Data: Removed {(~outlier_remover_iqr.final_mask_).sum()} rows ({(~outlier_remover_iqr.final_mask_).sum() / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443aedcf-68b9-457b-8d7e-0d9830e424ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Isolation Forest</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Identify and remove multivariate outliers using the isolation forest algorithm.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8d2e0-bca4-4d41-a331-4de343e9852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize isolation forest\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Create list of numerical and boolean features (without the target variable \"risk_flag\")\n",
    "numerical_boolean_features = numerical_columns + [\"married\", \"car_ownership\"]\n",
    "\n",
    "# Fit isolation forest on training data\n",
    "isolation_forest.fit(X_train[numerical_boolean_features])\n",
    "\n",
    "# Predict outliers on training, validation, and test data\n",
    "X_train[\"outlier\"] = isolation_forest.predict(X_train[numerical_boolean_features])\n",
    "X_train[\"outlier_score\"] = isolation_forest.decision_function(X_train[numerical_boolean_features])\n",
    "X_val[\"outlier\"] = isolation_forest.predict(X_val[numerical_boolean_features])\n",
    "X_val[\"outlier_score\"] = isolation_forest.decision_function(X_val[numerical_boolean_features])\n",
    "X_test[\"outlier\"] = isolation_forest.predict(X_test[numerical_boolean_features])\n",
    "X_test[\"outlier_score\"] = isolation_forest.decision_function(X_test[numerical_boolean_features])\n",
    "\n",
    "# Show number of outliers\n",
    "n_outliers_train = X_train[\"outlier\"].value_counts()[-1]\n",
    "contamination_train = X_train[\"outlier\"].value_counts()[-1] / X_train[\"outlier\"].value_counts().sum()\n",
    "print(f\"Training Data: Identified {n_outliers_train} rows ({100 * contamination_train:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_val = X_val[\"outlier\"].value_counts()[-1]\n",
    "contamination_val = X_val[\"outlier\"].value_counts()[-1] / X_val[\"outlier\"].value_counts().sum()\n",
    "print(f\"Validation Data: Identified {n_outliers_val} rows ({100 * contamination_val:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_test = X_test[\"outlier\"].value_counts()[-1]\n",
    "contamination_test = X_test[\"outlier\"].value_counts()[-1] / X_test[\"outlier\"].value_counts().sum()\n",
    "print(f\"Test Data: Identified {n_outliers_test} rows ({100 * contamination_test:.1f}%) as multivariate outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7c69a-dd5a-4d3f-9686-92c0f1ded562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to visualize outliers for a subsample of the training data\n",
    "X_train_subsample = X_train[numerical_boolean_features + [\"outlier\"]].sample(n=1000, random_state=42)\n",
    "sns.pairplot(X_train_subsample, hue=\"outlier\", palette={1: \"#4F81BD\", -1: \"#D32F2F\"}, plot_kws={\"alpha\":0.6, \"s\":40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe2890-0ad1-4ef6-8438-373b5d958c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = X_train[X_train[\"outlier\"] == 1]\n",
    "print(f\"Training Data: Removed {X_train[X_train['outlier'] == -1].shape[0]} rows ({X_train[X_train['outlier'] == -1].shape[0] / X_train.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_val_no_outliers = X_val[X_val[\"outlier\"] == 1]\n",
    "print(f\"Validation Data: Removed {X_val[X_val['outlier'] == -1].shape[0]} rows ({X_val[X_val['outlier'] == -1].shape[0] / X_val.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_test_no_outliers = X_test[X_test[\"outlier\"] == 1]\n",
    "print(f\"Test Data: Removed {X_test[X_test['outlier'] == -1].shape[0]} rows ({X_test[X_test['outlier'] == -1].shape[0] / X_test.shape[0] * 100:.1f}%) with multivariate outliers.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c818cb-9883-4a56-9762-53b1cb61a637",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Feature Scaling and Encoding</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "📌 Use a <code>ColumnTransformer</code> to preprocess columns based on their semantic type. This allows the appropriate transformation to each semantic column type in a single step.\n",
    "<ul>\n",
    "  <li>Scale numerical columns: <code>StandardScaler</code> to transform numerical columns to have mean = 0 and standard deviation = 1.</li>\n",
    "  <li>Encode categorical columns:</li>\n",
    "    <ul>\n",
    "      <li>Nominal columns (unordered categories): <code>OneHotEncoder</code> to convert string categories into binary (one-hot) encoded columns.</li>\n",
    "      <li>Ordinal columns (ordered categories): <code>OrdinalEncoder</code> to convert string categories into integers.</li>\n",
    "    </ul>\n",
    "  <li>Retain boolean columns: Pass through boolean columns unchanged using <code>remainder=\"passthrough\"</code>.</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74802f13-c326-403e-ba65-0786a78246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nominal and ordinal columns\n",
    "nominal_columns = [\"house_ownership\"]\n",
    "ordinal_columns = [\"job_stability\", \"city_tier\"]\n",
    "\n",
    "# Define the explicit order of categories for all ordinal columns\n",
    "ordinal_column_orders = [\n",
    "    [\"variable\", \"moderate\", \"stable\", \"very_stable\"],  # Order for job_stability\n",
    "    [\"unknown\", \"tier_3\", \"tier_2\", \"tier_1\"]  # Order for city_tier\n",
    "]\n",
    "\n",
    "# Initialize a column transformer \n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), numerical_columns), \n",
    "        (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit column transformer on the training data \n",
    "column_transformer.fit(X_train)\n",
    "\n",
    "# Apply feature scaling and encoding to training, validation and test data\n",
    "X_train_transformed = column_transformer.transform(X_train)\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "# Get transformed column names\n",
    "nominal_encoded_columns = list(column_transformer.named_transformers_[\"nominal_encoder\"].get_feature_names_out())\n",
    "passthrough_columns = list(X_train.columns.difference(numerical_columns + nominal_columns + ordinal_columns, sort=False))\n",
    "transformed_columns = numerical_columns + nominal_encoded_columns + ordinal_columns + passthrough_columns\n",
    "\n",
    "# Convert transformed data from arrays to DataFrames with column names \n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=transformed_columns)\n",
    "X_val_transformed = pd.DataFrame(X_val_transformed, columns=transformed_columns)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=transformed_columns)\n",
    "\n",
    "# Convert transformed data types\n",
    "X_train_transformed[numerical_columns] = X_train_transformed[numerical_columns].astype(float)\n",
    "X_train_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_train_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "X_val_transformed[numerical_columns] = X_val_transformed[numerical_columns].astype(float)\n",
    "X_val_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_val_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "X_test_transformed[numerical_columns] = X_test_transformed[numerical_columns].astype(float)\n",
    "X_test_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]] = X_test_transformed[nominal_encoded_columns + ordinal_columns + [\"married\", \"car_ownership\"]].astype(int)\n",
    "\n",
    "# Reset the index to match the untransformed DataFrames\n",
    "X_train_transformed.index = X_train_transformed[\"id\"] - 1\n",
    "X_val_transformed.index = X_val_transformed[\"id\"] - 1\n",
    "X_test_transformed.index = X_test_transformed[\"id\"] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c9c2-14c5-4b4a-bad8-f9a2b71f839d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Saving Data</h2>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Save preprocessed data from a Pandas DataFrame to a <code>.csv</code> file in the <code>data</code> directory.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afee6f-836b-4efc-a74a-93e4b8677660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"data\" directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Merge transformed X features and y target variable\n",
    "df_train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "df_val_transformed = pd.concat([X_val_transformed, y_val], axis=1)\n",
    "df_test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "\n",
    "# Save as .csv  \n",
    "df_train_transformed.to_csv(\"data/training_data_preprocessed.csv\", index=False)\n",
    "df_val_transformed.to_csv(\"data/validation_data_preprocessed.csv\", index=False)\n",
    "df_test_transformed.to_csv(\"data/test_data_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dcb62-3ed6-4437-a100-ae5c317edd5e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Exploratory Data Analysis (EDA)</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Merge <code>X_train</code> and <code>y_train</code> to create a single DataFrame with both features and target, containing all partially preprocessed columns (not yet scaled or encoded) for EDA. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213d4b3-1408-40ba-a016-2bf7194e8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train \n",
    "df_train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae0b1c-675f-41b3-89ab-6d4f3cb308f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Univariate EDA</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Analyze the distribution of a single column using descriptive statistics and visualizations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a00fe4-0b5d-4266-94cf-8145c92f3c83",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Examine descriptive statistics (e.g., mean, median, standard deviation) and visualize the distributions (e.g., histograms) of numerical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e14d9-3d91-4860-a5e8-1d536e78716a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br>\n",
    "    📌 Examine descriptive statistics of numerical columns. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfd81f-2d6a-4200-9e00-fa158797e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "descriptives_df = df_train[numerical_columns].describe()\n",
    "\n",
    "# Format for better readability\n",
    "descriptives_df[\"income\"] = descriptives_df[\"income\"].map(lambda x: f\"{x:,.0f}\")  # with thousand separator and no decimals \n",
    "descriptives_df = descriptives_df.transpose()\n",
    "descriptives_df[\"count\"] = descriptives_df[\"count\"].map(lambda x: f\"{x:,.0f}\" if isinstance(x, (int, float)) else x)\n",
    "descriptives_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66f2fc-c59b-44aa-beba-146ee896aef4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    📌 Histogram matrix that shows the distribution of all 6 numerical columns in a 2x3 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb5218-7f14-4612-b73b-af6fab378901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "# Function to visualize distributions of all numerical columns using a histogram matrix\n",
    "def plot_numerical_distributions(df, numerical_columns, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(numerical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Set the figure size based on 4x3 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    \n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all numerical columns\n",
    "    for i, column in enumerate(numerical_columns):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create histogram for the current column\n",
    "        sns.histplot(data=df, x=column, ax=ax)\n",
    "        \n",
    "        # Customize histogram title and axes labels\n",
    "        ax.set_title(column.title().replace(\"_\", \" \"), fontsize=14)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.tick_params(axis=\"both\", labelsize=10)\n",
    "    \n",
    "        # Customize individual plots for better readability\n",
    "        # Format income axis in millions (no decimals)\n",
    "        if column == \"income\":\n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1_000_000:.0f}M\"))   \n",
    "        # Format current job years axis from 0 to 14 in steps of 2\n",
    "        if column == \"current_job_yrs\":\n",
    "            ax.set_xticks(np.arange(0, 15, 2))  \n",
    "        # Format state default rate axis as percentages (no decimals) ranging from 5% to 20% in steps of 5%\n",
    "        if column == \"state_default_rate\":\n",
    "            ax.set_xticks(np.arange(0.05, 0.21, 0.05))  \n",
    "            ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\"))   \n",
    "    \n",
    "    # Hide any unused subplots \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)  \n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical distributions plot (histogram matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical distributions plot (histogram matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical distributions plot (histogram matrix) to file: '{image_path}' already exists.\")\n",
    "            \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize distributions of all numerical columns in the training data\n",
    "plot_numerical_distributions(df_train, numerical_columns, safe_to_file=\"numerical_distributions_histograms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19845aad-bacf-400a-a5cd-b10ccddbb4c7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Customize the histograms of income, current job years, and state default rate for better interpretability.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d4e48-891a-4a97-871b-ba7259370d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Income ---\n",
    "# Imports\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create histogram \n",
    "sns.histplot(df_train[\"income\"])\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"Income\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1000000:.0f}M\"))   # Format x-axis tick labels in millions (no decimals)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe712e-ff7d-4dfe-9cc6-6d91fa9eb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Current job years ---\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Current job years histogram \n",
    "sns.histplot(df_train[\"current_job_yrs\"])\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"Current Job Years\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.xticks(np.arange(0, 15, 1))  # Set x-axis tick labels from 0 to 14 in steps of 1\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd17dd-fc0b-46cb-a7fe-14f62644cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- State default rate ---\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create histogram \n",
    "sns.histplot(df_train[\"state_default_rate\"])\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"State Default Rate\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\"))  # x tick labels as percentages (no decimals)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9af566-4914-4c75-af6d-a929bee130a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Examine descriptive statistics (absolute and relative frequencies) and visualize the distributions (e.g., bar plots) of categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774077bf-c131-4e01-a869-82ae5c7f02dc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br> \n",
    "    📌 Examine frequencies of categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb3385-b3c3-46f5-b661-9d2bfdf0f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create frequency tables for all categorical columns \n",
    "def calculate_frequencies(df, categorical_columns):\n",
    "    # Initialize dictionary to store all frequency tables \n",
    "    frequencies = {}\n",
    "\n",
    "    # Iterate over each categorical column\n",
    "    for column in categorical_columns:\n",
    "        # Calculate frequencies for current column\n",
    "        absolute_frequencies = df[column].value_counts()\n",
    "        relative_frequencies = df[column].value_counts(normalize=True) \n",
    "        percent_frequencies = relative_frequencies.map(lambda x: f\"{x * 100:.2f}%\")  # formatted as string with % sign\n",
    "\n",
    "        # Create frequency table\n",
    "        frequency_table = pd.concat([absolute_frequencies, relative_frequencies, percent_frequencies], axis=1).reset_index()\n",
    "        frequency_table.columns = [\"category\", \"absolute_frequency\", \"relative_frequency\", \"percent_frequency\"] \n",
    "        \n",
    "        # Add current frequency table to dictionary \n",
    "        frequencies[column] = frequency_table\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "\n",
    "# Use function to create frequency tables of all categorical and boolean columns in the training data\n",
    "frequencies = calculate_frequencies(df_train, categorical_columns + boolean_columns)\n",
    "\n",
    "# Display all frequency tables\n",
    "for column, frequency_table in frequencies.items():\n",
    "    print(f\"{column.title().replace('_', ' ')} Frequencies:\")\n",
    "    display(frequency_table[[\"category\", \"absolute_frequency\", \"percent_frequency\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d8d0-b416-423b-8b90-02b8b0fc4a79",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Examine frequency tables of profession, city, and state (high-cardinality columns) separately for better interpretability.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3c41a-f7e7-49a1-8c08-8dc815b3b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profession\n",
    "frequencies[\"profession\"][[\"category\", \"absolute_frequency\", \"percent_frequency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6216576-ad25-4fb3-a8c0-4fd06e1877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City (25 most and 25 least frequent cities out of 317)\n",
    "pd.concat([frequencies[\"city\"].head(25), frequencies[\"city\"].tail(25)])[[\"category\", \"absolute_frequency\", \"percent_frequency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63990c-2ee5-4bbc-9c73-f82353ce403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "frequencies[\"state\"][[\"category\", \"absolute_frequency\", \"percent_frequency\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff349806-af1b-47e3-ab49-3cc4090c8746",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    📌 Bar plot matrix that shows the frequency distribution of all 9 categorical and boolean columns in a 3x3 matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2daa752-5f8a-4802-80af-1f302a20c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "# Function to visualize categorical frequency distributions using a bar plot matrix \n",
    "def plot_categorical_distributions(df, categorical_columns, max_categories=5, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(categorical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Set the figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all categorical columns\n",
    "    for i, column in enumerate(categorical_columns):      \n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Calculate frequencies for the current column\n",
    "        column_frequencies = df[column].value_counts(normalize=True)  # False for absolute frequencies\n",
    "    \n",
    "        # Format plot title\n",
    "        plot_title = column.title().replace(\"_\", \" \")\n",
    "    \n",
    "        # Limit number of categories for better readability  \n",
    "        if len(column_frequencies) > max_categories:\n",
    "            column_frequencies = column_frequencies.head(max_categories)\n",
    "            plot_title += f\" (Top {max_categories})\"\n",
    "\n",
    "        # Customize risk flag labels for better readability\n",
    "        if column == \"risk_flag\":\n",
    "            column_frequencies = column_frequencies.rename({0: \"Non-Defaulter\", 1: \"Defaulter\"})\n",
    "\n",
    "        # Retain inherent order of categories for ordinal columns\n",
    "        if column == \"job_stability\":\n",
    "            column_frequencies = column_frequencies.reindex([\"variable\", \"moderate\", \"stable\", \"very_stable\"])\n",
    "        if column == \"city_tier\":\n",
    "            column_frequencies = column_frequencies.reindex([\"unknown\", \"tier_1\", \"tier_2\", \"tier_3\"])\n",
    "            \n",
    "        # Create bar plot for the current column\n",
    "        sns.barplot(x=column_frequencies.index, y=column_frequencies.values, ax=ax)\n",
    "        \n",
    "        # Customize title and axes labels \n",
    "        ax.set_title(plot_title, fontsize=14)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "        # Customize axes tick labels\n",
    "        xticks = range(len(column_frequencies.index))\n",
    "        xticklabels = [str(label).title().replace(\"_\", \" \") for label in column_frequencies.index]\n",
    "        if len(column_frequencies) >= 5:  # rotate x-tick labels if 5 or more categories \n",
    "            ax.set_xticks(xticks, labels=xticklabels, fontsize=11, rotation=45, ha=\"right\")\n",
    "        else:\n",
    "            ax.set_xticks(xticks, labels=xticklabels, fontsize=11)\n",
    "        ax.tick_params(axis=\"y\", labelsize=10)\n",
    "        ax.set_ylim(0, column_frequencies.max() * 1.1)  # set y-axis from 0 to 10% above maximum frequency\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis as percent\n",
    "\n",
    "        # Create value labels\n",
    "        value_labels = [f\"{value * 100:.1f}%\" for value in column_frequencies.values]\n",
    " \n",
    "        # Customize individual plots for better readability\n",
    "        if column == \"profession\":\n",
    "            # Format profession frequencies as percentages (2 decimals) ranging from 0% to 2.5% in steps of 0.5%\n",
    "            ax.set_yticks(np.arange(0, 0.03, 0.005)) \n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))\n",
    "            value_labels = [f\"{value * 100:.2f}%\" for value in column_frequencies.values]  \n",
    "        if column == \"city\":\n",
    "            # Format city frequencies as percentages (2 decimals) ranging from 0% to 0.6% in steps of 0.1%\n",
    "            ax.set_yticks(np.arange(0, 0.007, 0.001)) \n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))\n",
    "            value_labels = [f\"{value * 100:.2f}%\" for value in column_frequencies.values]\n",
    "\n",
    "        # Add value labels (smaller size if large number of categories)\n",
    "        ax.bar_label(ax.containers[0], labels=value_labels, padding=2, fontsize=10 if len(column_frequencies) <= 6 else 7)  \n",
    "\n",
    "    # Hide any unused subplots \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Categorical distributions plot (bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving categorical distributions plot (bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving categorical distributions plot (bar plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Use function to visualize categorical frequencies of all boolean and categorical columns in training data\n",
    "plot_categorical_distributions(df_train, boolean_columns + categorical_columns, max_categories=8, safe_to_file=\"categorical_frequencies_barplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba37dc1-2770-4eab-abbc-f9dcdcc10321",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Customize the bar plots of profession, city, and state (high-cardinality columns) for better interpretability.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651052f4-b1e2-49b2-b442-086a243d41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Profession ---\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=frequencies[\"profession\"][\"relative_frequency\"], \n",
    "                 y=frequencies[\"profession\"][\"category\"].str.title().str.replace(\"_\", \" \"))\n",
    "\n",
    "# Add value labels (inside bars)\n",
    "for i, value in enumerate(frequencies[\"profession\"][\"relative_frequency\"]):\n",
    "    ax.text(value * 0.98,  # x position (slightly from right end)\n",
    "            i,    # y position\n",
    "            f\"{value * 100:.1f}%\",  # text (frequency with 1 decimal place)\n",
    "            ha=\"right\",  # horizontal alignment\n",
    "            va=\"center\") # vertical alignment\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"Profession\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))  # x tick labels as percentages (1 decimal)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ff497-b343-4121-86ce-18e311da51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- City ---\n",
    "# Select top 50 cities (out of 317) for better readability\n",
    "top_cities = frequencies[\"city\"].nlargest(50, \"relative_frequency\")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=top_cities[\"relative_frequency\"], \n",
    "                 y=top_cities[\"category\"].str.title().str.replace(\"_\", \" \"))\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(top_cities[\"relative_frequency\"]):\n",
    "    # Add dynamic positioning: Value label inside for large bars vs. outside for small bars\n",
    "    threshold = max(top_cities[\"relative_frequency\"]) * 0.9  \n",
    "    if value > threshold:\n",
    "        # Place label inside for large bars\n",
    "        ax.text(value * 0.99, i, f\"{value * 100:.2f}%\", ha=\"right\", va=\"center\")\n",
    "    else:\n",
    "        # Place label outside for small bars\n",
    "        ax.text(value * 1.01, i, f\"{value * 100:.2f}%\", ha=\"left\", va=\"center\")\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"City\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))  # x tick labels as percentages (1 decimal)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a31bc-033e-424d-b1f3-314a0218f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- State ---\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=frequencies[\"state\"][\"relative_frequency\"],\n",
    "                 y=frequencies[\"state\"][\"category\"].str.title().str.replace(\"_\", \" \")) \n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(frequencies[\"state\"][\"relative_frequency\"]):\n",
    "    # Add dynamic positioning: Value label inside for large bars vs. outside for small bars\n",
    "    threshold = max(frequencies[\"state\"][\"relative_frequency\"]) * 0.8  \n",
    "    if value > threshold:\n",
    "        # Place label inside for large bars\n",
    "        ax.text(value * 0.99, i, f\"{value * 100:.1f}%\", ha=\"right\", va=\"center\")\n",
    "    else:\n",
    "        # Place label outside for small bars\n",
    "        ax.text(value * 1.01, i, f\"{value * 100:.1f}%\", ha=\"left\", va=\"center\")\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"State\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))  # x tick labels as percentages (1 decimal)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910582f-4d19-47d1-a4d8-4e40cd9af55d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Bivariate EDA</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Analyze relationships between two columns using correlations and group-wise statistics and visualize relationships using scatter plots and bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a10ff-b9a4-4245-a353-526e855b3d0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical vs. Numerical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Analyze relationships between two numerical columns using correlation coefficients and visualize relationships using scatter plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e58f08-960c-4c26-89a0-723a7823b7b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Correlations</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Correlation heatmap of all numerical and boolean columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f19c9-ae06-4236-9756-a3baa5cc69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot correlation heatmap \n",
    "def plot_correlation_heatmap(df, numerical_columns, safe_to_file=False):\n",
    "    # Create correlation matrix and round to 2 decimals\n",
    "    correlation_matrix = round(df[numerical_columns].corr(), 2) \n",
    "    \n",
    "    # Create upper triangle mask (k=1 excludes diagonal)\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) \n",
    "    \n",
    "    # Set upper triangle to NaN to avoid redundancy\n",
    "    correlation_matrix[mask] = np.nan\n",
    "\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        cmap=\"viridis\",  # Colorblind-friendly colormap (other options: \"cividis\", \"magma\", \"YlOrBr\", \"RdBu\") \n",
    "        annot=True,  # Show correlation values\n",
    "        fmt=\".2f\",  # Ensure uniform decimal formatting\n",
    "        linewidth=0.5  # Thin white lines between cells\n",
    "    )\n",
    "\n",
    "    # Customize title and axes tick labels\n",
    "    plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "    formatted_column_names = correlation_matrix.columns.str.title().str.replace(\"_\", \" \") \n",
    "    ax.set_xticklabels(formatted_column_names)\n",
    "    ax.set_yticklabels(formatted_column_names)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save heatmap to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)  \n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:    \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Correlation heatmap saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving correlation heatmap: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving correlation heatmap: '{image_path}' already exists.\")\n",
    "        \n",
    "    # Show heatmap\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Use function to plot correlation heatmap of all numerical and boolean columns in training data\n",
    "plot_correlation_heatmap(df_train, boolean_columns + numerical_columns, safe_to_file=\"correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e014f-9f2c-48f5-8f3a-2cfb30511f7b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Feature-target correlations by order of magnitude.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3ba7d-e006-42e1-8926-4de5c84e07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-target correlations sorted by absolute values in descending order \n",
    "feature_target_correlations = df_train[numerical_columns + boolean_columns].corr()[\"risk_flag\"].sort_values(key=abs, ascending=False)\n",
    "feature_target_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbbb5f-2676-48e3-a595-7b2c88ef73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize feature-target correlations using a bar plot\n",
    "def plot_feature_target_correlations(feature_target_correlations, y_min=-1, y_max=1, safe_to_file=False):\n",
    "    # Remove target variable self-correlation\n",
    "    feature_target_correlations = feature_target_correlations[1:]\n",
    "    \n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x=feature_target_correlations.index, y=feature_target_correlations.values)  # for pandas Series\n",
    "\n",
    "    # Add horizontal line\n",
    "    ax.axhline(0, color=\"gray\", alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    ax.bar_label(ax.containers[0], fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "    \n",
    "    # Customize title, axes labels and ticks\n",
    "    plt.title(\"Feature-Target Correlations\", fontsize=14)\n",
    "    plt.ylabel(\"Correlation\", fontsize=12)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.yticks(np.arange(y_min, y_max + 0.2, 0.2), fontsize=10)\n",
    "    formatted_xtick_labels = [label.title().replace(\"_\", \" \") for label in feature_target_correlations.index]\n",
    "    plt.xticks(ticks=range(len(formatted_xtick_labels)), labels=formatted_xtick_labels, rotation=45, ha=\"right\", fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Feature-target correlations bar plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving feature-target correlations bar plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving feature-target correlations bar plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Use function to plot feature-target correlations of training data\n",
    "plot_feature_target_correlations(feature_target_correlations, y_min=-0.5, y_max=0.5, safe_to_file=\"feature_target_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6411282-2fa8-4a1c-a4d6-4676fd57d6ef",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    💡 No numerical or boolean feature has a strong linear relationship with the target variable. However, they may have non-linear relationships.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef0d64-6a45-4a88-a6ba-a42f5658ec10",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Numerical-Numerical Relationships (Scatter Plot Matrix)</strong> <br>\n",
    "    📌 Visualize pairwise relationships between all 6 numerical columns using a scatter plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c2ce6-d558-409d-9101-ee622f7ff6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Helper function to customize formatting of specific columns for better readability\n",
    "def customize_formatting(ax, axis, column_name):\n",
    "    # Format income in millions (no decimals)\n",
    "    if column_name == \"income\":\n",
    "        formatter = FuncFormatter(lambda x, _: f\"{x / 1_000_000:.0f}M\")  \n",
    "        if axis == \"x\":\n",
    "            ax.xaxis.set_major_formatter(formatter)\n",
    "        else:\n",
    "            ax.yaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    # Format experience axis from 0 to 20 in steps of 2\n",
    "    elif column_name == \"experience\":\n",
    "        ticks = np.arange(0, 21, 2)  \n",
    "        if axis == \"x\":\n",
    "            ax.set_xticks(ticks)\n",
    "        else:\n",
    "            ax.set_yticks(ticks)\n",
    "\n",
    "    # Format current job years axis from 0 to 14 in steps of 2\n",
    "    elif column_name == \"current_job_yrs\":\n",
    "        ticks = np.arange(0, 15, 2)  \n",
    "        if axis == \"x\":\n",
    "            ax.set_xticks(ticks)\n",
    "        else:\n",
    "            ax.set_yticks(ticks)\n",
    "\n",
    "    # Format current house years axis from 10 to 14 in steps of 1\n",
    "    elif column_name == \"current_house_yrs\":\n",
    "        ticks = np.arange(10, 15, 1)  \n",
    "        if axis == \"x\":\n",
    "            ax.set_xticks(ticks)\n",
    "        else:\n",
    "            ax.set_yticks(ticks)\n",
    "\n",
    "    # Format state default rate as percentages (no decimals) with axis ranging from 5% to 20% in steps of 5%\n",
    "    elif column_name == \"state_default_rate\":\n",
    "        ticks = np.arange(0.05, 0.21, 0.05)  \n",
    "        formatter = FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\")  \n",
    "        if axis == \"x\":\n",
    "            ax.set_xticks(ticks)\n",
    "            ax.xaxis.set_major_formatter(formatter)\n",
    "        else:\n",
    "            ax.set_yticks(ticks)\n",
    "            ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "\n",
    "# Function to visualize pairwise relationships between numerical columns using a scatter plot matrix\n",
    "def plot_numerical_relationships(df, numerical_columns, safe_to_file=False):\n",
    "    # Get all possible pairs of numerical columns\n",
    "    column_pairs = list(itertools.combinations(numerical_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Set the figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over each column pair\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(data=df, x=column_1, y=column_2, ax=ax)\n",
    "        \n",
    "        # Customize title and axes labels\n",
    "        column_1_name = column_1.title().replace(\"_\", \" \")\n",
    "        column_2_name = column_2.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=13)\n",
    "        ax.set_xlabel(column_1_name, fontsize=12)\n",
    "        ax.set_ylabel(column_2_name, fontsize=12)\n",
    "\n",
    "        # Customize formatting of specific columns for better readability\n",
    "        customize_formatting(ax, \"x\", column_1)\n",
    "        customize_formatting(ax, \"y\", column_2) \n",
    "            \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical relationships plot (scatter plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical relationships plot (scatter plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical relationships plot (scatter plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between numerical columns in the training data\n",
    "plot_numerical_relationships(df_train, numerical_columns, safe_to_file=\"numerical_relationships_scatterplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e1f18-21a1-4d66-8578-b576a6b114e2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Customize individual scatter plots for better interpretability.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1e0d-0b8e-424d-b326-cd821c2c2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experience and Current Job Years ---\n",
    "# Set figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot \n",
    "sns.scatterplot(data=df_train, x=\"experience\", y=\"current_job_yrs\")\n",
    "\n",
    "# Customize title and axis labels\n",
    "plt.title(\"Years of Work Experience vs. Years in Current Job\", fontsize=14)\n",
    "plt.xlabel(\"Years of Work Experience\", fontsize=12)\n",
    "plt.ylabel(\"Years in Current Job\", fontsize=12)\n",
    "\n",
    "# Axes tick labels in 1-year steps\n",
    "plt.xticks(range(0, 21, 1), fontsize=10)\n",
    "plt.yticks(range(0, 15, 1), fontsize=10)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dee6f-bad0-4f41-9428-15b1aa7c5022",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical vs. Categorical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Analyze relationships between a numerical column and a categorical column using group-wise statistics (e.g., median or mean by category) and visualize relationships using bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0e9de-db02-41d1-b482-0ba3f45baa1b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Group-Wise Statistics</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Descriptive statistics of all 6 numerical columns grouped by risk flag.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4c477-c154-4157-a2e4-a7dd7c21b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics of all numerical columns grouped by risk flag (focus on median, mean, and std for easier readability)\n",
    "stats_by_riskflag = df_train[numerical_columns].groupby(df_train[\"risk_flag\"]).agg([\"median\", \"mean\", \"std\"])\n",
    "\n",
    "# Format table for better readability\n",
    "stats_by_riskflag = stats_by_riskflag.rename(index={0: \"Non-Defaulter\", 1: \"Defaulter\"})\n",
    "for statistic in [\"median\", \"mean\", \"std\"]:\n",
    "    stats_by_riskflag[(\"income\", statistic)] = stats_by_riskflag[(\"income\", statistic)].map(lambda x: f\"{x:,.0f}\")\n",
    "    stats_by_riskflag[(\"age\", statistic)] = stats_by_riskflag[(\"age\", statistic)].map(lambda x: f\"{x:.1f}\")\n",
    "    stats_by_riskflag[(\"experience\", statistic)] = stats_by_riskflag[(\"experience\", statistic)].map(lambda x: f\"{x:.1f}\")\n",
    "    stats_by_riskflag[(\"current_job_yrs\", statistic)] = stats_by_riskflag[(\"current_job_yrs\", statistic)].map(lambda x: f\"{x:.1f}\")\n",
    "    stats_by_riskflag[(\"current_house_yrs\", statistic)] = stats_by_riskflag[(\"current_house_yrs\", statistic)].map(lambda x: f\"{x:.1f}\")\n",
    "    stats_by_riskflag[(\"state_default_rate\", statistic)] = stats_by_riskflag[(\"state_default_rate\", statistic)].map(lambda x: f\"{x * 100:.1f}%\")\n",
    "\n",
    "# Show table of group-wise statistics\n",
    "stats_by_riskflag.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fcd3b-d66d-45c2-8228-249df8d71c7d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Group-wise statistics of income by job stabilty.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2ce84-0a88-413b-9672-78ef43636b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics of income grouped by job stabilty\n",
    "income_by_jobstability = df_train[\"income\"].groupby(df_train[\"job_stability\"]).describe()\n",
    "\n",
    "# Format table for better readability\n",
    "for statistic in income_by_jobstability.columns:\n",
    "    income_by_jobstability[statistic] = income_by_jobstability[statistic].map(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "# Show table of group-wise statistics\n",
    "income_by_jobstability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b233-b8d1-462e-a6f7-79602dfa2631",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Effect Size</strong> <br>\n",
    "    📌 Quantify the magnitude of the difference between two groups using Cohen's d.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6469094-f8ab-48f4-8528-f0c178336f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cohens_d(df, numerical_column, categorical_column, group1, group2):\n",
    "    x1 = df[df[categorical_column] == group1][numerical_column]\n",
    "    x2 = df[df[categorical_column] == group2][numerical_column]\n",
    "    \n",
    "    mean1, mean2 = np.mean(x1), np.mean(x2)\n",
    "    std1, std2 = np.std(x1, ddof=1), np.std(x2, ddof=1)  # Sample standard deviation using N−1\n",
    "    n1, n2 = len(x1), len(x2)\n",
    "    \n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    return (mean1 - mean2) / pooled_std if pooled_std != 0 else 0\n",
    "\n",
    "# Use function to calculate Cohen's d for all numerical columns\n",
    "cohens_d_results = {column: calculate_cohens_d(df_train, column, \"risk_flag\", 1, 0) for column in numerical_columns}\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "cohens_d_df = pd.DataFrame.from_dict(cohens_d_results, orient=\"index\", columns=[\"Cohen's d\"])\n",
    "cohens_d_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dc890-3977-4a33-9339-2c35e7abbf59",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> 💡 The difference between defaulters and non-defaulters is very small across all 6 numerical features, indicating that the numerical features provide very little separation between the two classes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d83310-bdc2-4eb6-aa25-9a1b837d79fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Numerical-Categorical Relationships (Bar Plot Matrix)</strong> <br>\n",
    "    📌 Visualize pairwise relationships between all 6 numerical columns and risk flag (categorical) using a bar plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a3195-ddf3-47db-adfe-9207c4e62a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize pairwise relationships between multiple numerical columns and a single categorical column using a bar plot matrix\n",
    "def plot_numerical_categorical_relationships(df, numerical_columns, categorical_column, estimator=np.median, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(numerical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Set the figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all numerical columns\n",
    "    for i, numerical_column in enumerate(numerical_columns):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create bar plot \n",
    "        sns.barplot(data=df, x=categorical_column, y=numerical_column, estimator=estimator, errorbar=None, ax=ax)\n",
    "\n",
    "        # Add value labels \n",
    "        for bar in ax.patches:  # patches are the bars themselves\n",
    "            # Get the height of the bar (which is the value)\n",
    "            height = bar.get_height()\n",
    "            \n",
    "            # Format value labels            \n",
    "            # Format income values in millions (no decimals)\n",
    "            if numerical_column == \"income\":    \n",
    "                value = f\"{height:,.0f}M\"\n",
    "            # Format state default rate values as percentages (1 decimal) \n",
    "            elif numerical_column == \"state_default_rate\":\n",
    "                value = f\"{height * 100:.1f}%\"\n",
    "            # Format all other values with 1 decimal\n",
    "            else:\n",
    "                value = f\"{height:.1f}\"\n",
    "                \n",
    "            # Get the x and y coordinates for the text label\n",
    "            x_pos = bar.get_x() + bar.get_width() / 2.\n",
    "            y_pos = height * 1.01\n",
    "\n",
    "            # Add the text label \n",
    "            ax.text(x_pos, y_pos, value, ha=\"center\", va=\"bottom\", fontsize=10) \n",
    "\n",
    "        # Extend the y-axis upper limit by 5% to make room for value labels\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.set_ylim(y_min, y_max * 1.05)\n",
    "        \n",
    "        # Customize title, axes labels and ticks\n",
    "        numerical_column_name = numerical_column.title().replace(\"_\", \" \")\n",
    "        categorical_column_name = categorical_column.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{estimator.__name__.title()} {numerical_column_name} by {categorical_column_name}\", fontsize=13)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(numerical_column_name, fontsize=12)\n",
    "        ax.set_xticks([0, 1], labels=[\"Non-Defaulter\", \"Defaulter\"], fontsize=11)\n",
    "\n",
    "        # Customize individual columns for better readability\n",
    "        # Format income axis in millions (no decimals)\n",
    "        if numerical_column == \"income\":\n",
    "            ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1_000_000:.0f}M\"))   \n",
    "        # Format state default rate axis as percentages (no decimals) ranging from 5% to 20% in steps of 5%\n",
    "        if numerical_column == \"state_default_rate\":\n",
    "            ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x * 100:.0f}%\"))  \n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical-categorical relationships plot (bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical-categorical relationships plot (bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical-categorical relationships plot (bar plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between all numerical columns and risk flag in the training data\n",
    "plot_numerical_categorical_relationships(df_train, numerical_columns, categorical_column=\"risk_flag\", safe_to_file=\"numerical_categorical_relationships_barplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec30c4-a3ce-43dd-afe5-d7b5c300541c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Individual bar plot to visualize relationship between income and job stability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75edd9-7e0d-492b-a371-30c510a31807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct order of categories for job stability\n",
    "job_stability_categories = [\"variable\", \"moderate\", \"stable\", \"very_stable\"]\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Create bar plot \n",
    "sns.barplot(data=df_train, x=\"job_stability\", order=job_stability_categories, y=\"income\", estimator=np.median, errorbar=None, ax=ax)\n",
    "\n",
    "# Add value labels \n",
    "for bar in ax.patches:  \n",
    "    height = bar.get_height()\n",
    "    value = f\"{height:,.0f}M\"  # format in millions with thousand separator (no decimals)\n",
    "    x_pos = bar.get_x() + bar.get_width() / 2.\n",
    "    y_pos = height * 1.01\n",
    "    ax.text(x_pos, y_pos, value, ha=\"center\", va=\"bottom\", fontsize=10) \n",
    "\n",
    "# Extend the y-axis upper limit by 5% \n",
    "y_min, y_max = ax.get_ylim()\n",
    "ax.set_ylim(y_min, y_max * 1.05)\n",
    "\n",
    "# Customize title and axes labels\n",
    "numerical_column_name = \"income\".title().replace(\"_\", \" \")\n",
    "categorical_column_name = \"job_stability\".title().replace(\"_\", \" \")\n",
    "ax.set_title(f\"Median {numerical_column_name} by {categorical_column_name}\", fontsize=14)\n",
    "ax.set_xlabel(categorical_column_name, fontsize=12)\n",
    "ax.set_ylabel(numerical_column_name, fontsize=12)\n",
    "\n",
    "# Customize axes tick labels\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1_000_000:.0f}M\"))  # format income in millions (no decimals)\n",
    "ax.tick_params(axis=\"y\", labelsize=10)\n",
    "ax.set_xticks(range(len(job_stability_categories)))\n",
    "ax.set_xticklabels([label.title().replace(\"_\", \" \") for label in job_stability_categories], fontsize=10) \n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc8dab-ec4b-4f11-8449-6773e99e3392",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical vs. Categorical</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Analyze relationships between two categorical columns using contingency tables and visualize relationships using grouped bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1eed5-f837-4289-8ede-854be3b58297",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Contingency Tables</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">📌 Contingency tables between all categorical features and the categorical target variable.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c5c76-6246-4aa0-b9db-443e717b00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all categorical and boolean columns with number of unique categories\n",
    "df_train[categorical_columns + boolean_columns].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a81365-682c-400c-a935-2592c7f6804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create contingency tables between each categorical feature and a single categorical target variable \n",
    "def calculate_feature_target_crosstabs(df, categorical_columns, target_variable, normalize=\"index\"):\n",
    "    # Dictionary to store contingency tables\n",
    "    contingency_tables = {}\n",
    "\n",
    "    # Customize individual columns' order of categories\n",
    "    df[\"job_stability\"] = pd.Categorical(df[\"job_stability\"], categories=[\"variable\", \"moderate\", \"stable\", \"very_stable\"], ordered=True)\n",
    "    df[\"city_tier\"] = pd.Categorical(df[\"city_tier\"], categories=[\"unknown\", \"tier_1\", \"tier_2\", \"tier_3\"], ordered=True)\n",
    "    \n",
    "    # Get all possible pairs of the target variable with the categorical features\n",
    "    feature_target_pairs = [(feature, target_variable) for feature in categorical_columns if feature != target_variable]\n",
    "    \n",
    "    # Iterate over each feature-target pair\n",
    "    for feature, target in feature_target_pairs:\n",
    "        # Create contingency table (normalize=\"index\" gives percentage distribution of the target variable within each category of the feature)\n",
    "        contingency_tables[(feature, target)] = pd.crosstab(df[feature], df[target], normalize=normalize)\n",
    "\n",
    "    return contingency_tables\n",
    "\n",
    "\n",
    "# Use function to create contingency tables between risk flag and each categorical or boolean feature in the training data\n",
    "contingency_tables = calculate_feature_target_crosstabs(df_train, categorical_columns + boolean_columns, target_variable=\"risk_flag\")\n",
    "\n",
    "# Display a single feature-target contingency table (formatted as percent with 1 decimal)\n",
    "display(contingency_tables[(\"married\", \"risk_flag\")].map(lambda x: f\"{x * 100:.1f}\"))\n",
    "\n",
    "# Display all feature-target contingency tables \n",
    "for (feature, target) in contingency_tables.keys():\n",
    "    display(contingency_tables[(feature, target)].map(lambda x: f\"{x * 100:.1f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef5a25-3f30-4703-987f-2e8ede32b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create heatmap of a contingency table\n",
    "def plot_crosstab_heatmap(contingency_table, safe_to_file=False):\n",
    "    # Get number of categories\n",
    "    n_categories_feature = contingency_table.index.nunique()\n",
    "    n_categories_target = contingency_table.columns.nunique()\n",
    "    \n",
    "    # Set the figure size based on number of categories\n",
    "    if n_categories_feature > 4:\n",
    "        plt.figure(figsize=(n_categories_target * 1.5, n_categories_feature * 0.4))\n",
    "    else:\n",
    "        plt.figure(figsize=(n_categories_target * 2, n_categories_feature * 0.8))\n",
    "\n",
    "    # Create heatmap (formatted as percent with 1 decimal)\n",
    "    ax = sns.heatmap(contingency_table, annot=True, fmt=\".1%\", cmap=\"viridis\", cbar=False, linewidth=0.5)\n",
    "\n",
    "    # Customize title and axes labels\n",
    "    feature_name = contingency_table.index.name.title().replace(\"_\", \" \")\n",
    "    target_name = contingency_table.columns.name.title().replace(\"_\", \" \")\n",
    "    ax.set_title(f\"{target_name} Distribution Within {feature_name}\", fontsize=14)\n",
    "    ax.set_xlabel(target_name, fontsize=12) \n",
    "    ax.set_ylabel(feature_name, fontsize=12) \n",
    "\n",
    "    # Customize axes tick labels\n",
    "    ax.set_xticklabels([\"Non-Defaulter\", \"Defaulter\"], fontsize=10)\n",
    "    formatted_yticklabels = [label.get_text().title().replace(\"_\", \" \") for label in ax.get_yticklabels()]\n",
    "    ax.set_yticklabels(formatted_yticklabels, rotation=0, fontsize=10)\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Contingency table heatmap saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving contingency table heatmap: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving contingency table heatmap to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show heatmap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to create heatmaps of all feature-target contingency tables \n",
    "for (feature, target) in contingency_tables.keys():\n",
    "    plot_crosstab_heatmap(contingency_tables[feature, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c2e81-8e45-448d-ba5e-1cd5c57d5933",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Customize individual contingency table heatmaps for better interpretability. Display only the feature categories with the 5 highest and 5 lowest default rates for features with high cardinality (profession, city, state).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765a2d-0145-4f65-8c6f-2b7486f09653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in [\"profession\", \"city\", \"state\"]:\n",
    "    # Create contingency table with percentage distribution of risk flag within each category of the feature \n",
    "    contingency_table = pd.crosstab(df_train[feature], df_train[\"risk_flag\"], normalize=\"index\")\n",
    "    \n",
    "    # Sort feature categories from highest to lowest default rate (class 1)\n",
    "    contingency_table = contingency_table.sort_values(by=1, ascending=False)\n",
    "    \n",
    "    # Filter feature categories with 5 highest and 5 lowest default rates\n",
    "    contingency_table = pd.concat([contingency_table.head(5), contingency_table.tail(5)])\n",
    "    \n",
    "    # Create heatmap of contingency table \n",
    "    plot_crosstab_heatmap(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebc102-6922-4a18-9c56-23e37ebfe423",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Categorical-Categorical Relationships (Grouped Bar Plot Matrix)</strong> <br>\n",
    "    📌 Visualize pairwise relationships between all 6 categorical columns with low cardinality using a grouped bar plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635f3b4-0271-4fbc-9dd5-d2b3be7398c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize pairwise relationships between all categorical columns using a grouped bar plot matrix\n",
    "def plot_categorical_relationships(df, categorical_columns, max_categories=5, safe_to_file=False):\n",
    "    # Filter columns with low cardinality (small number of unique categories) \n",
    "    low_cardinality_columns = [column for column in categorical_columns if df[column].nunique() <= max_categories]\n",
    "\n",
    "    # Get all possible pairs of categorical columns\n",
    "    column_pairs = tuple(itertools.combinations(low_cardinality_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Set the figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over each column pair\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Ensure column_1 (plot x-axis) has higher cardinality (more categories) for plot readability\n",
    "        if df[column_1].nunique() < df[column_2].nunique():\n",
    "            column_1, column_2 = column_2, column_1\n",
    "        \n",
    "        # Create contingency table (normalize=\"index\" calculates the percentage distribution of column_2 within each category of column_1)\n",
    "        contingency_table = pd.crosstab(df[column_1], df[column_2], normalize=\"index\") \n",
    "\n",
    "        # Reshape data for easier plotting  \n",
    "        plot_df = contingency_table.stack().reset_index()\n",
    "        plot_df.columns = [column_1, column_2, \"Frequency\"]\n",
    "\n",
    "        # Customize individual columns' category order   \n",
    "        order = None\n",
    "        hue_order = None\n",
    "        if column_1 == \"job_stability\":\n",
    "            order = [\"variable\", \"moderate\", \"stable\", \"very_stable\"]\n",
    "        if column_2 == \"job_stability\":\n",
    "            hue_order = [\"variable\", \"moderate\", \"stable\", \"very_stable\"]\n",
    "        if column_1 == \"city_tier\":\n",
    "            order = [\"unknown\", \"tier_1\", \"tier_2\", \"tier_3\"]\n",
    "        if column_2 == \"city_tier\":\n",
    "            hue_order = [\"unknown\", \"tier_1\", \"tier_2\", \"tier_3\"]\n",
    "            \n",
    "        # Create grouped bar plot\n",
    "        sns.barplot(data=plot_df, x=column_1, order=order, y=\"Frequency\", hue=column_2, hue_order=hue_order, palette=\"viridis\", ax=ax)\n",
    "\n",
    "        # Add value labels\n",
    "        n_categories_col2 = plot_df[column_2].nunique()\n",
    "        value_label_size = {1: 10, 2: 10, 3: 8}.get(n_categories_col2, 7)  # dynamic fontsize based on number of categories (default fontsize 7)\n",
    "        for container in ax.containers:\n",
    "            value_labels = [f\"{value * 100:.0f}%\" for value in container.datavalues]\n",
    "            ax.bar_label(container, labels=value_labels, padding=2, fontsize=value_label_size) \n",
    "                    \n",
    "        # Extend the y-axis upper limit by 5% \n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.set_ylim(y_min, y_max * 1.05)\n",
    "        \n",
    "        # Customize title and axes labels\n",
    "        column_1_name = column_1.title().replace(\"_\", \" \")\n",
    "        column_2_name = column_2.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=14)\n",
    "        ax.set_xlabel(column_1_name, fontsize=12)\n",
    "        ax.set_ylabel(f\"% within {column_1_name}\", fontsize=12)\n",
    "        \n",
    "        # Customize axes ticks\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis tick labels as percentages\n",
    "        if column_1 == \"risk_flag\":\n",
    "            ax.set_xticks([0, 1], labels=[\"Non-Defaulter\", \"Defaulter\"], fontsize=10)\n",
    "        else:\n",
    "            xticks = range(plot_df[column_1].nunique())\n",
    "            xticklabels = [label.get_text().title().replace(\"_\", \" \") for label in ax.get_xticklabels()]\n",
    "            ax.set_xticks(xticks, labels=xticklabels, fontsize=10)\n",
    "\n",
    "        # Customize legend\n",
    "        legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "        if column_2 == \"risk_flag\":\n",
    "            legend_labels = [{\"0\": \"Non-Defaulter\", \"1\": \"Defaulter\"}.get(label) for label in legend_labels]\n",
    "        else:\n",
    "            legend_labels = [str(label).title().replace(\"_\", \" \") for label in legend_labels]  \n",
    "        ax.legend(handles=legend_handles, labels=legend_labels, title=column_2_name, \n",
    "                  loc=\"center right\", bbox_to_anchor=(1, 0.65))  # legend position x=1 (right edge), y=0.65 (slightly above vertical center)  \n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Categorical relationships plot (grouped bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving categorical relationships plot (grouped bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving categorical relationships plot (grouped bar plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between all categorical columns in the training data\n",
    "plot_categorical_relationships(df_train, categorical_columns + boolean_columns, safe_to_file=\"categorical_relationships_groupedbarplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2d6b-7f71-4c41-a18d-7554ac773c45",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Individual grouped bar plots for high-cardinality features profession, city, and state. Display only the categories with the 5 highest and 5 lowest default rates. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84a430-59a8-45a6-9c3f-cdd2f9d758d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (column_1, column_2) in [(\"profession\", \"risk_flag\"), (\"city\", \"risk_flag\"), (\"state\", \"risk_flag\")]:\n",
    "    # Create contingency table (normalize=\"index\" calculates the percentage distribution of column_2 within each category of column_1)\n",
    "    contingency_table = pd.crosstab(df[column_1], df[column_2], normalize=\"index\") \n",
    "    \n",
    "    # Sort categories from highest to lowest (by class 1 \"defaulter\")\n",
    "    contingency_table = contingency_table.sort_values(by=1, ascending=False)\n",
    "    \n",
    "    # Filter 5 highest and 5 lowest categories\n",
    "    contingency_table = pd.concat([contingency_table.head(5), contingency_table.tail(5)])\n",
    "    \n",
    "    # Reshape data for easier plotting  \n",
    "    plot_df = contingency_table.stack().reset_index()\n",
    "    plot_df.columns = [column_1, column_2, \"Frequency\"]\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    sns.barplot(data=plot_df, x=column_1, y=\"Frequency\", hue=column_2, palette=\"viridis\", ax=ax)\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        value_labels = [f\"{value * 100:.1f}%\" for value in container.datavalues]\n",
    "        ax.bar_label(container, labels=value_labels, padding=2, fontsize=10) \n",
    "                \n",
    "    # Extend the y-axis upper limit by 5% \n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.set_ylim(y_min, y_max * 1.05)\n",
    "    \n",
    "    # Customize title and axes labels\n",
    "    column_1_name = column_1.title().replace(\"_\", \" \")\n",
    "    column_2_name = column_2.title().replace(\"_\", \" \")\n",
    "    ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=14)\n",
    "    ax.set_xlabel(column_1_name, fontsize=12)\n",
    "    ax.set_ylabel(f\"% Within {column_1_name}\", fontsize=12)\n",
    "    \n",
    "    # Customize axes ticks\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis tick labels as percentages\n",
    "    column_1_categories = contingency_table.index.tolist()  # Get the specific order of categories\n",
    "    ax.set_xticks(range(len(column_1_categories)))\n",
    "    ax.set_xticklabels([label.title().replace(\"_\", \" \") for label in column_1_categories], rotation=45, ha=\"right\")  \n",
    "\n",
    "    # Customize legend\n",
    "    legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "    legend_labels = [{\"0\": \"Non-Defaulter\", \"1\": \"Defaulter\"}.get(label) for label in legend_labels]\n",
    "    ax.legend(handles=legend_handles, labels=legend_labels, title=column_2_name)    \n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b521a-4a4b-40e6-a561-3dbd0d59d458",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Modeling</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Helper functions to save and load models using <code>pickle</code>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641d606-cb48-4ef2-8497-14f782c9c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save model as .pkl file\n",
    "def save_model(model, filename):\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    # Save model as .pkl file \n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "        print(f\"Model saved successfully under 'models/{filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "\n",
    "\n",
    "# Function to load model from .pkl file \n",
    "def load_model(filename):\n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"rb\") as file:  # ensure model is stored in \"models\" directory\n",
    "            model = pickle.load(file)\n",
    "        print(f\"{filename} loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59829c-68d0-44b3-8d4d-137de88a5251",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Baseline Models</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Train 8 baseline models (default hyperparameter values):  \n",
    "    <ul>\n",
    "        <li>Logistic Regression</li>\n",
    "        <li>Elastic Net Logistic Regression</li>\n",
    "        <li>K-Nearest Neighbors Classifier</li>\n",
    "        <li>Support Vector Classifier</li>\n",
    "        <li>Decision Tree Classifier</li>\n",
    "        <li>Random Forest Classifier</li>\n",
    "        <li>Multi-Layer Perceptron Classifier</li>\n",
    "        <li>XGBoost Classifier</li>\n",
    "    </ul>\n",
    "    Train each model with 4 outlier handling methods:\n",
    "    <ul>\n",
    "        <li>3SD Method</li>\n",
    "        <li>1.5 IQR Method</li>\n",
    "        <li>Isolation Forest</li>\n",
    "        <li>No Outlier Handling</li>\n",
    "    </ul>\n",
    "    🎯 Evaluate model performance:  \n",
    "    <ul>\n",
    "        <li>Primary Metric:\n",
    "            <ul>\n",
    "                <li>Area Under the Precision-Recall Curve (AUC-PR)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Secondary Metrics:\n",
    "            <ul>\n",
    "                <li>Recall (Class 1)</li>\n",
    "                <li>Precision (Class 1)</li>\n",
    "                <li>F1-score (Class 1)</li>\n",
    "            </ul>     \n",
    "        </li>\n",
    "        <li>Additional Diagnostics:\n",
    "            <ul>\n",
    "                <li>Metrics Comparison Plots and Tables</li>\n",
    "                <li>Precision-Recall Curves</li>\n",
    "                <li>Classification Report</li>\n",
    "                <li>Confusion Matrix</li>                \n",
    "                <li>Overfitting</li>\n",
    "                <li>Feature Misclassification Analysis</li> \n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135f7f7-1d36-49b6-90a9-c9035074c06d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Training</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Train each baseline model with each outlier handler and store fitted models, predicted values, and evaluation metrics in a results dictionary.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf6996-3d74-4fc6-9a1f-33440c6f55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to be used for model training\n",
    "columns_to_keep = [\"income\", \"age\", \"experience\", \"current_job_yrs\", \"current_house_yrs\", \"state_default_rate\", \n",
    "                   \"house_ownership_owned\", \"house_ownership_rented\", \"job_stability\", \"city_tier\", \"married\", \"car_ownership\"]\n",
    "X_train_transformed = X_train_transformed[columns_to_keep].copy()\n",
    "X_val_transformed = X_val_transformed[columns_to_keep].copy()\n",
    "X_test_transformed = X_test_transformed[columns_to_keep].copy()\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Elastic Net\": LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True, max_iter=1000),\n",
    "    \"Neural Network\": MLPClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define outlier handlers\n",
    "outlier_handlers = {\n",
    "    \"No Outlier Handling\": None,\n",
    "    \"3SD Method\": OutlierRemover3SD(),\n",
    "    \"1.5 IQR Method\": OutlierRemoverIQR(),\n",
    "    \"Isolation Forest\": IsolationForest(contamination=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "# Function to train and evaluate a single model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Fit model on the training data and measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()    \n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate evaluate metrics\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "    auc_pr = auc(recall_curve, precision_curve)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred)\n",
    "\n",
    "    # Return fitted model, predicted values, and evaluation metrics\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"training_time\": end_time - start_time,\n",
    "        \"y_val_pred\": y_val_pred,\n",
    "        \"y_val_proba\": y_val_proba,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Class 1)\": precision[1],\n",
    "        \"Recall (Class 1)\": recall[1],\n",
    "        \"F1-Score (Class 1)\": f1[1]\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# tree = evaluate_model(baseline_models[\"Decision Tree\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "# knn = evaluate_model(baseline_models[\"K-Nearest Neighbors\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "\n",
    "# Function to train and evaluate all models \n",
    "def evaluate_all_models(models, X_train, y_train, X_val, y_val):\n",
    "    results = {}   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        result = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "        results[model_name] = result\n",
    "        print(f\"Training Time: {round(result['training_time'], 1)} sec\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Use function to train all baseline models\n",
    "# baseline_model_results = evaluate_all_models(baseline_models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "\n",
    "# Function to train and evaluate all models with all outlier handling methods\n",
    "def evaluate_all_models_and_outlier_handlers(models, outlier_handlers, X_train, y_train, X_val, y_val, numerical_columns):\n",
    "    results = {}\n",
    "    for outlier_handler_name, outlier_handler in outlier_handlers.items():\n",
    "        # Show current outlier handler\n",
    "        print(f\"\\nOutlier Handling: {outlier_handler_name}...\")\n",
    "    \n",
    "        # Initialize sub-dictionary for current outlier handler\n",
    "        results[outlier_handler_name] = {}\n",
    "    \n",
    "        # Handle outliers depending on method\n",
    "        if outlier_handler_name in [\"3SD Method\", \"1.5 IQR Method\"]:\n",
    "            # Identify and remove outliers on training data\n",
    "            X_train_no_outliers = outlier_handler.fit_transform(X_train, numerical_columns)\n",
    "            y_train_no_outliers = y_train.loc[outlier_handler.final_mask_]\n",
    "            n_outliers_train = (~outlier_handler.final_mask_).sum()\n",
    "            pct_outliers_train = n_outliers_train / len(outlier_handler.final_mask_) * 100\n",
    "            print(f\"Training Data: Removed {n_outliers_train} rows ({pct_outliers_train:.1f}%) with outliers.\")\n",
    "    \n",
    "            # Identify and remove outliers on validation data\n",
    "            X_val_no_outliers = outlier_handler.transform(X_val)\n",
    "            y_val_no_outliers = y_val.loc[outlier_handler.final_mask_]\n",
    "            n_outliers_val = (~outlier_handler.final_mask_).sum()\n",
    "            pct_outliers_val = n_outliers_val / len(outlier_handler.final_mask_) * 100\n",
    "            print(f\"Validation Data: Removed {n_outliers_val} rows ({pct_outliers_val:.1f}%) with outliers.\")\n",
    "        \n",
    "        elif outlier_handler_name == \"Isolation Forest\":\n",
    "            # Fit isolation forest on training data\n",
    "            outlier_handler.fit(X_train[numerical_columns])\n",
    "            \n",
    "            # Identify and remove outliers on training data\n",
    "            outlier_train = outlier_handler.predict(X_train[numerical_columns])\n",
    "            X_train_no_outliers = X_train[outlier_train == 1].copy()\n",
    "            y_train_no_outliers = y_train.iloc[outlier_train == 1].copy()\n",
    "            n_outliers_train = (outlier_train == -1).sum()\n",
    "            pct_outliers_train = n_outliers_train / len(outlier_train) * 100\n",
    "            print(f\"Training Data: Removed {n_outliers_train} rows ({pct_outliers_train:.1f}%) as multivariate outliers.\")\n",
    "            \n",
    "            # Identify and remove outliers on validation data\n",
    "            outlier_val = outlier_handler.predict(X_val[numerical_columns])\n",
    "            X_val_no_outliers = X_val[outlier_val == 1].copy()\n",
    "            y_val_no_outliers = y_val.iloc[outlier_val == 1].copy()\n",
    "            n_outliers_val = (outlier_val == -1).sum()\n",
    "            pct_outliers_val = n_outliers_val / len(outlier_val) * 100\n",
    "            print(f\"Validation Data: Removed {n_outliers_val} rows ({pct_outliers_val:.1f}%) as multivariate outliers.\")    \n",
    "    \n",
    "        else:\n",
    "            # No outlier handling\n",
    "            X_train_no_outliers = X_train.copy()\n",
    "            y_train_no_outliers = y_train.copy()\n",
    "            X_val_no_outliers = X_val.copy()\n",
    "            y_val_no_outliers = y_val.copy()\n",
    "    \n",
    "        # Train all models with current outlier handler\n",
    "        results[outlier_handler_name] = evaluate_all_models(models, X_train_no_outliers, y_train_no_outliers, X_val_no_outliers, y_val_no_outliers)\n",
    "\n",
    "    return results\n",
    "\n",
    "    \n",
    "# Use function to train 8 baseline models with 4 outlier handling methods\n",
    "# baseline_model_results = evaluate_all_models_and_outlier_handlers(baseline_models, outlier_handlers, X_train_transformed, y_train, X_val_transformed, y_val, numerical_columns)\n",
    "\n",
    "# Save baseline model results as .pkl file (using helper function)  \n",
    "# save_model(baseline_model_results, \"baseline_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e193f6a-60e2-4c1a-acd8-65f8ab015cfb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Outlier Handler Comparison</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Comparison Plots</strong> <br>\n",
    "    📌 Compare evaluation metrics of outlier handling methods on the validation data using grouped bar plots.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1695345-dde9-4c91-b2b5-9c2ff825cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model results (using helper function)\n",
    "baseline_model_results = load_model(\"baseline_models.pkl\")\n",
    "\n",
    "\n",
    "# Function to compare a single evaluation metric by model and outlier handling method with a grouped bar chart\n",
    "def model_comparison_plot(metric, model_results, return_df=False, safe_to_file=False):\n",
    "    # Get iterable model names from model results dictionary \n",
    "    models = model_results[\"No Outlier Handling\"].keys()\n",
    "\n",
    "    # Create DataFrame for grouped bar chart\n",
    "    metric_df = pd.DataFrame({\n",
    "        \"Model\": models,\n",
    "        \"No Outlier Handling\": [model_results[\"No Outlier Handling\"][model][metric] for model in models],\n",
    "        \"3SD Method\": [model_results[\"3SD Method\"][model][metric] for model in models],\n",
    "        \"1.5 IQR Method\": [model_results[\"1.5 IQR Method\"][model][metric] for model in models],\n",
    "        \"Isolation Forest\": [model_results[\"Isolation Forest\"][model][metric] for model in models]\n",
    "    })\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    metric_df = pd.melt(\n",
    "        metric_df,\n",
    "        id_vars=[\"Model\"], \n",
    "        value_vars=[\"No Outlier Handling\", \"3SD Method\", \"1.5 IQR Method\", \"Isolation Forest\"],\n",
    "        var_name=\"Outlier Handling\", \n",
    "        value_name=metric\n",
    "    )\n",
    "    \n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(x=\"Model\", y=metric, hue=\"Outlier Handling\", data=metric_df, palette=\"viridis\", ax=ax)\n",
    "\n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        value_labels = [round(value_label, 2) for value_label in container.datavalues]\n",
    "        ax.bar_label(container, labels=value_labels, fontsize=8, padding=2)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(f\"{metric} Comparison by Baseline Model and Outlier Handling Method\", fontsize=16, pad=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(f\"{metric}\", fontsize=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12, rotation=45)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    \n",
    "    # Adjust layout\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        metric_prefix = {\n",
    "            \"AUC-PR\": \"aucpr\",\n",
    "            \"Precision (Class 1)\": \"precision_c1\",\n",
    "            \"Recall (Class 1)\": \"recall_c1\",\n",
    "            \"F1-Score (Class 1)\": \"f1_c1\",\n",
    "            \"Accuracy\": \"accuracy\"\n",
    "        }\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\") \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Model comparison plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model comparison plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving model comparison plot to file: '{image_path}' already exists.\")\n",
    "        \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    if return_df:\n",
    "        return metric_df\n",
    "\n",
    "\n",
    "# Use function to compare metrics by baseline model and outlier handling method\n",
    "model_comparison_plot(\"AUC-PR\", baseline_model_results, safe_to_file=\"aucpr_comparison_baseline.png\")\n",
    "model_comparison_plot(\"Recall (Class 1)\", baseline_model_results)\n",
    "model_comparison_plot(\"Precision (Class 1)\", baseline_model_results)\n",
    "model_comparison_plot(\"F1-Score (Class 1)\", baseline_model_results)\n",
    "model_comparison_plot(\"Accuracy\", baseline_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7b55e-2ef7-4e85-bd86-0a64b658621b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Comparison Tables</strong> <br>\n",
    "    📌 Compare evaluation metrics of outlier handling methods on the validation data using tables (one table for each baseline model).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fddbf-e340-4707-bcb0-d25b8999c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier handler comparison dictionary\n",
    "outlier_handler_comparison = {}  \n",
    "\n",
    "# Iterate over each baseline model\n",
    "for model_name in baseline_models.keys():\n",
    "    # Extract metrics for each outlier handler under the current model\n",
    "    outlier_handler_comparison[model_name] = {\n",
    "        outlier_handler_name: {\n",
    "            metric: baseline_model_results[outlier_handler_name][model_name][metric]\n",
    "            for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "        }\n",
    "        for outlier_handler_name in baseline_model_results.keys()\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame \n",
    "    outlier_handler_comparison[model_name] = pd.DataFrame(outlier_handler_comparison[model_name]).transpose()\n",
    "    \n",
    "# Compare outlier handlers for Logistic Regression\n",
    "round(outlier_handler_comparison[\"Logistic Regression\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b11da-724e-44a7-a39d-a3a7017cddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for Elastic Net \n",
    "round(outlier_handler_comparison[\"Elastic Net\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f47f5-6110-4747-be63-471134c06768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for K-Nearest Neighbors\n",
    "round(outlier_handler_comparison[\"K-Nearest Neighbors\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b054b4-8104-4120-830b-110446fc29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for Support Vector Machine\n",
    "round(outlier_handler_comparison[\"Support Vector Machine\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae7f67-6b53-4173-a654-c2299284848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for Neural Network\n",
    "round(outlier_handler_comparison[\"Neural Network\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b4692-4bc1-479b-8089-dc2bf26adbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for Decision Tree\n",
    "round(outlier_handler_comparison[\"Decision Tree\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5332e6-cb58-4a7b-8824-1d499c823b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outlier handlers for Random Forest\n",
    "round(outlier_handler_comparison[\"Random Forest\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156ecf8-c0a5-4a0b-8f10-4ae7b9b9d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare outlier handlers for XGBoost\n",
    "round(outlier_handler_comparison[\"XGBoost\"], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368a2f6-e87e-44b6-b1ab-99440f124d73",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    💡 Since outlier handling does not meaningfully improve AUC-PR, hyperparameter tuning and other downstream modeling steps will proceed without it.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bc526-fe4f-4972-8c5c-3fc0ee5c7b64",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Compare evaluation metrics of all baseline models on the validation data (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a990fa7-8b5d-4cf0-86f1-2acc2100a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract evaluation metrics \n",
    "baseline_model_comparison = {\n",
    "    model_name: {\n",
    "        metric: baseline_model_results[\"No Outlier Handling\"][model_name][metric]\n",
    "        for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "    }\n",
    "    for model_name in baseline_model_results[\"No Outlier Handling\"]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame \n",
    "baseline_model_comparison = pd.DataFrame(baseline_model_comparison).transpose()\n",
    "\n",
    "# Display model comparison table\n",
    "round(baseline_model_comparison, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7bac9-5678-4590-9da0-a428275c4588",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Precision-Recall Curves</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Plot precision-recall curves of all baseline models on the validation data in a single graph (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38613e7-7fbe-4bc1-af0f-cd3b2c849d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot precision-recall curve of one or more models\n",
    "def plot_precision_recall_curve(y_true, model_results, title=\"Precision-Recall Curves\", safe_to_file=False):\n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get colors for different models (colormap \"viridis\" is colorblind-friendly)\n",
    "    cmap = plt.get_cmap(\"viridis\", len(model_results))\n",
    "    \n",
    "    # Plot baseline performance of random classifier\n",
    "    baseline = np.sum(y_true) / len(y_true)\n",
    "    ax.axhline(y=baseline, color=\"black\", linestyle=\"--\", alpha=0.5, label=f\"Baseline = {baseline:.2f}\")\n",
    "    \n",
    "    # Iterate over each model in the results dictionary\n",
    "    for i, (model_name, model_result) in enumerate(model_results.items()):\n",
    "        # Plot precision-recall curve for the current model\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true, model_result[\"y_val_proba\"])\n",
    "        auc_pr = auc(recall_curve, precision_curve)\n",
    "        ax.plot(recall_curve, precision_curve, color=cmap(i), label=f\"{model_name} AUC-PR={auc_pr:.2f}\")\n",
    "    \n",
    "    # Customize title, axes labels, axes ticks, legend, and grid\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel(\"Precision\", fontsize=12)\n",
    "    ax.set_xlabel(\"Recall\", fontsize=12)\n",
    "    ax.set_ylim(0, 1.02)  # slightly extend y-axis for visibility\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.legend(loc=\"best\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Precision-recall curve plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving precision-recall curve plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving precision-recall curve plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot precision-recall curves of all baseline models on the validation data (no outlier handling)\n",
    "plot_precision_recall_curve(\n",
    "    y_val, \n",
    "    baseline_model_results[\"No Outlier Handling\"], \n",
    "    title=\"Precision-Recall Curves: Baseline Models (No Outlier Handling)\",\n",
    "    safe_to_file=\"precision_recall_curves_baseline.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45c631-db7f-4693-9f50-a228ca2406b4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Create classification report for all baseline models on the validation data (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9313774-34f1-4a91-af35-c1a1ad3bd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all baseline models\n",
    "for model_name, model_result in baseline_model_results[\"No Outlier Handling\"].items():\n",
    "    print(f\"\\n{model_name}: Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"], zero_division=0))  # disable zero-division warning if no predictions for a given class (sets metric to 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96be650-66ec-4137-9f79-29a06578bec4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Plot confusion matrix for all baseline models on the validation data (no outlier handling).\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03f09c-0372-4f2f-96d7-8c59b4c413bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix (y, y_pred, title=\"\", display_labels=None, safe_to_file=False, axes=None):\n",
    "    # Create axis if not provided\n",
    "    if axes is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = axes\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    cm_disp.plot(cmap=\"viridis\", values_format=\"d\", colorbar=False, ax=ax)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(f\"{title}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax.set_ylabel(\"True\", fontsize=12)\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Confusion matrix saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving confusion matrix: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving confusion matrix to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show the plot\n",
    "    if axes is None:\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Use function to plot confusion matrix for all baseline models ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(baseline_model_results[\"No Outlier Handling\"])\n",
    "n_cols = 3  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 5x5 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(baseline_model_results[\"No Outlier Handling\"].items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name}\", \n",
    "                          display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[i])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049d0cb-46c6-43af-ab5c-9bf18efd74e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Diagnose overfitting for all baseline models (no outlier handling) by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97847309-00fe-4d7e-94c9-51bba2348ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze overfitting\n",
    "def analyze_overfitting(X_train, y_train, model_results):\n",
    "    # Store overfitting results as a list of dictionaries\n",
    "    overfitting_results = [] \n",
    "    \n",
    "    # Iterate over each model\n",
    "    for model_name, model_result in model_results.items():\n",
    "        model = model_result[\"model\"]\n",
    "    \n",
    "        # Predict on training data\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        if \"best_threshold\" in model_result:\n",
    "            y_train_pred = (y_train_proba >= result[\"best_threshold\"]).astype(int)\n",
    "        else:\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            \n",
    "        # Evaluate model on training data: AUC-PR\n",
    "        precision_curve_train, recall_curve_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "        auc_pr_train = auc(recall_curve_train, precision_curve_train)\n",
    "        \n",
    "        # Evaluate model on training data: Class-1 precision, recall, and F1-score\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred, average=\"binary\", pos_label=1, zero_division=0)\n",
    "    \n",
    "        # Get validation metrics\n",
    "        auc_pr_val = model_result[\"AUC-PR\"]\n",
    "        recall_val = model_result[\"Recall (Class 1)\"]\n",
    "        precision_val = model_result[\"Precision (Class 1)\"]\n",
    "        f1_val = model_result[\"F1-Score (Class 1)\"] \n",
    "    \n",
    "        # Create results dictionary for current model\n",
    "        model_data = {\n",
    "            \"Model\": model_name,\n",
    "            \"AUC-PR (Train)\": auc_pr_train,\n",
    "            \"AUC-PR (Val)\": auc_pr_val,\n",
    "            \"AUC-PR (Diff)\": auc_pr_train - auc_pr_val,\n",
    "            \"Recall (Train)\": recall_train,\n",
    "            \"Recall (Val)\": recall_val,\n",
    "            \"Recall (Diff)\": recall_train - recall_val,\n",
    "            \"Precision (Train)\": precision_train,\n",
    "            \"Precision (Val)\": precision_val,\n",
    "            \"Precision (Diff)\": precision_train - precision_val,\n",
    "            \"F1-Score (Train)\": f1_train,\n",
    "            \"F1-Score (Val)\": f1_val, \n",
    "            \"F1-Score (Diff)\": f1_train - f1_val,\n",
    "        }\n",
    "        overfitting_results.append(model_data)\n",
    "    \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    overfitting_results = pd.DataFrame(overfitting_results)\n",
    "    \n",
    "    # Set model as index\n",
    "    overfitting_results = overfitting_results.set_index(\"Model\")\n",
    "    \n",
    "    return overfitting_results\n",
    "\n",
    "\n",
    "# Use function to analyze overfitting for all baseline models (no outlier handling)\n",
    "file_path = \"models/baseline_models_overfitting.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        baseline_models_overfitting = pd.read_csv(file_path, index_col=\"Model\")\n",
    "        print(f\"{file_path} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {file_path}: {e}\") \n",
    "else:\n",
    "    baseline_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=baseline_model_results[\"No Outlier Handling\"])\n",
    "    try:\n",
    "        baseline_models_overfitting.to_csv(file_path)\n",
    "        print(f\"Overfitting results saved successfully under '{file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the overfitting results: {e}\")\n",
    "\n",
    "# Display overfitting results\n",
    "round(baseline_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bc60d-7fa8-4fbd-afcb-53409e7a7d62",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Plot training and validation metrics of all baseline models side-by-side using grouped bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15bd53-543e-41b5-946e-9f65f8078965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train vs. validation metrics of multiple models \n",
    "def plot_train_val_metrics(metrics, overfitting_results, safe_to_file=False):\n",
    "    # Define metric name mapping dictionary\n",
    "    metric_name_map = {\n",
    "        \"Recall\": \"Recall (Class 1)\",\n",
    "        \"Precision\": \"Precision (Class 1)\",\n",
    "        \"F1-Score\": \"F1-Score (Class 1)\"\n",
    "    }\n",
    "    \n",
    "    # Ensure metrics is a list, even if a single metric is provided\n",
    "    if not isinstance(metrics, list):\n",
    "        metrics = [metrics]\n",
    "\n",
    "    # Get number of metrics\n",
    "    n = len(metrics)\n",
    "    \n",
    "    # Set up the subplot layout based on the number of metrics\n",
    "    if n == 1:\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "        axes = [ax]\n",
    "    else:\n",
    "        # Create a grid with 2 columns and enough rows to accommodate all metrics\n",
    "        rows = int(np.ceil(n / 2))\n",
    "        fig, axes = plt.subplots(rows, 2, figsize=(16, 6 * rows))\n",
    "        # Flatten the axes for easier iteration\n",
    "        axes = axes.flat\n",
    "\n",
    "    # Add overall figure title only if there are multiple metrics\n",
    "    if n > 1:\n",
    "        fig.suptitle(\"Overfitting: Train vs. Validation Metrics\", fontsize=16, y=0.98)\n",
    "    \n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        # Get metric display name \n",
    "        display_name = metric_name_map.get(metric, metric)\n",
    "        \n",
    "        # Create DataFrame with only training and validation metric\n",
    "        metric_df = overfitting_results[[f\"{metric} (Train)\", f\"{metric} (Val)\"]].reset_index()\n",
    "    \n",
    "        # Rename columns for clarity\n",
    "        metric_df = metric_df.rename(columns={f\"{metric} (Train)\": \"Training\", f\"{metric} (Val)\": \"Validation\"})\n",
    "        \n",
    "        # Melt the DataFrame for easier plotting \n",
    "        metric_df = pd.melt(\n",
    "            metric_df,\n",
    "            id_vars=[\"Model\"], \n",
    "            value_vars=[\"Training\", \"Validation\"],\n",
    "            var_name=\"Data\", \n",
    "            value_name=display_name\n",
    "        )\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        sns.barplot(x=\"Model\", y=display_name, hue=\"Data\", data=metric_df, palette=\"viridis\", ax=ax)\n",
    "    \n",
    "        # Add value labels \n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "       \n",
    "        # Customize plot \n",
    "        if n > 1:\n",
    "            ax.set_title(f\"{display_name}\", fontsize=14, pad=12)\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.set_title(f\"Overfitting: Train vs. Validation {display_name}\", fontsize=14, pad=12)\n",
    "            ax.set_ylabel(display_name, fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "        ax.tick_params(axis=\"x\", labelrotation=45 if len(overfitting_results.index) > 5 else 0, labelsize=12)  # rotate xticks if more than 5 models\n",
    "        ax.tick_params(axis=\"y\", labelsize=10)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "    \n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Overfitting plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving overfitting plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving overfitting plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot train vs. validation AUC-PR of all baseline models  \n",
    "plot_train_val_metrics(\"AUC-PR\", baseline_models_overfitting)\n",
    "\n",
    "# Use function to plot train vs. validation comparison of all metrics for all baseline models \n",
    "plot_train_val_metrics([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], baseline_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355cc17-e7a7-4485-89c1-94adf9e76b66",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Plot train-validation difference scores of all metrics and all baseline models in a single grouped bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94a448-dcfc-4a59-9d72-63571d8e11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train-validation difference scores of multiple models across multiple metrics in a single bar plot\n",
    "def plot_train_val_difference(metrics, overfitting_results, safe_to_file=False):\n",
    "    # Extract difference scores from overfitting results\n",
    "    diff_metrics = [metric + \" (Diff)\" for metric in metrics]\n",
    "    metric_df = overfitting_results[diff_metrics].reset_index()\n",
    "    \n",
    "    # Rename columns for better reabability\n",
    "    metric_df = metric_df.rename(columns={\n",
    "        \"AUC-PR (Diff)\": \"AUC-PR\",\n",
    "        \"Recall (Diff)\": \"Recall (Class 1)\",\n",
    "        \"Precision (Diff)\": \"Precision (Class 1)\",\n",
    "        \"F1-Score (Diff)\": \"F1-Score (Class 1)\"\n",
    "    })\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    metric_df = pd.melt(\n",
    "        metric_df,\n",
    "        id_vars=[\"Model\"], \n",
    "        var_name=\"Metric\", \n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "    \n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "    # Create grouped bar plot\n",
    "    sns.barplot(data=metric_df, x=\"Model\", y=\"Value\", hue=\"Metric\", palette=\"viridis\", ax=ax)\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=8)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(\"Overfitting: Train-Validation Difference Scores\", fontsize=14, pad=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Difference (Train - Val)\", fontsize=12)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12, labelrotation=45 if len(overfitting_results.index) > 5 else 0)  # rotate xticks if more than 5 models\n",
    "    ax.tick_params(axis=\"y\", labelsize=10)  \n",
    "    ax.set_ylim(metric_df[\"Value\"].min() - 0.05, metric_df[\"Value\"].max() + 0.05)  # expand y-axis limits by 0.05 \n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Overfitting plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving overfitting plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving overfitting plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot train-validation difference scores of all metrics and all baseline models\n",
    "plot_train_val_difference([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], baseline_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1da40-dba2-4d3d-ab2a-b234ff92b805",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Analyze relationships between the features and misclassifications on the validation data through correlations and grouped box plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e96b7b-e2a6-4e5f-97bf-77b82f6aaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze feature correlations with misclassifications\n",
    "def analyze_feature_misclassification(X, y, y_pred, numerical_features=None):  \n",
    "    # Combine features with actual and predicted target values into a single DataFrame\n",
    "    df = X.copy()  \n",
    "    df[\"Actual\"] = y\n",
    "    df[\"Predicted\"] = y_pred\n",
    "\n",
    "    # Create misclassification column\n",
    "    df[\"Misclassification\"] = (df[\"Predicted\"] != df[\"Actual\"]).astype(int)\n",
    "    \n",
    "    # Create correlations between features and misclassifications\n",
    "    correlations = df.drop(columns=[\"Actual\", \"Predicted\"]).corr()[\"Misclassification\"]\n",
    "    \n",
    "    # --- Create box plot matrix ---\n",
    "    # Ensure only numerical features\n",
    "    if numerical_features is None:\n",
    "        numerical_features = X.select_dtypes(include=[\"number\"]).columns  \n",
    "        # Include continuous numerical features, exclude binary and ordinal features with less than 5 categories\n",
    "        numerical_features = [column for column in numerical_features if X[column].nunique() > 4]\n",
    "        \n",
    "    # Number of columns and rows for matrix grid\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(len(numerical_features) / n_cols)\n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over the numerical features \n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create a box plot of the current feature grouped by misclassification\n",
    "        sns.boxplot(data=df, x=\"Misclassification\", y=feature, ax=ax)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f\"{feature.title().replace('_', ' ')} by Misclassification\")\n",
    "        ax.set_xlabel(\"Misclassification\")\n",
    "        ax.set_ylabel(f\"{feature.title().replace('_', ' ')}\")\n",
    "        ax.set_xticks(ticks=[0, 1], labels=[\"Correct\", \"Misclassified\"])\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return the misclassification correlations\n",
    "    return correlations\n",
    "\n",
    "\n",
    "# Example usage for a single model  \n",
    "# rf_misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, baseline_model_results[\"No Outlier Handling\"][\"Random Forest\"][\"y_val_pred\"])\n",
    "\n",
    "# --- Use function to analyze feature misclassification relationships of all baseline models (no outlier handling) ---\n",
    "# Initialize results dictionary\n",
    "baseline_misclassification_correlations = {}\n",
    "# Iterate over each model\n",
    "for model_name, model_result in baseline_model_results[\"No Outlier Handling\"].items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    # Add current model results to dictionary\n",
    "    baseline_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "# Convert results dictionary into a DataFrame    \n",
    "baseline_misclassification_correlations = pd.DataFrame(baseline_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3307c8f-f08a-45d5-b360-78d2b8b33cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of all baseline models (with 2 decimals)\n",
    "round(baseline_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488aa402-6ef8-4e00-a3e5-b82eed8173e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Hyperparameter Tuning</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Model Selection</strong> <br>\n",
    "    💡 The following models were selected for hyperparameter tuning based on good performance across the primary evaluation metric (AUC-PR) and the secondary metrics (class-1-specific recall, precision, and F1-score) on the validation data. While Random Forest and Decision Tree exhibited overfitting (large AUC-PR difference between training and validation data), their good validation metrics indicate potential that can be refined through tuning.\n",
    "    <ul>\n",
    "        <li><b>Random Forest</b>: Highest AUC-PR of 0.62 and F1-score of 0.58. Good balance between recall of 0.54 and precision of 0.62, despite overfitting (AUC-PR Diff of 0.16).</li>\n",
    "        <li><b>XGBoost</b>: Good AUC-PR of 0.56 and highest precision of 0.67 despite low recall of 0.21. Low overfitting (AUC-PR Diff: -0.02).</li>\n",
    "        <li><b>K-Nearest Neighbors</b>: Good AUC-PR of 0.56 with recall of 0.49 and precision of 0.57 (F1-score: 0.52) and low overfitting (AUC-PR Diff: 0.03).</li>\n",
    "        <li><b>Decision Tree</b>: Decent AUC-PR of 0.47 and highest recall of 0.57 with balanced precision of 0.54 (F1-score: 0.55), despite overfitting (AUC-PR Diff: 0.25).</li>\n",
    "    </ul>\n",
    "    <strong>Next steps</strong> <br>\n",
    "    <ul>\n",
    "        <li>Tune the hyperparameters of each model using randomized search.</li>\n",
    "        <li>Retrain the best-performing model from each algorithm and plot precision-recall curves.</li>\n",
    "        <li>Optimize decision thresholds.</li>\n",
    "        <li>Evaluate the best hyperparameter-tuned models with both default and optimized thresholds using:\n",
    "            <ul>\n",
    "                <li>Evaluation metrics (AUC-PR, class-1 recall, precision, and F1-score).</li>\n",
    "                <li>Additional diagnostics (classification report, confusion matrix, overfitting, feature misclassification analysis).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e876c40-a6cf-46cc-a231-ada757fcd5e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Random Forest</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>ℹ️ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees in the forest.</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features considered for the best split; default <code>\"auto\"</code> uses the square root of all features.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are supposed to have weight one. Use <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies in the input data.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">scikit-learn RandomForestClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c087c9c-d650-44df-8c14-3778fa210ac3",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd49c05-07f9-440c-8ae3-45f6b53eb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "rf_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(5, 31),  # Random integers between 5 and 30            \n",
    "    \"min_samples_split\": randint(2, 21),  # Random integers between 2 and 20\n",
    "    \"min_samples_leaf\": randint(1, 11),  # Random integers between 1 and 10\n",
    "    \"max_features\": uniform(0.1, 0.9),  # Random floats between 0.1 and 1.0  \n",
    "    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=rf_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2,  # print training progress messages\n",
    "    refit=False  # Prevent storing \"best_estimator_\" to save storage\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# rf_random_search.fit(X_train_transformed, y_train)\n",
    " \n",
    "# Save fitted random search as .pkl file using helper function  \n",
    "# save_model(rf_random_search, \"rf_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbcac9-2c9b-402f-8df1-9de0f4d492ab",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2a5eb-230e-43c7-8642-1b4ae41085e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function\n",
    "rf_random_search = load_model(\"rf_random_search.pkl\")\n",
    "\n",
    "# DataFrame of randomized search results\n",
    "rf_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": rf_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": rf_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in rf_param_distributions:\n",
    "    rf_random_search_results[parameter] = rf_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_random_search_results = rf_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "rf_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f95ed-8eb7-4761-9124-618bd8530926",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">XGBoost</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>ℹ️ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees (boosting rounds).</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree.</li>\n",
    "        <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li>\n",
    "        <li><code>subsample</code>: Fraction of training samples used per tree.</li>\n",
    "        <li><code>colsample_bytree</code>: Fraction of features used per tree.</li>\n",
    "        <li><code>gamma</code>: Minimum loss reduction required to split a leaf node.</li>\n",
    "        <li><code>min_child_weight</code>: Minimum sum of instance weights (hessian) in a child.</li>\n",
    "        <li><code>scale_pos_weight</code>: Balances positive and negative class weights for imbalanced datasets.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928573d-3194-4c76-be86-1a3dbe0f1454",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc244-2ccd-426f-98ff-a57ffef568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "xgb_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(3, 11),  # Random integers between 3 and 10            \n",
    "    \"learning_rate\": uniform(0.01, 0.29),  # Random floats between 0.01 and 0.30\n",
    "    \"subsample\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"gamma\": uniform(0, 0.5),  # Random floats between 0.0 and 0.5  \n",
    "    \"min_child_weight\": randint(1, 10),  # Random integers between 1 and 9\n",
    "    \"scale_pos_weight\": randint(1, 16)  # Random integers between 1 and 15\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=xgb_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2  # print training progress messages\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# xgb_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file using helper function  \n",
    "# save_model(xgb_random_search, \"xgb_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced2cd5-3c3f-4468-a427-563e1ef8ce86",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55717fdd-a4df-4947-96f0-e6174b70d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function \n",
    "xgb_random_search = load_model(\"xgb_random_search.pkl\")\n",
    "\n",
    "# DataFrame of randomized search results\n",
    "xgb_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": xgb_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": xgb_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in xgb_param_distributions:\n",
    "    xgb_random_search_results[parameter] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_random_search_results = xgb_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "xgb_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ed213-dd35-450d-b11d-6da1efb7e341",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Decision Tree</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>ℹ️ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>max_depth</code>: Maximum depth of the tree. <code>None</code> allows nodes to expand until all leaves are pure or contain fewer samples than <code>min_samples_split</code>.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features to consider for the best split. If <code>None</code>, all features are considered.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are given equal weight. Can be <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies.</li>\n",
    "        <li><code>\"ccp_alpha\"</code>: Complexity parameter for pruning. A higher value encourages pruning by penalizing tree complexity.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\">scikit-learn DecisionTreeClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf937112-0826-4d9e-86d8-cf678970f52f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136f223-9f7b-4269-985e-9387c61d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "tree_param_distributions = {\n",
    "    \"max_depth\": randint(5, 31),  # Random integers between 5 and 30            \n",
    "    \"min_samples_split\": randint(2, 11),  # Random integers between 2 and 10\n",
    "    \"min_samples_leaf\": randint(1, 6),  # Random integers between 1 and 5\n",
    "    \"max_features\": uniform(0.3, 0.7),  # Random floats between 0.3 and 1.0  \n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"ccp_alpha\": uniform(0.0, 0.02)  # Random floats between 0.0 and 0.02\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "tree_random_search = RandomizedSearchCV(\n",
    "    estimator=tree, \n",
    "    param_distributions=tree_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2  # print training progress messages\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# tree_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file using helper function\n",
    "# save_model(tree_random_search, \"tree_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383846-fd20-4aac-8fff-d6a28b2d54f1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa13fab-9525-4031-848c-a366d16a9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function \n",
    "tree_random_search = load_model(\"tree_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "tree_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": tree_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": tree_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in tree_param_distributions:\n",
    "    tree_random_search_results[parameter] = tree_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "tree_random_search_results = tree_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "tree_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc51e-b71c-4d4d-b341-54e0b9e3628a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">K-Nearest Neighbors</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <p>ℹ️ The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "                <li><code>n_neighbors</code>: The number of neighbors to use for prediction. A higher value makes the model more general, while a lower value may lead to overfitting.</li>\n",
    "                <li><code>weights</code>: Determines how neighbors are weighted during prediction. <code>\"uniform\"</code> gives equal weight to all neighbors, while <code>\"distance\"</code> gives closer neighbors more influence.</li>\n",
    "                <li><code>p</code>: The power parameter for the Minkowski distance. <code>p=1</code> corresponds to the Manhattan distance and <code>p=2</code> to the Euclidean distance.</li>\n",
    "                <li><code>algorithm</code>: The algorithm used to compute nearest neighbors. <code>\"auto\"</code> selects the best algorithm based on the dataset (options include <code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>, and <code>\"brute\"</code>).</li>\n",
    "                <li><code>leaf_size</code>: Relevant for tree-based algorithms (<code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>), ignored for <code>\"brute\"</code>.</li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\">scikit-learn KNeighborsClassifier documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722dae3-e64f-4ae3-a5b7-8bb0d2fedae5",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dda29f-8792-463d-bb71-573ae5b8de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "knn_param_distributions = {\n",
    "    \"n_neighbors\": randint(3, 31),  # Random integers between 3 and 30            \n",
    "    \"weights\": [\"uniform\", \"distance\"],  # \"distance\" can help with imbalanced classes\n",
    "    \"p\": [1, 2],  # p=1 (Manhattan) and p=2 (Euclidean)\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  \n",
    "    \"leaf_size\": randint(20, 51)\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "knn_random_search = RandomizedSearchCV(\n",
    "    estimator=knn, \n",
    "    param_distributions=knn_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"average_precision\",  # no built-in AUC-PR, but average precision score is a common proxy for AUC-PR\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2  # print training progress messages\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "# knn_random_search.fit(X_train_transformed, y_train)\n",
    "       \n",
    "# Save fitted random search as .pkl file using helper function  \n",
    "# save_model(knn_random_search, \"knn_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6409f90-4ed1-4275-ad7c-60db299c691f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180292e2-2d86-4513-bb25-e0bb26a45593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function \n",
    "knn_random_search = load_model(\"knn_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "knn_random_search_results = pd.DataFrame({\n",
    "    \"validation_average_precision\": knn_random_search.cv_results_[\"mean_test_score\"],  # average precision on validation data\n",
    "    \"parameters\": knn_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in knn_param_distributions:\n",
    "    knn_random_search_results[parameter] = knn_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "knn_random_search_results = knn_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "knn_random_search_results.sort_values(\"validation_average_precision\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34fb1e-aae6-4ed0-9b3e-6c3ea33b881f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Retraining</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Retrain the best hyperparameter-tuned model from each algorithm on the full training dataset.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a82e27-9b3a-4664-ad59-5899da11f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best hyperparameter-tuned model from each algorithm\n",
    "tuned_models = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(**knn_random_search.best_params_),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(**tree_random_search.best_params_, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_random_search.best_params_, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(**xgb_random_search.best_params_, random_state=42)\n",
    "}\n",
    "\n",
    "# Retrain models on the full training data\n",
    "# tuned_model_results = evaluate_all_models(tuned_models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Save hyperparameter-tuned model results as .pkl file \n",
    "# save_model(tuned_model_results, \"tuned_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3e39b-2fb8-44d8-86f4-e08250e3e822",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Precision-Recall Curves</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Plot precision-recall curves of the tuned models on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9513d-8d7a-4b5c-96ac-b158049b678f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load hyperparameter-tuned model results\n",
    "tuned_model_results = load_model(\"tuned_models.pkl\")\n",
    "\n",
    "# Plot precision-recall curves of hyperparameter-tuned models\n",
    "plot_precision_recall_curve(\n",
    "    y_val, \n",
    "    tuned_model_results, \n",
    "    title=\"Precision-Recall Curves: Hyperparameter-Tuned Models\",\n",
    "    safe_to_file=\"precision_recall_curves_tuned.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021ed63-1bf5-4697-ae0c-f448495d577c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    💡 Random Forest demonstrated the highest AUC-PR (0.62), followed by XGBoost (0.61).\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470af27b-2fb6-4b8b-948d-8259b82a5912",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Threshold Optimization</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ The default decision threshold (typically 0.5) may not be ideal to achieve the business goals, especially when certain performance targets are non-negotiable or misclassification has quantifiable costs. For loan defaults, recall (finding actual defaulters) is often prioritized because missing a defaulter (a false negative) is typically more costly than flagging a non-defaulter as risky (a false positive). \n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Determine the optimal threshold for each model that maximizes the F1-score while ensuring minimum recall of 0.80 and precision of 0.40.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114ed5-9e1a-49ff-84c9-3fe95d92cce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate a single model across multiple decision thresholds and determine the best threshold \n",
    "def optimize_threshold(y_true, y_pred_proba, thresholds=None, optimize=\"Accuracy\", min_accuracy=0, min_recall=0, min_precision=0, min_f1=0,\n",
    "                       cost_fp=0, cost_fn=0, title=\"Metrics by Threshold\", safe_to_file=False):\n",
    "    # Use 1% to 99% in 1%-steps in the absence of custom thresholds\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "    # --- Calculate metrics for each threshold ---\n",
    "    # Store threshold evaluation results as list of dictionaries\n",
    "    threshold_results = []   \n",
    "    \n",
    "    # Iterate over each threshold\n",
    "    for threshold in thresholds:\n",
    "        # Get class predictions for current threshold\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate evaluation metrics for current threshold\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, zero_division=0) \n",
    "\n",
    "        # Calculate cost for current threshold\n",
    "        if optimize == \"Cost\" and cost_fp == 0 and cost_fn == 0:\n",
    "            print(\"Warning: Cannot optimize for 'Cost' when cost_fn and cost_fp are both 0. Defaulting to 'Accuracy'.\")\n",
    "            optimize=\"Accuracy\"\n",
    "            total_cost = None\n",
    "        elif optimize == \"Cost\" and (cost_fp > 0 or cost_fn > 0):\n",
    "            # Calculate number of true negatives (tn), false positives (fp), false negatives (fn) and true positives (tp)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            # Calculate total cost\n",
    "            total_cost = cost_fp * fp + cost_fn * fn\n",
    "        else:\n",
    "            total_cost = None\n",
    "        \n",
    "        # Add evaluation metrics dictionary to list\n",
    "        threshold_results.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Recall (Class 1)\": recall[1],\n",
    "            \"Precision (Class 1)\": precision[1],\n",
    "            \"F1-Score (Class 1)\": f1[1],\n",
    "            \"Cost\": total_cost\n",
    "        })    \n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    threshold_results = pd.DataFrame(threshold_results)\n",
    "\n",
    "    # --- Determine the best threshold --- \n",
    "    # Filter thresholds that satisfy minimum accuracy, recall, precision, and F1-score\n",
    "    filtered_thresholds = threshold_results[\n",
    "        (threshold_results[\"Accuracy\"] >= min_accuracy) & \n",
    "        (threshold_results[\"Recall (Class 1)\"] >= min_recall) & \n",
    "        (threshold_results[\"Precision (Class 1)\"] >= min_precision) & \n",
    "        (threshold_results[\"F1-Score (Class 1)\"] >= min_f1)\n",
    "    ]\n",
    "    \n",
    "    # Fallback to no minimum criteria if not a single threshold satisfies all of them\n",
    "    if filtered_thresholds.empty:\n",
    "        print(\"Warning: No threshold satisfies all minimum criteria.\")\n",
    "        print(\"Defaulting to optimization without any minimum criteria.\")\n",
    "        filtered_thresholds = threshold_results.copy()\n",
    "    \n",
    "    # Optimize accuracy\n",
    "    if optimize == \"Accuracy\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Accuracy\"].idxmax(), \"threshold\"] \n",
    "    # Optimize recall\n",
    "    elif optimize == \"Recall (Class 1)\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Recall (Class 1)\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize precision\n",
    "    elif optimize == \"Precision (Class 1)\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Precision (Class 1)\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize F1-score\n",
    "    elif optimize == \"F1-Score (Class 1)\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"F1-Score (Class 1)\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize cost\n",
    "    elif optimize == \"Cost\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Cost\"].idxmin(), \"threshold\"]  \n",
    "    # Fallback to accuracy if metric unkown \n",
    "    else:\n",
    "        print(f\"Warning: Unknown optimize metric '{optimize}'. Defaulting to accuracy.\")\n",
    "        optimize = \"Accuracy\"\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Accuracy\"].idxmax(), \"threshold\"] \n",
    "    \n",
    "    # --- Plot metrics by threshold --- \n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Define metrics to use in plot starting with the optimization metric\n",
    "    metrics = [optimize]\n",
    "    # When plotting anything other than cost, also plot all metrics with minimum criteria\n",
    "    if optimize != \"Cost\":\n",
    "        if min_accuracy > 0 and \"Accuracy\" not in metrics:\n",
    "            metrics.append(\"Accuracy\")\n",
    "        if min_recall > 0 and \"Recall (Class 1)\" not in metrics:\n",
    "            metrics.append(\"Recall (Class 1)\")\n",
    "        if min_precision > 0 and \"Precision (Class 1)\" not in metrics:\n",
    "            metrics.append(\"Precision (Class 1)\")\n",
    "        if min_f1 > 0 and \"F1-Score (Class 1)\" not in metrics:\n",
    "            metrics.append(\"F1-Score (Class 1)\")\n",
    "\n",
    "    # Order metrics\n",
    "    ordered_metrics = [\"Cost\", \"Accuracy\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\"]\n",
    "    ordered_metrics = [metric for metric in ordered_metrics if metric in metrics]\n",
    "        \n",
    "    # Get a color from viridis colormap for each metric\n",
    "    n_metrics = len(ordered_metrics)\n",
    "    cmap = plt.get_cmap(\"viridis\", n_metrics)\n",
    "\n",
    "    # Iterate over each metric\n",
    "    for i, metric in enumerate(ordered_metrics):\n",
    "        # Create line plot of current metric by threshold\n",
    "        ax.plot(threshold_results[\"threshold\"], threshold_results[metric], label=metric, color=cmap(i))\n",
    "    \n",
    "    # Format cost plots\n",
    "    if optimize == \"Cost\":\n",
    "        title += f\" (FN Cost: {cost_fn}, FP Cost: {cost_fp})\"  # add FN and FP costs to title \n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))  # format cost y-axis with thousand separator and no decimals\n",
    "\n",
    "    # Format plots with all other metrics\n",
    "    else:\n",
    "        ax.set_ylim(-0.02, 1.02)  # value range of metrics is 0 to 1, slightly extend y-axis for better visibility\n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.1))  # set y-axis ticks from 0 to 1 in 0.1 steps        \n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(\"Threshold\", fontsize=12)\n",
    "    ax.set_ylabel(\"Metric Value\" if n_metrics > 1 else metric, fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.legend(fontsize=11).set_visible(True if n_metrics > 1 else False)\n",
    "    ax.grid(True, alpha=0.3)\n",
    " \n",
    "    # Add dashed line for best threshold \n",
    "    ax.axvline(x=best_threshold, color=\"gray\", linestyle=\"--\")\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.text(best_threshold+0.01, y_min+(y_max-y_min)*0.05, f\"Best Threshold: {best_threshold:.2f}\", rotation=90, fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Metrics by threshold plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving metrics by threshold plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving metrics by threshold plot to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold, threshold_results\n",
    "\n",
    "\n",
    "# Example usage: Optimize F1-score while ensuring minimum recall of 0.80 and precision of 0.40 \n",
    "# rf_best_threshold, rf_threshold_results = optimize_threshold(\n",
    "#     y_true=y_val, \n",
    "#     y_pred_proba=tuned_model_results[\"Random Forest\"][\"y_val_proba\"], \n",
    "#     optimize=\"F1-Score (Class 1)\",\n",
    "#     min_recall=0.8, \n",
    "#     min_precision=0.4\n",
    "# )\n",
    "\n",
    "\n",
    "# --- Use function to optimize thresholds for all tuned models --- \n",
    "# Define filename prefix for saving to file  \n",
    "model_prefix = {\n",
    "    \"K-Nearest Neighbors\": \"knn\",\n",
    "    \"Decision Tree\": \"tree\",\n",
    "    \"Random Forest\": \"rf\",\n",
    "    \"XGBoost\": \"xgb\"\n",
    "}\n",
    "\n",
    "# Store threshold optimization results as dictionary\n",
    "tuned_threshold_model_results = {}\n",
    "\n",
    "# Iterate over each tuned model\n",
    "for model_name, model_results in tuned_model_results.items():\n",
    "    # Optimize threshold for current model on the validation data \n",
    "    best_threshold, threshold_results = optimize_threshold(\n",
    "        y_true=y_val, \n",
    "        y_pred_proba=model_results[\"y_val_proba\"], \n",
    "        optimize=\"F1-Score (Class 1)\",  # maximize F1-score\n",
    "        min_recall=0.8,  # ensure minimum recall of 0.8 \n",
    "        min_precision=0.4,  # ensure minimum precision of 0.4\n",
    "        title=f\"{model_name}: Metrics by Threshold\",\n",
    "        safe_to_file=f\"{model_prefix[model_name]}_metrics_by_threshold_tuned.png\"\n",
    "    )\n",
    "\n",
    "    # Add best threshold and resulting class predictions to tuned threshold model results dictionary\n",
    "    tuned_threshold_model_results[model_name] = {}\n",
    "    tuned_threshold_model_results[model_name][\"best_threshold\"] = best_threshold\n",
    "    tuned_threshold_model_results[model_name][\"y_val_pred\"] = (tuned_model_results[model_name][\"y_val_proba\"] >= best_threshold).astype(int)\n",
    "\n",
    "    # Add evaluation metrics of optimized threshold model to results dictionary\n",
    "    tuned_threshold_model_results[model_name][\"AUC-PR\"] = model_results[\"AUC-PR\"]\n",
    "    tuned_threshold_model_results[model_name][\"Accuracy\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Accuracy\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"Recall (Class 1)\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Recall (Class 1)\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"Precision (Class 1)\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Precision (Class 1)\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"F1-Score (Class 1)\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"F1-Score (Class 1)\"].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c02ad-9e14-472d-8123-66d12be1934f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    📌 Compare evaluation metrics of hyperparameter-tuned models with default and optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40736775-ae3a-48c0-ae07-d6d526da0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics with default thresholds\n",
    "metrics_default_thresholds = {\n",
    "    model_name: {\n",
    "        metric: tuned_model_results[model_name][metric]\n",
    "        for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "    }\n",
    "    for model_name in tuned_model_results\n",
    "}\n",
    "\n",
    "# Extract metrics with optimized thresholds\n",
    "metrics_optimized_thresholds = {\n",
    "    model_name: {\n",
    "        metric: tuned_threshold_model_results[model_name][metric]\n",
    "        for metric in [\"AUC-PR\", \"Recall (Class 1)\", \"Precision (Class 1)\", \"F1-Score (Class 1)\", \"Accuracy\"]\n",
    "    }\n",
    "    for model_name in tuned_threshold_model_results\n",
    "}\n",
    "\n",
    "# Create dictionary with tuned model comparison tables \n",
    "tuned_model_comparison = {\n",
    "    \"default_thresholds\": pd.DataFrame(metrics_default_thresholds).transpose(),\n",
    "    \"optimized_thresholds\": pd.DataFrame(metrics_optimized_thresholds).transpose()\n",
    "}\n",
    "\n",
    "# Display comparison table for default thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "display(round(tuned_model_comparison[\"default_thresholds\"], 2))\n",
    "\n",
    "# Display comparison table for optimized thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "display(round(tuned_model_comparison[\"optimized_thresholds\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1fc07-963f-4bdd-b17c-25a3052a06d8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    💡 Random Forest and XGBoost demonstrated the highest F1-score (0.64) while meeting minimum recall (0.80) and exceeding precision (0.54 vs. min. 0.40).\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26dbc1-7576-4350-a792-c44ccaf3260e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    📌 Show classification report of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2114f12-081d-4343-a69f-3d7a5d9fb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all tuned models with default thresholds\n",
    "for model_name, model_result in tuned_model_results.items():\n",
    "    print(f\"\\n{model_name} (Default Threshold): Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88687a-a9df-4550-ad9d-1e29c585d37b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    📌 Show classification report of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c8aa3-c1c0-4933-ac93-a3b02a9be4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all tuned models with optimized thresholds\n",
    "for model_name, model_result in tuned_threshold_model_results.items():\n",
    "    print(f\"\\n{model_name} (Optimized Threshold): Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805b40a-7697-453f-a0df-ee3971af5d1a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    📌 Plot confusion matrix of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b72dcb-7408-419c-b9ae-659a95a6e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for all tuned models with default thresholds ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(tuned_model_results)\n",
    "n_cols = 2  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 6x6 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(tuned_model_results.items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name} (Default Threshold)\", \n",
    "                          display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[i])\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6a1a6-cf2d-4238-9509-5c926a6703af",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    📌 Plot confusion matrix of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128206b-486b-4f7b-9e48-17075c24e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for all tuned models with optimized thresholds ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(tuned_threshold_model_results)\n",
    "n_cols = 2  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 6x6 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(tuned_threshold_model_results.items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name} (Optimized Threshold)\", \n",
    "                          display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[i])\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd8994-733b-48e1-b687-8051b8c56a69",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    📌 Diagnose overfitting of hyperparameter-tuned models with default thresholds by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6a274-9046-42bc-9284-b25ea44c4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting for tuned models with default thresholds \n",
    "file_path = \"models/tuned_models_overfitting.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        tuned_models_overfitting = pd.read_csv(file_path, index_col=\"Model\")\n",
    "        print(f\"{file_path} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {file_path}: {e}\") \n",
    "else:\n",
    "    tuned_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=tuned_model_results)\n",
    "    try:\n",
    "        tuned_models_overfitting.to_csv(file_path)\n",
    "        print(f\"Overfitting results saved successfully under '{file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the overfitting results: {e}\")\n",
    "\n",
    "# Display overfitting results\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "round(tuned_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c183c7-7373-4fbb-8503-79c2f6163f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs. validation AUC-PR of tuned models with default thresholds \n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "plot_train_val_metrics(\"AUC-PR\", tuned_models_overfitting)\n",
    "\n",
    "# Plot train vs. validation comparison of all metrics for tuned models with default thresholds  \n",
    "plot_train_val_metrics([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], tuned_models_overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d6a26-5326-4f57-b258-e4647562c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-validation difference scores of all metrics for tuned models with default thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "plot_train_val_difference([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], tuned_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39e1d-9fd3-4e9e-bc7b-f9c742a138c3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    📌 Diagnose overfitting of hyperparameter-tuned models with optimized thresholds by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8c991-d2ef-4102-a0f9-17c1267dab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting for tuned models with optimized thresholds \n",
    "file_path = \"models/tuned_threshold_models_overfitting.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        tuned_threshold_models_overfitting = pd.read_csv(file_path, index_col=\"Model\")\n",
    "        print(f\"{file_path} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {file_path}: {e}\") \n",
    "else:\n",
    "    tuned_threshold_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=tuned_threshold_model_results)\n",
    "    try:\n",
    "        tuned_threshold_models_overfitting.to_csv(file_path)\n",
    "        print(f\"Overfitting results saved successfully under '{file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the overfitting results: {e}\")\n",
    "\n",
    "# Display overfitting results\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "round(tuned_threshold_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097220b-0f63-4e65-b870-4e56b11ebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs. validation AUC-PR of tuned models with optimized thresholds \n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "plot_train_val_metrics(\"AUC-PR\", tuned_threshold_models_overfitting, safe_to_file=\"overfitting_tuned_thresholds.png\")\n",
    "\n",
    "# Plot train vs. validation comparison of all metrics for tuned models with optimized thresholds  \n",
    "plot_train_val_metrics([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], tuned_threshold_models_overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb2a70-d43c-4df1-8a90-9897b47d35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-validation difference scores of all metrics for tuned models with optimized thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "plot_train_val_difference([\"AUC-PR\", \"Recall\", \"Precision\", \"F1-Score\"], tuned_threshold_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9559f84-2c2b-4ece-b2fb-3c632fe30ecd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    📌 Analyze feature misclassification relationships of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab291099-6cc8-4feb-b75e-c9ab3ccce5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze feature misclassification relationships of tuned models with default thresholds ---\n",
    "# Initialize results dictionary\n",
    "tuned_misclassification_correlations = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model_result in tuned_model_results.items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    \n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    \n",
    "    # Add current model results to dictionary\n",
    "    tuned_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "    \n",
    "# Convert results dictionary into a DataFrame    \n",
    "tuned_misclassification_correlations = pd.DataFrame(tuned_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b67a1-635f-4fc2-82ca-7153ee4df30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of tuned models with default thresholds\n",
    "round(tuned_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3471c-8f2f-4619-aad5-760f29ea212b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    📌 Analyze feature misclassification relationships of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af543601-1107-4d2b-aeba-70d5927536ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze feature misclassification relationships of tuned models with default thresholds ---\n",
    "# Initialize results dictionary\n",
    "tuned_threshold_misclassification_correlations = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model_result in tuned_threshold_model_results.items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    \n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    \n",
    "    # Add current model results to dictionary\n",
    "    tuned_threshold_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "    \n",
    "# Convert results dictionary into a DataFrame    \n",
    "tuned_threshold_misclassification_correlations = pd.DataFrame(tuned_threshold_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d2cbf-4873-4620-b630-ca2c1d99f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of tuned models with optimized thresholds\n",
    "round(tuned_threshold_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635ddf4-7c1c-4767-b8c0-aad06537395b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Final Model</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Model Selection</strong> <br>\n",
    "    💡 Random Forest with optimized threshold was selected for its good performance, low overfitting, and interpretability.\n",
    "    <ul>\n",
    "        <li>Performance: Highest validation AUC-PR (0.62), highest F1-score together with XGBoost (0.64), while meeting minimum recall (0.80) and precision (0.54 vs. min. 0.40). </li> \n",
    "        <li>Overfitting: Lowest AUC-PR difference between training and validation (0.06) compared to XGBoost (0.13), Decision Tree (0.13), and KNN (0.26).</li> \n",
    "        <li>Interpretability: Higher degree of interpretability than XGBoost, crucial for transparency and regulatory compliance in finance. Selected Random Forest for its superior combination of good performance, low overfitting and interpretability.</li> \n",
    "    </ul>\n",
    "    The final model is a Random Forest with a decision threshold of 0.29 and the following hyperparameters:\n",
    "    <ul>\n",
    "        <li><code>n_estimators=225</code></li>\n",
    "        <li><code>max_depth=26</code></li>\n",
    "        <li><code>min_samples_split=2</code></li>\n",
    "        <li><code>min_samples_leaf=1</code></li>\n",
    "        <li><code>max_features=0.13</code></li>\n",
    "        <li><code>class_weight=\"balanced\"</code></li>\n",
    "    </ul>\n",
    "    <strong>Next steps</strong> <br>\n",
    "    <ul>\n",
    "        <li>Retrain the final model, save it to a file, and apply the optimized threshold.</li>\n",
    "        <li>Evaluate the final model on the training, validation, and test sets to confirm its generalizability.</li>\n",
    "        <li>Use the same performance metrics as for the baseline and hyperparameter-tuned models \n",
    "            (AUC-PR, class-1 recall, precision, and F1-score) along with additional diagnostics \n",
    "            (classification report, confusion matrix, overfitting, feature misclassification analysis).\n",
    "        </li>\n",
    "        <li>In addition, conduct a feature importance analysis and review model prediction examples \n",
    "            to further interpret and validate model behavior.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c2446-6c72-4f56-a1ee-5cc2ba4edac4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Retraining</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Retrain the final model with optimized hyperparameters and save it to a <code>.pkl</code> file in the <code>model</code> directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa992c-4526-49d9-b36a-97cb9171472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with tuned hyperparameters\n",
    "rf_final_model = RandomForestClassifier(**rf_random_search.best_params_, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "# rf_final_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save final model as .pkl file \n",
    "# save_model(rf_final_model, \"rf_final_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13878f90-dedc-4bc9-857c-436893588136",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Apply optimized threshold to obtain predicted values on the training, validation, and test sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2643618-e56e-41cc-94c4-538b87468c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final model\n",
    "rf_final_model = load_model(\"rf_final_model.pkl\")\n",
    "\n",
    "# Predict probabilities for class-1 (default) on the training, validation and test data\n",
    "y_train_proba = rf_final_model.predict_proba(X_train_transformed)[:, 1]\n",
    "y_val_proba = rf_final_model.predict_proba(X_val_transformed)[:, 1]\n",
    "y_test_proba = rf_final_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Apply optimized threshold to convert probabilities to binary predictions\n",
    "threshold = 0.29\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "y_test_pred = (y_test_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9941d-f8ca-496f-8f42-f81d121d4c9d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Compare evaluation metrics of the final model on the training, validation, and test sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f942bd-c3bb-4e55-baf4-4f659c628772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-PR \n",
    "precision_curve_train, recall_curve_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "precision_curve_val, recall_curve_val, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "precision_curve_test, recall_curve_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "auc_pr_train = auc(recall_curve_train, precision_curve_train)\n",
    "auc_pr_val = auc(recall_curve_val, precision_curve_val)\n",
    "auc_pr_test = auc(recall_curve_test, precision_curve_test)\n",
    "\n",
    "# Precision, recall, and F1-score\n",
    "precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "precision_val, recall_val, f1_val, _ = precision_recall_fscore_support(y_val, y_val_pred)\n",
    "precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Create comparison table\n",
    "final_model_comparison = pd.DataFrame({\n",
    "    \"Data\": [\"Training\", \"Validation\", \"Test\"],\n",
    "    \"AUC-PR\": [auc_pr_train, auc_pr_val, auc_pr_test],\n",
    "    \"Recall (Class 1)\": [recall_train[1], recall_val[1], recall_test[1]],\n",
    "    \"Precision (Class 1)\": [precision_train[1], precision_val[1], precision_test[1]],\n",
    "    \"F1-Score (Class 1)\": [f1_train[1], f1_val[1], f1_test[1]],\n",
    "    \"Accuracy\": [accuracy_train, accuracy_val, accuracy_test],\n",
    "})\n",
    "\n",
    "# Display comparison table\n",
    "round(final_model_comparison, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873ddd-1a31-4188-a89a-bfa97d1522ea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Show classification report for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64847875-0e79-4dbf-87bb-5505fa991897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report: Training\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Classification Report: Validation\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Classification Report: Test\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5aea4-09c7-4c75-923e-a5edd5e40160",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Plot confusion matrix for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c093b-3b8a-4890-b992-5ead89c688be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for training, validation, and test data ---\n",
    "# Create subplot grid \n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(\"Random Forest: Confusion Matrix\", fontsize=16)\n",
    "\n",
    "# Create confusion matrix subplots for training, validation, and test data (using helper function)\n",
    "plot_confusion_matrix(y_train, y_train_pred, title=\"Training\", display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[0])\n",
    "plot_confusion_matrix(y_val, y_val_pred, title=\"Validation\", display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[1])\n",
    "plot_confusion_matrix(y_test, y_test_pred, title=\"Test\", display_labels=[\"Non-Defaulter\", \"Defaulter\"], axes=axes[2])\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d791c45-0e67-421d-a7e6-d8cd5eb30f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test data confusion matrix to file\n",
    "plot_confusion_matrix(y_test, y_test_pred, display_labels=[\"Non-Defaulter\", \"Defaulter\"], \n",
    "                      title=\"Random Forest: Confusion Matrix (Test)\", safe_to_file=\"rf_confusion_matrix_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e88ad-8de9-4b19-9f8d-49627c08c63f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Diagnose overfitting of final model by comparing evaluation metrics between training, validation, and test data using a grouped bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7218290-1779-4766-97c3-f31ba5350261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Overfitting grouped bar plot ---\n",
    "# Melt the final_model_comparison DataFrame (from the \"Metrics\" section) for easier plotting \n",
    "metric_df = pd.melt(final_model_comparison, id_vars=[\"Data\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "sns.barplot(data=metric_df, x=\"Metric\", y=\"Value\", hue=\"Data\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Add value labels \n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "\n",
    "# Customize plot \n",
    "ax.set_title(\"Random Forest: Overfitting\", fontsize=14)\n",
    "ax.set_ylabel(\"Metric Value\", fontsize=12)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylim(0, 1.05)  # slightly extend y-axis upper limit for better visibility of value labels\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))  # y-axis ticks from 0 to 1 in 0.1 steps\n",
    "ax.tick_params(axis=\"x\", labelsize=12) \n",
    "ax.tick_params(axis=\"y\", labelsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Adjust the layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f164c-99e2-4e19-8250-12150972800c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Analyze relationships between the features and misclassifications on the training, validation, and test sets through correlations and grouped box plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bdc9f-da1a-41b5-b21b-9e7b785c25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the training data (using helper function)\n",
    "print(\"Feature Misclassification Relationships: Training\")\n",
    "misclassification_correlations_train = analyze_feature_misclassification(X_train_transformed, y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651da44-6172-4f88-80a7-0915a4f035a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the validation data\n",
    "print(\"Feature Misclassification Relationships: Validation\")\n",
    "misclassification_correlations_val = analyze_feature_misclassification(X_val_transformed, y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e8ba4-b3b3-40b8-9cc4-3fac0ff0cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the test data\n",
    "print(\"Feature Misclassification Relationships: Test\")\n",
    "misclassification_correlations_test = analyze_feature_misclassification(X_test_transformed, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a1fc7-58f2-4067-994c-a2f01f70289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Misclassification Correlations ---\n",
    "# Merge training, validation, and test correlations into a single DataFrame\n",
    "misclassification_correlations = pd.concat(\n",
    "    [misclassification_correlations_train, misclassification_correlations_val, misclassification_correlations_test], \n",
    "    axis=1,\n",
    "    keys=[\"Training\", \"Validation\", \"Test\"]\n",
    ")\n",
    "\n",
    "# Show misclassification correlations (rounded to 2 decimals and sorted by absolute correlation values in test data)\n",
    "round(misclassification_correlations.sort_values(\"Test\", key=abs, ascending=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c923a-c73a-4da9-bf97-5ee42157452f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Importance</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Visualize feature importances with a bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1ffda-51aa-489b-a576-ef584fec0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = rf_final_model.feature_importances_\n",
    "\n",
    "# Get feature names in proper format \n",
    "feature_names = X_train_transformed.columns.str.title().str.replace(\"_\", \" \")\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot of top 10 features\n",
    "sns.barplot(data=feature_importance_df.head(10), x=\"importance\", y=\"feature\", hue=\"feature\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Random Forest: Top 10 Most Important Features\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Importance\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(feature_importance_df[\"importance\"].head(10)):\n",
    "    ax.text(value + 0.001, i, f\"{value:.2f}\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"rf_feature_importance_final.png\")  \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "        print(f\"Feature importance plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature importance plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving feature importance plot: '{image_path}' already exists.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425c0ee-709f-4a91-aed2-d2c2d9ec6625",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Prediction Examples</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ℹ️ Show illustrative examples of model predictions from test data to showcase performance on unseen data. Present a table highlighting the top 5 most important features, actual vs. predicted values, prediction confidence, and whether the example was misclassified.\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Identify best examples (correct with high confidence), worst examples (incorrect with high confidence), and typical examples (average confidence).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d002ad-b329-4bed-a2f7-6be180f70c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataFrame with top 5 features, actual and predicted values, prediction confidence, and misclassified from test data ---\n",
    "# Combine test features with actual and predicted target values into a single DataFrame\n",
    "prediction_examples = X_test.copy()  # raw features (before transformation) for easier interpretability\n",
    "prediction_examples.columns = prediction_examples.columns.str.title().str.replace(\"_\", \" \")  # format feature names\n",
    "prediction_examples[\"Actual Default\"] = y_test\n",
    "prediction_examples[\"Predicted Default\"] = y_test_pred\n",
    "\n",
    "# Calculate prediction confidence \n",
    "prediction_examples[\"Confidence Score\"] = rf_final_model.predict_proba(X_test_transformed).max(axis=1)\n",
    "prediction_examples[\"Confidence\"] = prediction_examples[\"Confidence Score\"].apply(lambda x: f\"{x:.0%}\")  # format as percentages\n",
    "\n",
    "# Calculate misclassified\n",
    "prediction_examples[\"Misclassified\"] = prediction_examples[\"Predicted Default\"] != prediction_examples[\"Actual Default\"]\n",
    "\n",
    "# Get top 5 most important features\n",
    "top5_features = feature_importance_df.sort_values(\"importance\", ascending=False).head(5)[\"feature\"]\n",
    "\n",
    "# Filter DataFrame columns\n",
    "columns_to_keep = list(top5_features) + [\"Actual Default\", \"Predicted Default\", \"Confidence Score\", \"Confidence\", \"Misclassified\"]\n",
    "prediction_examples = prediction_examples[columns_to_keep].copy()\n",
    "\n",
    "# Format columns\n",
    "prediction_examples[\"Income\"] = prediction_examples[\"Income\"].apply(lambda x: f\"{x:,}\")  # format with thousand separator\n",
    "prediction_examples[\"State Default Rate\"] = prediction_examples[\"State Default Rate\"].apply(lambda x: f\"{x:.1%}\")  # format as percentages\n",
    "prediction_examples[\"Actual Default\"] = prediction_examples[\"Actual Default\"].map({0: \"No\", 1: \"Yes\"})\n",
    "prediction_examples[\"Predicted Default\"] = prediction_examples[\"Predicted Default\"].map({0: \"No\", 1: \"Yes\"})\n",
    "prediction_examples[\"Misclassified\"] = prediction_examples[\"Misclassified\"].map({True: \"❌ Yes\", False: \"✅ No\"})\n",
    "\n",
    "# --- Identify best examples ---\n",
    "# Get the top 5 correctly classified cases with highest confidence scores for each class \n",
    "best_defaults = prediction_examples[(prediction_examples[\"Actual Default\"] == \"Yes\") & (prediction_examples[\"Misclassified\"] == \"✅ No\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "best_nondefaults = prediction_examples[(prediction_examples[\"Actual Default\"] == \"No\") & (prediction_examples[\"Misclassified\"] == \"✅ No\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "\n",
    "# Combine and show best prediction examples from each class\n",
    "best_examples = pd.concat([best_defaults, best_nondefaults]).drop(columns=[\"Confidence Score\"])\n",
    "print(\"Best examples (correct with high confidence):\")\n",
    "display(best_examples)\n",
    "\n",
    "# --- Identify worst examples ---\n",
    "# Get the top 5 misclassified cases despite high confidence scores for each class\n",
    "worst_defaults = prediction_examples[(prediction_examples[\"Actual Default\"] == \"Yes\") & (prediction_examples[\"Misclassified\"] == \"❌ Yes\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "worst_nondefaults = prediction_examples[(prediction_examples[\"Actual Default\"] == \"No\") & (prediction_examples[\"Misclassified\"] == \"❌ Yes\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "\n",
    "# Combine and show worst prediction examples from each class\n",
    "worst_examples = pd.concat([worst_defaults, worst_nondefaults]).drop(columns=[\"Confidence Score\"])\n",
    "print(\"Worst examples (incorrect with high confidence):\")\n",
    "display(worst_examples)\n",
    "\n",
    "# --- Identify typical examples ---\n",
    "# Calculate average confidence score\n",
    "mean_confidence = prediction_examples[\"Confidence Score\"].mean()\n",
    "\n",
    "# Calculate difference from average confidence for each case\n",
    "prediction_examples[\"Difference from Mean Confidence\"] = np.abs(prediction_examples[\"Confidence Score\"] - mean_confidence)\n",
    "\n",
    "# Get the top 5 cases with confidence scores closest to the average confidence for each class\n",
    "typical_defaults = prediction_examples[prediction_examples[\"Actual Default\"] == \"Yes\"].sort_values(\"Difference from Mean Confidence\").head(5)\n",
    "typical_nondefaults = prediction_examples[prediction_examples[\"Actual Default\"] == \"No\"].sort_values(\"Difference from Mean Confidence\").head(5)\n",
    "\n",
    "# Combine and show typical prediction examples from each class\n",
    "typical_examples = pd.concat([typical_defaults, typical_nondefaults]).drop(columns=[\"Confidence Score\", \"Difference from Mean Confidence\"])\n",
    "print(\"Typical examples (average confidence):\")\n",
    "display(typical_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ce32e-552f-404a-8a72-79207024321b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    📌 Display the best, worst, and typical prediction example of each class from the test data.\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    Option 1: Single Table\n",
    "</p>\n",
    "\n",
    "| Example | Income | Age | State Default Rate | Experience | Current Job Yrs | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|---------|-----------|---- |------------|------------|----------|----------|----------|------------|------------|\n",
    "| Best    | 495,619   | 26  | 12.8%      | 1          | 1        | Yes      | Yes      | 99%        | ✅ No     |\n",
    "| Best    | 2,901,323 | 56  | 13.7%      | 2          | 2        | No       | No       | 100%       | ✅ No     |\n",
    "| Worst   | 8,290,834 | 42  | 12.8%      | 3          | 3        | Yes      | No       | 95%        | ❌ Yes    |\n",
    "| Worst   | 7,644,982 | 24  | 12.2%      | 1          | 1        | No       | Yes      | 98%        | ❌ Yes    |\n",
    "| Typical | 4,570,845 | 47  | 15.5%      | 3          | 3        | Yes      | Yes      | 94%        | ✅ No     |\n",
    "| Typical | 8,391,288 | 24  | 11.7%      | 4          | 4        | No       | No       | 94%        | ✅ No     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d31905-c754-40dc-829f-5244aca1db8a",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    Option 2: Separate Tables\n",
    "</p>\n",
    "\n",
    "🏆 Best Predictions (Correct with High Confidence)\n",
    "| Income | Age | State Default Rate | Experience | Current Job Yrs | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|-----------|---- |------------|------------|----------|----------|----------|------------|------------|\n",
    "| 495,619   | 26  | 12.8%      | 1          | 1        | Yes      | Yes      | 99%        | ✅ No     |\n",
    "| 2,901,323 | 56  | 13.7%      | 2          | 2        | No       | No       | 100%       | ✅ No     |\n",
    "\n",
    "⚠️ Worst Predictions (Incorrect with High Confidence)\n",
    "| Income | Age | State Default Rate | Experience | Current Job Yrs | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|-----------|---- |------------|------------|----------|----------|----------|------------|------------|\n",
    "| 8,290,834 | 42  | 12.8%      | 3          | 3        | Yes      | No       | 95%        | ❌ Yes    |\n",
    "| 7,644,982 | 24  | 12.2% \t   | 1          | 1        | No       | Yes      | 98%        | ❌ Yes    |\n",
    "\n",
    "➖ Typical Predictions (Average Confidence)\n",
    "| Income | Age | State Default Rate | Experience | Current Job Yrs | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|-----------|---- |------------|------------|----------|----------|----------|------------|------------|\n",
    "| 4,570,845 | 47  | 15.5%      | 3          | 3        | Yes      | Yes      | 94%        | ✅ No     |\n",
    "| 8,391,288 | 24  | 11.7%      | 4          | 4        | No       | No       | 94%        | ✅ No     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12bbb0-4f4e-40ce-bafa-37ef0d7aac84",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Summary</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa512abc-b9e0-4f0f-8d3c-bb64a828dd14",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">🧹 Data Preprocessing</h2>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Used `pandas` and `sklearn` for data loading, cleaning, transformation, and saving.\n",
    "- **Loaded data** from .csv file using `pandas` `read_csv`.\n",
    "- **Standardized column names and labels** to `snake_case` using `pandas` string methods and `apply` with custom functions.\n",
    "- **Handled duplicates**: Verified the absence of duplicates using both the ID column and complete row comparison.\n",
    "- **Handled data types**: Converted string columns with two categories to boolean columns using `pandas` `map`.\n",
    "- **Train-validation-test split**: Split data into training (80%), validation (10%), and test (10%) sets using `sklearn` `train_test_split`.\n",
    "- **Engineered new features**: Derived job stability from profession and city tier from city using mapping functions with  `pandas` `map`. Derived state default rate from state using target encoding.\n",
    "- **Defined semantic type** for each column (numerical, categorical, boolean).\n",
    "- **Handled missing values**: Verified the absence of missing values in all columns and datasets.\n",
    "- **Handled outliers**: Identified multivariate outliers using `sklearn` `IsolationForest` and univariate outliers using statistical methods (3SD and 1.5 IQR) with custom transformer classes that inherit from `sklearn` `BaseEstimator` and `TransformerMixin`.\n",
    "- **Feature scaling and encoding**:\n",
    "    - Scaled numerical features: Used standard scaling with `sklearn` `StandardScaler`.\n",
    "    - Encoded categorical features: Used one-hot encoding for nominal features (`sklearn` `OneHotEncoder`) and ordinal encoding for ordinal features (`OrdinalEncoder`).\n",
    "    - Applied scaling and encoding together using `sklearn` `ColumnTransformer`.\n",
    "- **Saved the preprocessed data** for training, validation, and test sets as .csv files using `pandas` `to_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787982d-c7bd-47f4-9619-8040018b4d8d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">🔍 Exploratory Data Analysis (EDA)</h2>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Used `pandas`, `numpy`, `seaborn`, and `matplotlib` for statistical analysis and visualizations.\n",
    "- **Univariate EDA**:\n",
    "    - **Numerical columns**:\n",
    "        - Analyzed descriptive statistics (e.g., mean, median, standard deviation) using `pandas` `describe`.\n",
    "        - Visualized distributions with histograms using `seaborn` `histplot` and `matplotlib`.\n",
    "    - **Categorical columns**:\n",
    "        - Examined frequencies using `pandas` `value_counts`.\n",
    "        - Visualized frequency distributions with bar plots using `seaborn` `barplot` and `matplotlib`. \n",
    "- **Bivariate EDA**:\n",
    "    - **Numerical vs. numerical**:\n",
    "        - Analyzed pairwise relationships with a correlation matrix (`pandas` `corr` and `numpy`) and visualized them with a heatmap (`seaborn` `heatmap`).\n",
    "        - Visualized relationships with scatterplots using `seaborn` `scatterplot` and `matplotlib`.\n",
    "    - **Numerical vs. categorical**:\n",
    "        - Explored relationships with group-wise statistics (e.g., mean or median by category) using `pandas` `groupby` and `agg`.\n",
    "        - Quantified the magnitude of group differences with Cohen's d using a custom function.\n",
    "        - Visualized results with bar plots using `seaborn` `barplot` and `matplotlib`.\n",
    "    - **Categorical vs. categorical**:\n",
    "        - Analyzed relationships with contingency tables using `pandas` `crosstab`.\n",
    "        - Visualized relationships with grouped bar plots using `pandas` `crosstab` `plot` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b09d1e-0d12-4c17-a6d7-bc172169f347",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">🏗️ Modeling</h2>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Used `sklearn` and `xgboost` for model training, evaluation, and optimization.\n",
    "- **Baseline models**:\n",
    "    - Trained eight baseline models with default hyperparameter values: Logistic Regression, Elastic Net, K-Nearest Neighbors, Support Vector Machine, Multi-Layer Perceptron, Decision Tree, Random Forest, and XGBoost.\n",
    "    - Trained each model with four outlier handling methods. Proceeded without outlier handling, as it did not meaningfully improve performance.\n",
    "    - Evaluated model performance using metrics and diagnostics:\n",
    "        - Primary metric: Calculated AUC-PR using `sklearn` `precision_recall_curve` and `auc`.\n",
    "        - Secondary metrics: Calculated class-1 recall, precision, and F1-score using `sklearn` `precision_recall_fscore_support`.\n",
    "        - Compared metrics using tables (`pandas`) and comparison plots (`seaborn` `barplot` and `matplotlib`).\n",
    "        - Plotted precision-recall curves using `matplotlib`.\n",
    "        - Created classification reports with `sklearn` `classification_report`.\n",
    "        - Plotted confusion matrices with `sklearn` `confusion_matrix` and `ConfusionMatrixDisplay`.\n",
    "        - Analyzed overfitting using custom tables (`pandas`) and plots (`seaborn` `barplot` and `matplotlib`).\n",
    "        - Analyzed feature-misclassification relationships through correlations (`pandas` `corr`) and grouped box plots (`seaborn` `boxplot` and `matplotlib`).\n",
    "- **Hyperparameter tuning**:\n",
    "    - Performed random search with 5-fold cross-validation on the most promising baseline models using `sklearn` `RandomizedSearchCV`.\n",
    "    - Retrained the best-performing model from each algorithm and plotted precision-recall curves.\n",
    "    - Optimized decision thresholds and evaluated all tuned models using the above metrics and diagnostics.\n",
    "- **Final model**:\n",
    "    - Selected Random Forest for its good performance (highest AUC-PR), low overfitting (lowest AUC-PR difference), and interpretability.\n",
    "    - Retrained and saved the final model as a .pkl file using `pickle`.\n",
    "    - Compared training, validation, and test performance using the above metrics and diagnostics.\n",
    "    - Visualized feature importances using `seaborn` `barplot` and `matplotlib`.\n",
    "    - Showed model prediction examples of the best, worst, and typical cases using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa50fb3-a933-4733-a7d0-cd088409a9ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">🚀 Future Improvements</h2>\n",
    "</div>\n",
    "\n",
    "- **Data Enrichment**: Collect more data on crucial financial features (e.g., loan amount, loan duration, interest rate, type of loan, existing debt, credit score) to enable more precise risk assessment and improve model performance.\n",
    "- **Cost-Sensitive Optimization**: Refine model optimization by incorporating the actual cost of defaults (false negatives) and the opportunity cost of rejecting good loans (false positives) to better align the model with business goals and financial impact.\n",
    "- **Misclassification Deep Dive**: Conduct in-depth analysis of high-confidence misclassifications to understand model limitations and develop strategies to address them.\n",
    "- **Advanced Explainability**: Implement more granular model explainability techniques (e.g., SHAP, LIME) at an individual prediction level to provide clearer reasons for loan approval/denial decisions, enhancing transparency for stakeholders, customers, and regulatory compliance.\n",
    "- **MLOps Pipeline**: Develop a comprehensive MLOps pipeline for automated retraining, deployment, and continuous monitoring, ensuring sustained model performance and timely detection of data or concept drift in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-default-venv",
   "language": "python",
   "name": "loan-default-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
