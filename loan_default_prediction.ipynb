{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4275851-2816-4903-91c3-8b4a4c489b12",
   "metadata": {},
   "source": [
    "# Project Overview: Predicting Loan Defaults\n",
    "\n",
    "**Executive Summary**  \n",
    "This project aims to develop a machine learning model to predict whether the customers of a financial institution will default on a loan based on data from their loan application. Accurately identifying potential defaulters enables financial institutions to make informed lending decisions, adjust loan terms, and better manage financial risk.\n",
    "\n",
    "**Problem**  \n",
    "Predicting loan defaults is a challenging task due to the multitude of influencing factors such as customers' demographic, financial, location, and behavioral attributes. Traditional default prediction models often oversimplify complex relationships between customer features and default risk. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in loan application data, enabling more accurate predictions of loan default risk.\n",
    "\n",
    "**Objectives**  \n",
    "The project aims to:\n",
    "- Develop a machine learning model to predict loan defaults using customer data from loan applications.\n",
    "- Compare multiple models (e.g., Logistic Regression, Random Forest, XGBoost) using the Area Under the Precision-Recall Curve (AUC-PR) as the primary evaluation metric.\n",
    "- Identify key factors influencing loan default risk through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project enables financial institutions to reduce loan default rates and make better and faster lending decisions by leveraging machine learning for automated and improved risk assessment. \n",
    "\n",
    "**Business Goals**  \n",
    "The project aims to achieve the following business goals:\n",
    "- Reduce losses by 100,000-200,000$ within 12 months after model deployment by decreasing the loan default rate by 10%-20%.\n",
    "- Decrease loan processing time by 25%-40% by automating risk assessment, leading to less time spent on manual evaluations.\n",
    "- Achieve 100% regulatory compliance with fair lending practices.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains information provided by customers of a financial institution during the loan application process. It is sourced from the \"Loan Prediction Based on Customer Behavior\" dataset by Subham Jain, available on [Kaggle](https://www.kaggle.com/datasets/subhamjain/loan-prediction-based-on-customer-behavior). \n",
    "\n",
    "Dataset Statistics:\n",
    "- Training set size: 252,000 records (12.3% defaults)\n",
    "- Test set size: 28,000 records (12.8% defaults)\n",
    "- 11 Features:\n",
    "  - Demographic: Age, married, profession\n",
    "  - Financial: Income, house ownership, car ownership\n",
    "  - Location: City, state\n",
    "  - Behavioral: Experience, current job years, current house years\n",
    "\n",
    "Data Overview Table:\n",
    "\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Training Data Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Risk Flag | Defaulted on loan (0: No, 1: Yes) | Integer | Categorical (Binary) | [0, 1] | [0, 1] |\n",
    "| Income | Income of the applicant | Integer | Numerical | [0, ∞] | [10K, 10M] |\n",
    "| Age | Age of the applicant (in years) | Integer | Numerical | [18, ∞] | [21, 79] |\n",
    "| Experience | Work experience (in years) | Integer | Numerical | [0, ∞] | [0, 20] |\n",
    "| Profession | Applicant's profession | String | Categorical (Nominal) | Any profession [e.g., \"Architect\", \"Dentist\"] | 8 unique professions |\n",
    "| Married | Marital status | String | Categorical (Binary) | [\"single\", \"married\"] | [\"single\", \"married\"] |\n",
    "| House Ownership | Applicant owns or rents a house | String | Categorical (Nominal) | [\"rented\", \"owned\", \"norent_noown\"] | [\"rented\", \"owned\", \"norent_noown\"] |\n",
    "| Car Ownership | Whether applicant owns a car | String | Categorical (Binary) | [\"yes\", \"no\"] | [\"yes\", \"no\"] |\n",
    "| Current Job Years | Years in the current job | Integer | Numerical | [0, ∞] | [0, 14] |\n",
    "| Current House Years | Years in the current house | Integer | Numerical | [0, ∞] | [10, 14] |\n",
    "| City | City of residence | String | Categorical (Nominal) | Any city [e.g., \"Mumbai\", \"Bangalore\"] | 317 unique cities |\n",
    "| State | State of residence | String | Categorical (Nominal) | Any state [e.g., \"Maharashtra\", \"Tamil_Nadu\"] | 5 unique states |\n",
    "\n",
    "The dataset consists of three CSV files:\n",
    "1. `Training Data.csv`: Contains all the features along with the target variable (`Risk Flag`) and an `ID` column. \n",
    "2. `Test Data.csv`: Contains only the features and an `ID` column from the test data.\n",
    "3. `Sample Prediction Dataset.csv`: Contains the target variable (`Risk Flag`) and an `ID` column from the test data.\n",
    "\n",
    "Example Training Data:\n",
    "\n",
    "| Risk Flag | Income    | Age | Experience | Profession         | Married | House Ownership | Car Ownership | Current Job Years | Current House Years | City      | State         |\n",
    "| :-------- | :-------- | :-- | :--------- | :----------------- | :------ | :-------------- | :------------ | :---------------- | :------------------ | :-------- | :------------ |\n",
    "| 0         | 1,303,834 | 23  | 3          | Mechanical_engineer | single  | rented          | no            | 3                 | 13                   | Rewa      | Madhya_Pradesh |\n",
    "| 1         | 6,256,451 | 41  | 2          | Software_Developer | single  | rented          | yes           | 2                 | 12                   | Bangalore | Tamil_Nadu    |\n",
    "| 0         | 3,991,815 | 66  | 4          | Technical_writer   | married | rented          | no            | 4                 | 10                   | Alappuzha | Kerala        |\n",
    "\n",
    "**Technical Requirements**  \n",
    "The project must meet the following technical requirements:\n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, incorrect data types, missing values, and outliers.\n",
    "  - Extract features, scale numerical features, and encode categorical features.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models and perform hyperparameter tuning for binary classification task with `sklearn` and `xgboost`.\n",
    "  - Baseline models: Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest, Multi-Layer Perceptron, XGBoost.\n",
    "  - Evaluate model performance using Area Under the Precision-Recall Curve (AUC-PR).\n",
    "    - AUC-PR is more suitable to address class imbalance (12.3% defaults) with a focus on the positive class (preventing defaults) than accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "    - Success criterion: Minimum AUC-PR of 0.70 on the test data.\n",
    "  - Potentially use additional techniques to address class imbalance (e.g., SMOTE, class weights).\n",
    "  - Visualize feature importance, show model prediction examples, and save the final model with `pickle`.\n",
    "- Deployment:\n",
    "  - Create a REST API for model deployment and integration with existing loan processing systems.\n",
    "  - Implement efficient batch processing capabilities for multiple loan applications.\n",
    "  - Deploy using cloud infrastructure to ensure scalability and security.\n",
    "  - Develop monitoring systems for model performance and data drift.\n",
    "- Stakeholders:\n",
    "  - Loan officers: Direct users of the model predictions in day-to-day loan approvals.\n",
    "  - Credit risk analysts: Provide subject matter expertise on loan default risk.\n",
    "  - Compliance officers: Ensure the model complies with any legal and regulatory guidelines.\n",
    "  - IT department: Manage the IT infrastructure and ensure data access for the model's development and deployment. \n",
    "\n",
    "By fulfilling these objectives and requirements, the project will provide a valuable tool for predicting loan defaults, thereby enhancing decision-making for financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9b38-149a-4c08-bb6a-c1ad97d33d28",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd00e3c4-73f9-45bb-a474-58fc6049796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7316e-f065-4b92-8f2c-5cd5681b5a69",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70baf18b-5437-4c7d-b74f-d432ee23be5c",
   "metadata": {},
   "source": [
    "Load data from the three .csv files into three Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1e0114-5f1a-44ce-85e7-3136fe031aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"data/training_data.csv\")\n",
    "    X_test = pd.read_csv(\"data/test_data.csv\")\n",
    "    y_test = pd.read_csv(\"data/sample_prediction_dataset.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866b8d1-12e6-4f47-b54e-8132b55b4c1e",
   "metadata": {},
   "source": [
    "# Initial Data Inspection  \n",
    "Basic exploration of the dataset to understand its structure and detect obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc38bee-eefa-48ce-9ddb-b9464e6caa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252000 entries, 0 to 251999\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Id                 252000 non-null  int64 \n",
      " 1   Income             252000 non-null  int64 \n",
      " 2   Age                252000 non-null  int64 \n",
      " 3   Experience         252000 non-null  int64 \n",
      " 4   Married/Single     252000 non-null  object\n",
      " 5   House_Ownership    252000 non-null  object\n",
      " 6   Car_Ownership      252000 non-null  object\n",
      " 7   Profession         252000 non-null  object\n",
      " 8   CITY               252000 non-null  object\n",
      " 9   STATE              252000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    252000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  252000 non-null  int64 \n",
      " 12  Risk_Flag          252000 non-null  int64 \n",
      "dtypes: int64(7), object(6)\n",
      "memory usage: 25.0+ MB\n",
      "None\n",
      "\n",
      "Test Data - Features:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 28000 non-null  int64 \n",
      " 1   Income             28000 non-null  int64 \n",
      " 2   Age                28000 non-null  int64 \n",
      " 3   Experience         28000 non-null  int64 \n",
      " 4   Married/Single     28000 non-null  object\n",
      " 5   House_Ownership    28000 non-null  object\n",
      " 6   Car_Ownership      28000 non-null  object\n",
      " 7   Profession         28000 non-null  object\n",
      " 8   CITY               28000 non-null  object\n",
      " 9   STATE              28000 non-null  object\n",
      " 10  CURRENT_JOB_YRS    28000 non-null  int64 \n",
      " 11  CURRENT_HOUSE_YRS  28000 non-null  int64 \n",
      "dtypes: int64(6), object(6)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "Test Data - Target Variable:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   id         28000 non-null  int64\n",
      " 1   risk_flag  28000 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 437.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.info())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3692-d2c8-46f1-b0ad-36372c1fa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the training data\n",
    "print(\"Training Data:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890b354-634e-46b6-aaf0-0868ad775bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the test data features\n",
    "print(\"Test Data - Features:\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad411a4-8229-41b9-843b-ba280a6b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the test data target variable\n",
    "print(\"Test Data - Target Variable:\")\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9596-7328-43a4-8cf6-a18d1b64b4f7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af4f32-03ee-43b6-9289-6821b800328b",
   "metadata": {},
   "source": [
    "## Handling Inconsistent Column Names\n",
    "Some column names are inconsistent across the training and test dataset (e.g., \"Id\" vs. \"ID\", \"Risk_Flag\" vs. \"risk_flag\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e438b-50b7-4391-8b8e-0b54f1d8b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names to lowercase and replace \"/\" with \"_\"\n",
    "df_train.columns = df_train.columns.str.lower().str.replace(\"/\", \"_\")\n",
    "X_test.columns = X_test.columns.str.lower().str.replace(\"/\", \"_\")\n",
    "y_test.columns = y_test.columns.str.lower().str.replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07565d7a-e2b0-472d-91dd-f860878e384b",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1bd8b-5c42-4236-a481-eba15643a694",
   "metadata": {},
   "source": [
    "Duplicates based on all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d98e50-9696-47fe-b1ad-51ec59375e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates based on all columns\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated().value_counts())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.duplicated().value_counts())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b48ba-dc42-4591-bc44-e490092a79f2",
   "metadata": {},
   "source": [
    "No duplicates based on all columns in training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517791c-e872-4d32-96cf-5974efc0526d",
   "metadata": {},
   "source": [
    "Duplicates based on the ID column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e53c-dcfa-42eb-8542-7f215b4d3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates based on ID column\n",
    "print(\"Training Data:\")\n",
    "print(df_train.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.duplicated(subset=[\"id\"]).value_counts())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.duplicated(subset=[\"id\"]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57954bf9-09ec-49df-8cb7-ed39d546d14f",
   "metadata": {},
   "source": [
    "There are no duplicates based on the ID column in the training data, the test data features, or the test data target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89142d-7b4a-4b23-98b3-3c9e63ee5eb8",
   "metadata": {},
   "source": [
    "## Handling Incorrect Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be913b0-2788-4e8d-8797-37233709dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose incorrect data types\n",
    "print(\"Training Data:\")\n",
    "print(df_train.dtypes)\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.dtypes)\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64e9b1-5b3b-459c-8b1c-456c683691c2",
   "metadata": {},
   "source": [
    "All the columns in the training and test data have the correct data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413ec8b-66cd-4d93-a071-7b076db6fbf4",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07a4d2-f28d-4ad1-a618-fde9e0753d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore number of unique categories in string columns\n",
    "categorical_columns = df_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Training Data:\")\n",
    "print(df_train[categorical_columns].nunique())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test[categorical_columns].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332edd65-5789-48e4-8aa6-d055d94711b5",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f37db6-2329-44af-90ab-ec03a0e6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose missing values\n",
    "print(\"Training Data:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e30f60-8cfa-419d-a915-e37a9ddde4ac",
   "metadata": {},
   "source": [
    "There are no missing values in the training data, the test data features, or the test data target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386d0a-d26f-462c-96f4-4b557da2943c",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7375e-b714-44f3-b184-c28af316e22a",
   "metadata": {},
   "source": [
    "### Remove with 3SD Method  \n",
    "Remove univariate outliers from a numerical column by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342282c-c2ec-4971-80ec-5d2067c5e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_column):\n",
    "        # Calculate mean, standard deviation, and cutoff values of the numerical column\n",
    "        self.mean_ = df[numerical_column].mean()\n",
    "        self.sd_ = df[numerical_column].std()\n",
    "        self.lower_cutoff_ = self.mean_ - 3 * self.sd_\n",
    "        self.upper_cutoff_ = self.mean_ + 3 * self.sd_\n",
    "\n",
    "        # Create a mask for filtering outliers\n",
    "        self.mask_ = (df[numerical_column] >= self.lower_cutoff_) & (df[numerical_column] <= self.upper_cutoff_)\n",
    "\n",
    "        # Show cutoff values and number of outliers\n",
    "        print(f\"Lower cutoff for {numerical_column}: {self.lower_cutoff_}\")\n",
    "        print(f\"Upper cutoff for {numerical_column}: {self.upper_cutoff_}\")\n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers identified based on {numerical_column} using the 3SD method.\")\n",
    "  \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the mask \n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers removed based on {numerical_column} using the 3SD method.\")\n",
    "        return df[self.mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_column):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_column).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7efcbf-bec0-49f6-82bc-ec0d98ca51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Remove outliers\n",
    "outlier_remover_3sd.fit(df_train, \"income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815f4f-0f4f-456f-8eb3-6eaebf035b19",
   "metadata": {},
   "source": [
    "# Future Improvements\n",
    "\n",
    "**Data Enrichment**:  \n",
    "To enhance the model's performance and business value, data enrichment with the following financial features is recommended:\n",
    "- Loan amount \n",
    "- Loan duration\n",
    "- Interest rate\n",
    "- Type of loan (e.g., personal, home, vehicle)\n",
    "- Existing debt\n",
    "- Credit score\n",
    "\n",
    "**Enhanced Analysis Capabilities**:  \n",
    "The addition of these features, particularly loan amount, would enable more precise risk assessment and better alignment with business objectives through cost-sensitive evaluation metrics incorporating actual monetary values. Cost-sensitive metrics or expected monetary value incorporate the actual cost of defaults (false negatives) and the opportunity cost of rejecting good loans (false positives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-default-venv",
   "language": "python",
   "name": "loan-default-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
